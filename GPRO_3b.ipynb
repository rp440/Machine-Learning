{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cb1921306ae64ef198df4544e79d4918": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4d8d81e0389d4cfe8f1ec832dfb822df",
              "IPY_MODEL_128885171f9543c283e83f1fdc2e1cee",
              "IPY_MODEL_78c965cb33e54f518dfcb5342d7ef969"
            ],
            "layout": "IPY_MODEL_b9717ed4ae0b43eebf1d4b9b3eb9841e"
          }
        },
        "4d8d81e0389d4cfe8f1ec832dfb822df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6000d5ad5143458bbba4e7c4d11465fd",
            "placeholder": "​",
            "style": "IPY_MODEL_f71d70acba0143bdbfae388e18a0a2ef",
            "value": "README.md: 100%"
          }
        },
        "128885171f9543c283e83f1fdc2e1cee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06a8289bb4e7491588ee876537cebc93",
            "max": 2431,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a4408536e45244bfb937e10a9d0d3fe8",
            "value": 2431
          }
        },
        "78c965cb33e54f518dfcb5342d7ef969": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a336d43b8b014cc9aea08609df742431",
            "placeholder": "​",
            "style": "IPY_MODEL_15baa1367faa4bb694f53dc6a76cc096",
            "value": " 2.43k/2.43k [00:00&lt;00:00, 180kB/s]"
          }
        },
        "b9717ed4ae0b43eebf1d4b9b3eb9841e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6000d5ad5143458bbba4e7c4d11465fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f71d70acba0143bdbfae388e18a0a2ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "06a8289bb4e7491588ee876537cebc93": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4408536e45244bfb937e10a9d0d3fe8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a336d43b8b014cc9aea08609df742431": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15baa1367faa4bb694f53dc6a76cc096": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b9ffb48c0ff14e39af235c5bfb1bb7eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b01dd4545d0045a4bcc26ff5a1dff3d5",
              "IPY_MODEL_aabb54edec334789a9e77384b61a2455",
              "IPY_MODEL_fb29a83efebf4c12bdb36187fe9b8b9f"
            ],
            "layout": "IPY_MODEL_ea4496d9cdda4ac19fda0c41b34d607b"
          }
        },
        "b01dd4545d0045a4bcc26ff5a1dff3d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_28c36fa858cd49749281f7f70d582505",
            "placeholder": "​",
            "style": "IPY_MODEL_97579dd87ea64cd59b3029b9d821e36c",
            "value": "train-00000-of-00001.parquet: 100%"
          }
        },
        "aabb54edec334789a9e77384b61a2455": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9aeac10078f94da29cfe7052b6054d38",
            "max": 147342955,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6dfe36cec84d46be8b4a06418d456629",
            "value": 147342955
          }
        },
        "fb29a83efebf4c12bdb36187fe9b8b9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a18a7589ff34703a4bc33f15b643071",
            "placeholder": "​",
            "style": "IPY_MODEL_09f7236c602e4581921af9b8f03001fe",
            "value": " 147M/147M [00:00&lt;00:00, 326MB/s]"
          }
        },
        "ea4496d9cdda4ac19fda0c41b34d607b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28c36fa858cd49749281f7f70d582505": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97579dd87ea64cd59b3029b9d821e36c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9aeac10078f94da29cfe7052b6054d38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6dfe36cec84d46be8b4a06418d456629": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2a18a7589ff34703a4bc33f15b643071": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09f7236c602e4581921af9b8f03001fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3eb04fc58ab44a40949e9511c4c25417": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_caad1b4e1519403cb855c5b5fd0fa735",
              "IPY_MODEL_200573ab1f814d988e128eba43e7e001",
              "IPY_MODEL_234336b2a5d34089946cddab0d0bc930"
            ],
            "layout": "IPY_MODEL_55f85ce4885e4296a670413ae0e37708"
          }
        },
        "caad1b4e1519403cb855c5b5fd0fa735": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56a30f91680546e38ebfaf25ee16dd9c",
            "placeholder": "​",
            "style": "IPY_MODEL_944e143b03254b2483b01d7bbdbe1299",
            "value": "test-00000-of-00001.parquet: 100%"
          }
        },
        "200573ab1f814d988e128eba43e7e001": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_342d6197ffb6478aa36b08f9a887e05d",
            "max": 215035,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a5e2a429315345bb94340153b2463edc",
            "value": 215035
          }
        },
        "234336b2a5d34089946cddab0d0bc930": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc258216ca4a42888d7bc6249917e970",
            "placeholder": "​",
            "style": "IPY_MODEL_bc02ff06a2834903a4ba8eead439077a",
            "value": " 215k/215k [00:00&lt;00:00, 16.5MB/s]"
          }
        },
        "55f85ce4885e4296a670413ae0e37708": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56a30f91680546e38ebfaf25ee16dd9c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "944e143b03254b2483b01d7bbdbe1299": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "342d6197ffb6478aa36b08f9a887e05d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5e2a429315345bb94340153b2463edc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dc258216ca4a42888d7bc6249917e970": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc02ff06a2834903a4ba8eead439077a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "123a4ff192394a479cdd2e2a232b7c36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f847df33e7bb40fabe667148ef831af9",
              "IPY_MODEL_f42d1dc03fe543d99af507d92bb49edc",
              "IPY_MODEL_77fe904174304adeb0edb7cced2c561b"
            ],
            "layout": "IPY_MODEL_84f8f536421b4c84869e1e1c79d0df49"
          }
        },
        "f847df33e7bb40fabe667148ef831af9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_876759e08ed14d1184311ec6cfd04ed0",
            "placeholder": "​",
            "style": "IPY_MODEL_51cca855eca8497d87964df281478ef8",
            "value": "Generating train split: 100%"
          }
        },
        "f42d1dc03fe543d99af507d92bb49edc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6dfd00330cb64fd081a18adc8718e171",
            "max": 72441,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f49cb23031114641b05a79f20d546a91",
            "value": 72441
          }
        },
        "77fe904174304adeb0edb7cced2c561b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b736529a7ef2445db7e42c1f5d763a82",
            "placeholder": "​",
            "style": "IPY_MODEL_b0810095e4d14f9997540cc9a7a87c9a",
            "value": " 72441/72441 [00:01&lt;00:00, 44134.05 examples/s]"
          }
        },
        "84f8f536421b4c84869e1e1c79d0df49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "876759e08ed14d1184311ec6cfd04ed0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51cca855eca8497d87964df281478ef8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6dfd00330cb64fd081a18adc8718e171": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f49cb23031114641b05a79f20d546a91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b736529a7ef2445db7e42c1f5d763a82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0810095e4d14f9997540cc9a7a87c9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "27b9e17be73744cda1a9a6f232a870ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9c71bb156b9c425183a2235e277be14f",
              "IPY_MODEL_56137c3f50044ab2826cb3c45422cd2b",
              "IPY_MODEL_30a84f0374e44f16b22da827e969ed2b"
            ],
            "layout": "IPY_MODEL_10c7b950df3d409b945361d7f7fc5e11"
          }
        },
        "9c71bb156b9c425183a2235e277be14f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac05924d03814993a21a9d9468e99aea",
            "placeholder": "​",
            "style": "IPY_MODEL_475cc6a7a4094bdab6729e1b9619a89e",
            "value": "Generating test split: 100%"
          }
        },
        "56137c3f50044ab2826cb3c45422cd2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_938808caca1f462e9b0eb5d27abcec2e",
            "max": 99,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_95bc4e8e76dd410783f904b9bd06a071",
            "value": 99
          }
        },
        "30a84f0374e44f16b22da827e969ed2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82ea55b2d06b4a4a89d5a8702fcfb680",
            "placeholder": "​",
            "style": "IPY_MODEL_ce3b8763d91c477da577022043f2f5bd",
            "value": " 99/99 [00:00&lt;00:00, 2465.86 examples/s]"
          }
        },
        "10c7b950df3d409b945361d7f7fc5e11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac05924d03814993a21a9d9468e99aea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "475cc6a7a4094bdab6729e1b9619a89e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "938808caca1f462e9b0eb5d27abcec2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95bc4e8e76dd410783f904b9bd06a071": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "82ea55b2d06b4a4a89d5a8702fcfb680": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce3b8763d91c477da577022043f2f5bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "798b8ef3f3ec4f82982673cd3af4ad93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b63a9b0b53b04b3f88c44e900c302c3c",
              "IPY_MODEL_36f9e28a2bce4f7594098364840c44cd",
              "IPY_MODEL_528d1547ea7c4449939f92d03acde0df"
            ],
            "layout": "IPY_MODEL_5c7b86492565452393b6dd8e5a2ff216"
          }
        },
        "b63a9b0b53b04b3f88c44e900c302c3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54e0d4d738a3404db15738a42819e85c",
            "placeholder": "​",
            "style": "IPY_MODEL_609853e5bd1547b8b28d6a80d0f4e8ba",
            "value": "Map: 100%"
          }
        },
        "36f9e28a2bce4f7594098364840c44cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c596279f7b274e3281951575d39ea106",
            "max": 3622,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d5e0c9f1f7a84208a8631b06ead5a710",
            "value": 3622
          }
        },
        "528d1547ea7c4449939f92d03acde0df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5fc0b67dbac48d1ab21d131e0bd9abc",
            "placeholder": "​",
            "style": "IPY_MODEL_113e0b95e84e48aa948257788419a445",
            "value": " 3622/3622 [00:00&lt;00:00, 7244.95 examples/s]"
          }
        },
        "5c7b86492565452393b6dd8e5a2ff216": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54e0d4d738a3404db15738a42819e85c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "609853e5bd1547b8b28d6a80d0f4e8ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c596279f7b274e3281951575d39ea106": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5e0c9f1f7a84208a8631b06ead5a710": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a5fc0b67dbac48d1ab21d131e0bd9abc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "113e0b95e84e48aa948257788419a445": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b44c75a72dcf40419975b42379cd2452": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2eb35ea1e6784e38bf5c8647cbb35775",
              "IPY_MODEL_1f0620475fc44c20b033ad4cf8422fac",
              "IPY_MODEL_0d35c6ff25154dd08f21a562d8d747db"
            ],
            "layout": "IPY_MODEL_aa39681f075a465a9d22e3406ab76775"
          }
        },
        "2eb35ea1e6784e38bf5c8647cbb35775": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7cadebb3f5c74381bf222933af4d3a3a",
            "placeholder": "​",
            "style": "IPY_MODEL_25cdb322c88341e4b0f921a9aec87014",
            "value": "Map: 100%"
          }
        },
        "1f0620475fc44c20b033ad4cf8422fac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d99dc32a167e49ad85a369b213bc0a94",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_53110aa4254c46d3b60da27a00a2dc70",
            "value": 5
          }
        },
        "0d35c6ff25154dd08f21a562d8d747db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3a29670465c4ad08cbfdec472824d84",
            "placeholder": "​",
            "style": "IPY_MODEL_67124d81db6c45239f5fd7d33c44c1fb",
            "value": " 5/5 [00:00&lt;00:00, 231.39 examples/s]"
          }
        },
        "aa39681f075a465a9d22e3406ab76775": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7cadebb3f5c74381bf222933af4d3a3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25cdb322c88341e4b0f921a9aec87014": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d99dc32a167e49ad85a369b213bc0a94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53110aa4254c46d3b60da27a00a2dc70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f3a29670465c4ad08cbfdec472824d84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67124d81db6c45239f5fd7d33c44c1fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "32a564ac6b08404ab5280b8753b2051a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_68e84ee2761c46888aa3e4062c14bfe4",
              "IPY_MODEL_b9ca785e6d5c4b5c819f85c575425362",
              "IPY_MODEL_e7d22f83d6054a33a8d2714ba9895449"
            ],
            "layout": "IPY_MODEL_c719d6b162364eeeb6fa396c7436a073"
          }
        },
        "68e84ee2761c46888aa3e4062c14bfe4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad2a35b006d246e9bcec994a12f3638f",
            "placeholder": "​",
            "style": "IPY_MODEL_bcb98601d80b4669baaa0843f046c9ac",
            "value": "config.json: 100%"
          }
        },
        "b9ca785e6d5c4b5c819f85c575425362": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9ccdd48e2be347daa5c158a7e8712ff7",
            "max": 659,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f17ad365ca6f44499bb8aa9c0b78d75b",
            "value": 659
          }
        },
        "e7d22f83d6054a33a8d2714ba9895449": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34f19cbff346461aa114ed3e239367f2",
            "placeholder": "​",
            "style": "IPY_MODEL_1aa875dde21b46a6b19ec78370a4cda7",
            "value": " 659/659 [00:00&lt;00:00, 39.4kB/s]"
          }
        },
        "c719d6b162364eeeb6fa396c7436a073": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad2a35b006d246e9bcec994a12f3638f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bcb98601d80b4669baaa0843f046c9ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9ccdd48e2be347daa5c158a7e8712ff7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f17ad365ca6f44499bb8aa9c0b78d75b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "34f19cbff346461aa114ed3e239367f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1aa875dde21b46a6b19ec78370a4cda7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cdb7a92b8eac4fbf828fbd2619fd83bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_708d3a5904db4ac1ad86757429d0df97",
              "IPY_MODEL_e4ecbfbdcacf48cfa760777d418aa4f3",
              "IPY_MODEL_1803021b8fcd43da8656ce492ed0280c"
            ],
            "layout": "IPY_MODEL_e24d30dd77e24c84b21d5245992eaba9"
          }
        },
        "708d3a5904db4ac1ad86757429d0df97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c92948efe8242de8723e81c3ad95413",
            "placeholder": "​",
            "style": "IPY_MODEL_2911698e2e5743698a96893259b54b51",
            "value": "model.safetensors: 100%"
          }
        },
        "e4ecbfbdcacf48cfa760777d418aa4f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_07d7f4dd7cbc4c96a3ffc196440655aa",
            "max": 988097824,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ef26cde9271b4edcbb1a2e9615b1cbef",
            "value": 988097824
          }
        },
        "1803021b8fcd43da8656ce492ed0280c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2a7fc4932ef4061b9c443cde0381eb6",
            "placeholder": "​",
            "style": "IPY_MODEL_e75eb6a232dd47bc90db1fd80e1cea04",
            "value": " 988M/988M [00:03&lt;00:00, 237MB/s]"
          }
        },
        "e24d30dd77e24c84b21d5245992eaba9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c92948efe8242de8723e81c3ad95413": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2911698e2e5743698a96893259b54b51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "07d7f4dd7cbc4c96a3ffc196440655aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef26cde9271b4edcbb1a2e9615b1cbef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b2a7fc4932ef4061b9c443cde0381eb6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e75eb6a232dd47bc90db1fd80e1cea04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2baaeec2bda942c2af4a92a6b4830da4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d52fca5057b54ce3a1f291d91e795099",
              "IPY_MODEL_f2a52b52b98b4194ba6614a604358fd0",
              "IPY_MODEL_d349f0eac647402383b7bf349024cc6c"
            ],
            "layout": "IPY_MODEL_643ea45236a54e95a1a943c358fb19cc"
          }
        },
        "d52fca5057b54ce3a1f291d91e795099": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31092e224db64f40bd1b22fe90eabc64",
            "placeholder": "​",
            "style": "IPY_MODEL_a1b1f41f5b1f4e2aaadee6585ead9ea4",
            "value": "generation_config.json: 100%"
          }
        },
        "f2a52b52b98b4194ba6614a604358fd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf2b64c48ceb43e5bc33ac9422343f08",
            "max": 242,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a61dd72a85914e99926f7499b6ad935d",
            "value": 242
          }
        },
        "d349f0eac647402383b7bf349024cc6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a98e352114144379c146ea78bd4911a",
            "placeholder": "​",
            "style": "IPY_MODEL_11a81f05880b42fa84145f17770b3916",
            "value": " 242/242 [00:00&lt;00:00, 25.4kB/s]"
          }
        },
        "643ea45236a54e95a1a943c358fb19cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31092e224db64f40bd1b22fe90eabc64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1b1f41f5b1f4e2aaadee6585ead9ea4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bf2b64c48ceb43e5bc33ac9422343f08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a61dd72a85914e99926f7499b6ad935d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5a98e352114144379c146ea78bd4911a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11a81f05880b42fa84145f17770b3916": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "95f5e7e8469642b78032d327c2f9a2df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_85adca88285d499bb54932de3d9a377f",
              "IPY_MODEL_b21b9ee3fbe747a793f910218f6cb810",
              "IPY_MODEL_37c97612b40b46cd9b8e9231660eadce"
            ],
            "layout": "IPY_MODEL_1529f9beffc0455eadc5a0b8785c8837"
          }
        },
        "85adca88285d499bb54932de3d9a377f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_870c4ba4ca2f45cfab86a1abcdcb4a57",
            "placeholder": "​",
            "style": "IPY_MODEL_568581c813c34bd490b6df999e4696f1",
            "value": "config.json: 100%"
          }
        },
        "b21b9ee3fbe747a793f910218f6cb810": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2b5ba17010f4dfd98b5783f8260c6f4",
            "max": 659,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_522b5cde0bd44f0d9426865b8a0ad13b",
            "value": 659
          }
        },
        "37c97612b40b46cd9b8e9231660eadce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b3ff88c2e1f428aa25a99524f51eb3e",
            "placeholder": "​",
            "style": "IPY_MODEL_bc91f19570484652884865b78dbcd92a",
            "value": " 659/659 [00:00&lt;00:00, 70.2kB/s]"
          }
        },
        "1529f9beffc0455eadc5a0b8785c8837": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "870c4ba4ca2f45cfab86a1abcdcb4a57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "568581c813c34bd490b6df999e4696f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f2b5ba17010f4dfd98b5783f8260c6f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "522b5cde0bd44f0d9426865b8a0ad13b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0b3ff88c2e1f428aa25a99524f51eb3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc91f19570484652884865b78dbcd92a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "888474474c164133a11433ae88803da3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9907ae87e4de4061852191def067c0df",
              "IPY_MODEL_53d585d87bc1477aa3009ac4e4a6f3b9",
              "IPY_MODEL_e61f84148bda49458048a65deb8ac337"
            ],
            "layout": "IPY_MODEL_9792c465d6ad4f7a9b67205776740b6f"
          }
        },
        "9907ae87e4de4061852191def067c0df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_640b0663afc8454db44bea4092fedff2",
            "placeholder": "​",
            "style": "IPY_MODEL_43b5f375adcc4b7d9a6d37115ff7fdfc",
            "value": "model.safetensors: 100%"
          }
        },
        "53d585d87bc1477aa3009ac4e4a6f3b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8162b3a32223454fbb1af6f229a1fc06",
            "max": 988097824,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aa9c3ab287014d64b6384783e257ea9b",
            "value": 988097824
          }
        },
        "e61f84148bda49458048a65deb8ac337": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3fba33e9c124748874cf7b844392693",
            "placeholder": "​",
            "style": "IPY_MODEL_cb21234130b443abb61c0b39142a14c7",
            "value": " 988M/988M [00:07&lt;00:00, 54.7MB/s]"
          }
        },
        "9792c465d6ad4f7a9b67205776740b6f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "640b0663afc8454db44bea4092fedff2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43b5f375adcc4b7d9a6d37115ff7fdfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8162b3a32223454fbb1af6f229a1fc06": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa9c3ab287014d64b6384783e257ea9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a3fba33e9c124748874cf7b844392693": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb21234130b443abb61c0b39142a14c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6e3c847411c24e03849fde5b6c166ba8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2815ac160df441099c47078a85979550",
              "IPY_MODEL_fd0a7e0e4a294b2ba681dde7aa359b6d",
              "IPY_MODEL_957f6e5dd2574e6a9e8990bf55b97ce2"
            ],
            "layout": "IPY_MODEL_dac7b94099c34ecab2a57f068652dfc4"
          }
        },
        "2815ac160df441099c47078a85979550": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fa4baa38361c4cc6a5a14d2c6cc83207",
            "placeholder": "​",
            "style": "IPY_MODEL_f41646ddd8fa4b9f96408f7193b159a2",
            "value": "generation_config.json: 100%"
          }
        },
        "fd0a7e0e4a294b2ba681dde7aa359b6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d3218c34e754df9899cff897607faf9",
            "max": 242,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_57b3c28c9ec1428089e577ba5f5e15c3",
            "value": 242
          }
        },
        "957f6e5dd2574e6a9e8990bf55b97ce2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f5f35bf822f45608f0f340b2a4826c8",
            "placeholder": "​",
            "style": "IPY_MODEL_362588e4197d4ce0bc8524f13b0b6cd0",
            "value": " 242/242 [00:00&lt;00:00, 17.3kB/s]"
          }
        },
        "dac7b94099c34ecab2a57f068652dfc4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa4baa38361c4cc6a5a14d2c6cc83207": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f41646ddd8fa4b9f96408f7193b159a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6d3218c34e754df9899cff897607faf9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57b3c28c9ec1428089e577ba5f5e15c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4f5f35bf822f45608f0f340b2a4826c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "362588e4197d4ce0bc8524f13b0b6cd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c2c4ce6f7ba44a83ad30cbf93e517a39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5ce2aebb0b624bed8fd5644aa4083f12",
              "IPY_MODEL_4965de5bb3dc45e1bfb680683163bf39",
              "IPY_MODEL_371cbe5ccc394f42b250aa3dfdc5c0d8"
            ],
            "layout": "IPY_MODEL_a8a0a110d5b449009494feeeb4b1c33d"
          }
        },
        "5ce2aebb0b624bed8fd5644aa4083f12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_459f4d4327ad40d1945071ab25d68b47",
            "placeholder": "​",
            "style": "IPY_MODEL_5e69bafe78f44f4e8985318aa9037fd3",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "4965de5bb3dc45e1bfb680683163bf39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_335620f25fbe41b8a0cd9c68be4fb056",
            "max": 1287,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_40500fe3a021491ea73e7df3d59f6ef9",
            "value": 1287
          }
        },
        "371cbe5ccc394f42b250aa3dfdc5c0d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53de218bb9f0496bb8d796190259a484",
            "placeholder": "​",
            "style": "IPY_MODEL_0cab8bf53e2e43be84c4a3a6314ac7b4",
            "value": " 1.29k/1.29k [00:00&lt;00:00, 46.1kB/s]"
          }
        },
        "a8a0a110d5b449009494feeeb4b1c33d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "459f4d4327ad40d1945071ab25d68b47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e69bafe78f44f4e8985318aa9037fd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "335620f25fbe41b8a0cd9c68be4fb056": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40500fe3a021491ea73e7df3d59f6ef9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "53de218bb9f0496bb8d796190259a484": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0cab8bf53e2e43be84c4a3a6314ac7b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a77e2933c65345f9b203ab982c3b0ed2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4fbbe512dae647a6ae4ea1efa8af3801",
              "IPY_MODEL_ac2a03537f344e09904650df2c76be5c",
              "IPY_MODEL_17ae356def2f4f1295af8f14f700fa6b"
            ],
            "layout": "IPY_MODEL_78639a2ed4374a848f8356501b950d9f"
          }
        },
        "4fbbe512dae647a6ae4ea1efa8af3801": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1055f60bb0394ce9b9bf29a97676ed72",
            "placeholder": "​",
            "style": "IPY_MODEL_298ab97f90da435ab8b90d7b33c0e6d6",
            "value": "vocab.json: 100%"
          }
        },
        "ac2a03537f344e09904650df2c76be5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc9c09ac71bd443499aeef5b122f946c",
            "max": 2776833,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e512f80800144e5c8bb61185f0a7296d",
            "value": 2776833
          }
        },
        "17ae356def2f4f1295af8f14f700fa6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1df98dc84ba34ed8ab52bc14b66bbf12",
            "placeholder": "​",
            "style": "IPY_MODEL_10360a5bcdaf4e05b2214233f5ee1082",
            "value": " 2.78M/2.78M [00:00&lt;00:00, 6.44MB/s]"
          }
        },
        "78639a2ed4374a848f8356501b950d9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1055f60bb0394ce9b9bf29a97676ed72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "298ab97f90da435ab8b90d7b33c0e6d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fc9c09ac71bd443499aeef5b122f946c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e512f80800144e5c8bb61185f0a7296d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1df98dc84ba34ed8ab52bc14b66bbf12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10360a5bcdaf4e05b2214233f5ee1082": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b2e5e901a73542ac8a023cef2bedc3ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fef911f0bf3c4a4ea0181076e624117d",
              "IPY_MODEL_af6ffa6caa084e04bcf48327ecff633b",
              "IPY_MODEL_d1f0e8f42e504b5cbfe43ee9c3aec5b7"
            ],
            "layout": "IPY_MODEL_f5e0adb86f634fb9b3db097be18c6edc"
          }
        },
        "fef911f0bf3c4a4ea0181076e624117d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4202c658a0774594a6929d1b4bc83828",
            "placeholder": "​",
            "style": "IPY_MODEL_378825f09e9c419ca48f7cc3dd6b5adc",
            "value": "merges.txt: 100%"
          }
        },
        "af6ffa6caa084e04bcf48327ecff633b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c10cc846da474dafaa7504c7389fb0b0",
            "max": 1671839,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8c91baf8340840e7ae8d58132356b579",
            "value": 1671839
          }
        },
        "d1f0e8f42e504b5cbfe43ee9c3aec5b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35d7d2f4101a49b9bd2bd7284f09db95",
            "placeholder": "​",
            "style": "IPY_MODEL_726f6a65d47b440c9914e758a809b077",
            "value": " 1.67M/1.67M [00:00&lt;00:00, 7.34MB/s]"
          }
        },
        "f5e0adb86f634fb9b3db097be18c6edc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4202c658a0774594a6929d1b4bc83828": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "378825f09e9c419ca48f7cc3dd6b5adc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c10cc846da474dafaa7504c7389fb0b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c91baf8340840e7ae8d58132356b579": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "35d7d2f4101a49b9bd2bd7284f09db95": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "726f6a65d47b440c9914e758a809b077": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "15ec1574b3934fe3b7bd25d2af54b2bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_de5d85bf4ab04a268ebfc91e51752e7f",
              "IPY_MODEL_9bf7b5392fff40f6a9259a59ff3ff2da",
              "IPY_MODEL_4e9c7e5e5b564ab1b511b73a7db884af"
            ],
            "layout": "IPY_MODEL_c4c2e1091d4541e8affb6358b32c167b"
          }
        },
        "de5d85bf4ab04a268ebfc91e51752e7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d7fe4713309b4aa99a83c4dc646ba9b4",
            "placeholder": "​",
            "style": "IPY_MODEL_f4638bc4e6a64629a0649d165f5314c4",
            "value": "tokenizer.json: 100%"
          }
        },
        "9bf7b5392fff40f6a9259a59ff3ff2da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7532414a4bbb46cc8370698aa5fc3168",
            "max": 7028015,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_835c31cc178040cca9dd62409d49d3f3",
            "value": 7028015
          }
        },
        "4e9c7e5e5b564ab1b511b73a7db884af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23314a2426d24dbab592054be867761b",
            "placeholder": "​",
            "style": "IPY_MODEL_365c41cf78784c6e968b507485e74003",
            "value": " 7.03M/7.03M [00:00&lt;00:00, 26.8MB/s]"
          }
        },
        "c4c2e1091d4541e8affb6358b32c167b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7fe4713309b4aa99a83c4dc646ba9b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4638bc4e6a64629a0649d165f5314c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7532414a4bbb46cc8370698aa5fc3168": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "835c31cc178040cca9dd62409d49d3f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "23314a2426d24dbab592054be867761b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "365c41cf78784c6e968b507485e74003": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "00c6084f152743ab9c615dde1e3bcbbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9399b95574274358bd23a9a6ea0907a7",
              "IPY_MODEL_1b7f628d3f9449f7b18d99cea23a4cbe",
              "IPY_MODEL_66b03eee840f4fee919330f4b827fc89"
            ],
            "layout": "IPY_MODEL_f99c4a9f3e044ccfa1b127bb59685648"
          }
        },
        "9399b95574274358bd23a9a6ea0907a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a923b488c1e46e3809ae1f0e29ae628",
            "placeholder": "​",
            "style": "IPY_MODEL_b7c0146733414eab9669088d609c79a7",
            "value": "config.json: 100%"
          }
        },
        "1b7f628d3f9449f7b18d99cea23a4cbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_60e7fe8d95a04b1db4c0c8ca05d59a25",
            "max": 661,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2c9ca585bc4f46928b14e2226aed85b0",
            "value": 661
          }
        },
        "66b03eee840f4fee919330f4b827fc89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a2a02f7fa0f2451eb84a4789cee60cef",
            "placeholder": "​",
            "style": "IPY_MODEL_dced3593c4864e85b0952b60cf111974",
            "value": " 661/661 [00:00&lt;00:00, 30.9kB/s]"
          }
        },
        "f99c4a9f3e044ccfa1b127bb59685648": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a923b488c1e46e3809ae1f0e29ae628": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7c0146733414eab9669088d609c79a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "60e7fe8d95a04b1db4c0c8ca05d59a25": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c9ca585bc4f46928b14e2226aed85b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a2a02f7fa0f2451eb84a4789cee60cef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dced3593c4864e85b0952b60cf111974": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8ffeaec3d81747eeafd701505c8e2e38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ea33c278c6684419ae1195ac8626fb01",
              "IPY_MODEL_d64a5c9f4d5644db8e58176e6f002235",
              "IPY_MODEL_55287b9957e14725af071b4abdd472be"
            ],
            "layout": "IPY_MODEL_a0ce9fe9de114aeeb047bc7d5a80b2e8"
          }
        },
        "ea33c278c6684419ae1195ac8626fb01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4593a0ca90344f8a1b0e759a376afac",
            "placeholder": "​",
            "style": "IPY_MODEL_105c64c6e7224d9d85242339548b86aa",
            "value": "model.safetensors: 100%"
          }
        },
        "d64a5c9f4d5644db8e58176e6f002235": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_72c520d7b39d4d45a318e460b18fcdc1",
            "max": 1239173352,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8fa6adc6b7b64c0db191dccc437c51f2",
            "value": 1239173352
          }
        },
        "55287b9957e14725af071b4abdd472be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e3f3c18d54eb48459f3325e5a4a424e5",
            "placeholder": "​",
            "style": "IPY_MODEL_c56f014dd292477d91bf60af19614d99",
            "value": " 1.24G/1.24G [00:06&lt;00:00, 164MB/s]"
          }
        },
        "a0ce9fe9de114aeeb047bc7d5a80b2e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4593a0ca90344f8a1b0e759a376afac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "105c64c6e7224d9d85242339548b86aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "72c520d7b39d4d45a318e460b18fcdc1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8fa6adc6b7b64c0db191dccc437c51f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e3f3c18d54eb48459f3325e5a4a424e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c56f014dd292477d91bf60af19614d99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "259acd8944d14a20a146cca6cca21e13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_667af96db43542d1aaa31e4b17a3be21",
              "IPY_MODEL_bc35496d55994b71aca014d5a41cbbc3",
              "IPY_MODEL_7d12cfab7edd40f5a4ba2b8bc257ace3"
            ],
            "layout": "IPY_MODEL_1f220c2297b848fc9a9cf344debb4722"
          }
        },
        "667af96db43542d1aaa31e4b17a3be21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f48bf2484074471988847e34177e6d8",
            "placeholder": "​",
            "style": "IPY_MODEL_9e9d848e25554fb5983cc198e447e406",
            "value": "generation_config.json: 100%"
          }
        },
        "bc35496d55994b71aca014d5a41cbbc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_349ac936c6294a0dbf69db6992b2e8ea",
            "max": 206,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ec822aad480d496da18c53d9e57586ad",
            "value": 206
          }
        },
        "7d12cfab7edd40f5a4ba2b8bc257ace3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5634a99f3c1c45d8806042b4576f3478",
            "placeholder": "​",
            "style": "IPY_MODEL_06972232ea164adb94866ffcaedeacd5",
            "value": " 206/206 [00:00&lt;00:00, 21.2kB/s]"
          }
        },
        "1f220c2297b848fc9a9cf344debb4722": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f48bf2484074471988847e34177e6d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e9d848e25554fb5983cc198e447e406": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "349ac936c6294a0dbf69db6992b2e8ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec822aad480d496da18c53d9e57586ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5634a99f3c1c45d8806042b4576f3478": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06972232ea164adb94866ffcaedeacd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3c9a75ec91784893a2266da372a7fdae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_62dfc978f3d84404b8fc1ca5c50c5546",
              "IPY_MODEL_6b47ae5b03434bd9856cbb564adfc308",
              "IPY_MODEL_d1037115b07c4bdb977f29569ef8bd1c"
            ],
            "layout": "IPY_MODEL_a7f81cd0beae49f78a6d7429382ac765"
          }
        },
        "62dfc978f3d84404b8fc1ca5c50c5546": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_223a0cc24c8345bdb4162a6a9aefc00f",
            "placeholder": "​",
            "style": "IPY_MODEL_9ba404b7a9fc4dae92e0270210761c78",
            "value": "Generating train split: "
          }
        },
        "6b47ae5b03434bd9856cbb564adfc308": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ec2baa0334b48c9bdf455885d0b8a79",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_31d6dfa482cd4f32a9473953520eb649",
            "value": 1
          }
        },
        "d1037115b07c4bdb977f29569ef8bd1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6fdfbdd3fdd40f89d6db79edc8b6eb4",
            "placeholder": "​",
            "style": "IPY_MODEL_ab8b9e860e03493fad87b5249bbaa6a8",
            "value": " 2/0 [00:00&lt;00:00, 103.32 examples/s]"
          }
        },
        "a7f81cd0beae49f78a6d7429382ac765": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "223a0cc24c8345bdb4162a6a9aefc00f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ba404b7a9fc4dae92e0270210761c78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1ec2baa0334b48c9bdf455885d0b8a79": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "31d6dfa482cd4f32a9473953520eb649": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d6fdfbdd3fdd40f89d6db79edc8b6eb4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab8b9e860e03493fad87b5249bbaa6a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4f8260cacf0e40889ce8d44d85aa32d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2521f657376844c8ba8219b27aa4125d",
              "IPY_MODEL_28a5a88553ad4af1a12540155a861325",
              "IPY_MODEL_30ff21d915604c30bbaee2ad479a5ebc"
            ],
            "layout": "IPY_MODEL_b9ef314febd84ce7a27631f6a6747275"
          }
        },
        "2521f657376844c8ba8219b27aa4125d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_227b059ddd8e4d38a084e27cdce4461f",
            "placeholder": "​",
            "style": "IPY_MODEL_eaf9e67f49574fbb92c89061bc350225",
            "value": "Generating train split: "
          }
        },
        "28a5a88553ad4af1a12540155a861325": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6de8cd21e383418eaebd14404f8011d6",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_826bcca60c8a45fd9ab54f5dcdde801a",
            "value": 1
          }
        },
        "30ff21d915604c30bbaee2ad479a5ebc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cde77c2202e442809ccb6c52976e186a",
            "placeholder": "​",
            "style": "IPY_MODEL_9dc4d9d549ae499d98916b14e459d863",
            "value": " 483/0 [00:00&lt;00:00, 17524.80 examples/s]"
          }
        },
        "b9ef314febd84ce7a27631f6a6747275": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "227b059ddd8e4d38a084e27cdce4461f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eaf9e67f49574fbb92c89061bc350225": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6de8cd21e383418eaebd14404f8011d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "826bcca60c8a45fd9ab54f5dcdde801a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cde77c2202e442809ccb6c52976e186a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9dc4d9d549ae499d98916b14e459d863": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5cadb84487d84557ab9aaf9fa4f0fc29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_947ef962eaac4859ab902975b322b735",
              "IPY_MODEL_fae2691374ee436e9c8838fff9b71ad8",
              "IPY_MODEL_ec45e480698c48229f418ffd93f8d02d"
            ],
            "layout": "IPY_MODEL_099f6d29f2b846e39f27f0f7ea333037"
          }
        },
        "947ef962eaac4859ab902975b322b735": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e076a64c972f4b6990ba89d224bc41aa",
            "placeholder": "​",
            "style": "IPY_MODEL_859d729d2add4bab98f56e47297c53c1",
            "value": "Map: 100%"
          }
        },
        "fae2691374ee436e9c8838fff9b71ad8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9db879c83b343259475849a9ea99531",
            "max": 483,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_965a60b985fe4eb1b143ba21ce6b3530",
            "value": 483
          }
        },
        "ec45e480698c48229f418ffd93f8d02d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65af1e0afd6c4cf5bc3a87829c10fd81",
            "placeholder": "​",
            "style": "IPY_MODEL_ce94124d4f6e4208ac489391ff4e5a32",
            "value": " 483/483 [00:00&lt;00:00, 1058.85 examples/s]"
          }
        },
        "099f6d29f2b846e39f27f0f7ea333037": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e076a64c972f4b6990ba89d224bc41aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "859d729d2add4bab98f56e47297c53c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c9db879c83b343259475849a9ea99531": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "965a60b985fe4eb1b143ba21ce6b3530": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "65af1e0afd6c4cf5bc3a87829c10fd81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce94124d4f6e4208ac489391ff4e5a32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7e735713c3984c69be2c041ba73d39e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6cb2e50921e7447e89eb98d1e2f3f9c3",
              "IPY_MODEL_eed0f7f2590f45bbae487a0865b6f2c0",
              "IPY_MODEL_b80ad5e11469408b84d318d692aa75a5"
            ],
            "layout": "IPY_MODEL_dd07033057144a2cb07e19857e65592e"
          }
        },
        "6cb2e50921e7447e89eb98d1e2f3f9c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f366de9a2f3043b3b82513bde1ab3e48",
            "placeholder": "​",
            "style": "IPY_MODEL_bdee79ee09a942eb8b8e312dc72b8e3e",
            "value": "Generating train split: "
          }
        },
        "eed0f7f2590f45bbae487a0865b6f2c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3bf0081155244275be751e34c0ab7b1e",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8845ba76fa8a4985b942370a10acacfc",
            "value": 1
          }
        },
        "b80ad5e11469408b84d318d692aa75a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9e5848616304fd383309cd5e4a8ae30",
            "placeholder": "​",
            "style": "IPY_MODEL_bd728393791c455180e2ff318364e060",
            "value": " 100/0 [00:00&lt;00:00, 3483.64 examples/s]"
          }
        },
        "dd07033057144a2cb07e19857e65592e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f366de9a2f3043b3b82513bde1ab3e48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bdee79ee09a942eb8b8e312dc72b8e3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3bf0081155244275be751e34c0ab7b1e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "8845ba76fa8a4985b942370a10acacfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b9e5848616304fd383309cd5e4a8ae30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd728393791c455180e2ff318364e060": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "64bc88e0e13e49b19076817704184fe4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8c0b651f64bb497f8c10302aef7572e4",
              "IPY_MODEL_ba27c7a704274be59de6e6177be5b2de",
              "IPY_MODEL_67d0f23567264882841e77ae6bf79dca"
            ],
            "layout": "IPY_MODEL_1d6d8c2021b54920af72755da7b912c9"
          }
        },
        "8c0b651f64bb497f8c10302aef7572e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b8ab05ae3b946dd8bb055b961f9f607",
            "placeholder": "​",
            "style": "IPY_MODEL_b72c4d62bb964543a45090e73fd7b24e",
            "value": "Converting train dataset to ChatML: 100%"
          }
        },
        "ba27c7a704274be59de6e6177be5b2de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85aff509efff4be5ab1d1b1dd7857721",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2f3bb4d07f504356833e1f4370561da9",
            "value": 100
          }
        },
        "67d0f23567264882841e77ae6bf79dca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c76ca220b36414b8ae8ae0b2ff681a4",
            "placeholder": "​",
            "style": "IPY_MODEL_ee15ad838d72465a9fa9c3028a163c9c",
            "value": " 100/100 [00:00&lt;00:00, 2921.58 examples/s]"
          }
        },
        "1d6d8c2021b54920af72755da7b912c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b8ab05ae3b946dd8bb055b961f9f607": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b72c4d62bb964543a45090e73fd7b24e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "85aff509efff4be5ab1d1b1dd7857721": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f3bb4d07f504356833e1f4370561da9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8c76ca220b36414b8ae8ae0b2ff681a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee15ad838d72465a9fa9c3028a163c9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "64e4469a06fb4a91b1274965ddeaee35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_782f6a89987740d9942decd42b0200c1",
              "IPY_MODEL_f16058a451d741a8a4ff986aa464842e",
              "IPY_MODEL_cfa2893b1e6948b89bd762bc44e9c7c4"
            ],
            "layout": "IPY_MODEL_588a4aa7b74b4f91a4823cba21be8866"
          }
        },
        "782f6a89987740d9942decd42b0200c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b275c7b92bf6460cae23817bb0a4941b",
            "placeholder": "​",
            "style": "IPY_MODEL_c58414fc50ec4cc48ea02f47823cd23c",
            "value": "Adding EOS to train dataset: 100%"
          }
        },
        "f16058a451d741a8a4ff986aa464842e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fdebd8916b64440394f523aac014ded9",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d09069c89a164482a7f9cafc7f8932ab",
            "value": 100
          }
        },
        "cfa2893b1e6948b89bd762bc44e9c7c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bdfdff265e184d7e8de519b9ab16beec",
            "placeholder": "​",
            "style": "IPY_MODEL_5c6f7c0da7e547e7a1b767fe684d3445",
            "value": " 100/100 [00:00&lt;00:00, 3344.23 examples/s]"
          }
        },
        "588a4aa7b74b4f91a4823cba21be8866": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b275c7b92bf6460cae23817bb0a4941b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c58414fc50ec4cc48ea02f47823cd23c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fdebd8916b64440394f523aac014ded9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d09069c89a164482a7f9cafc7f8932ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bdfdff265e184d7e8de519b9ab16beec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c6f7c0da7e547e7a1b767fe684d3445": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "67833c3d4ef348abbb9a80ad7396dcb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d9da1eb7ed9e4779b7516b87329298f7",
              "IPY_MODEL_7ecfc9d360514d6f992ca56b61e684a2",
              "IPY_MODEL_e8cfd6e6fff04981aad6e7773c9dc3fb"
            ],
            "layout": "IPY_MODEL_48d1859165884085bf8ec867aa7a7ca8"
          }
        },
        "d9da1eb7ed9e4779b7516b87329298f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc12a5058f32465da60aaad3ea7c7cc1",
            "placeholder": "​",
            "style": "IPY_MODEL_55300839c10a413ba6915aa7d08153fb",
            "value": "Tokenizing train dataset: 100%"
          }
        },
        "7ecfc9d360514d6f992ca56b61e684a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a12859cb0c3642279df729f623af6207",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ff2eb0a63234430c9768e720e0d4ed9f",
            "value": 100
          }
        },
        "e8cfd6e6fff04981aad6e7773c9dc3fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ee8aa63e095408d8b22dce9a52cc519",
            "placeholder": "​",
            "style": "IPY_MODEL_502220cc7d8b466aa4617ee0f13cb1de",
            "value": " 100/100 [00:00&lt;00:00, 171.11 examples/s]"
          }
        },
        "48d1859165884085bf8ec867aa7a7ca8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc12a5058f32465da60aaad3ea7c7cc1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55300839c10a413ba6915aa7d08153fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a12859cb0c3642279df729f623af6207": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff2eb0a63234430c9768e720e0d4ed9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6ee8aa63e095408d8b22dce9a52cc519": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "502220cc7d8b466aa4617ee0f13cb1de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7455f38d60d647a0afda0c910b88daad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dd5e2af6eb2e48a3bdcb580aadc49ed9",
              "IPY_MODEL_3d7d6ffb80d747488197e8014f4d7ce4",
              "IPY_MODEL_435d4cabc63d4ce587a1d8590b793359"
            ],
            "layout": "IPY_MODEL_a81ce26c129d476591af5e2dc34d1730"
          }
        },
        "dd5e2af6eb2e48a3bdcb580aadc49ed9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3904bb68392149029b77d4e2c766620a",
            "placeholder": "​",
            "style": "IPY_MODEL_126bf658920c4afaa81620216286b4af",
            "value": "Truncating train dataset: 100%"
          }
        },
        "3d7d6ffb80d747488197e8014f4d7ce4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29652c3442b0405cb09f23171f1af53e",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_22e35192e85e42028e0aa9b647c9f5fc",
            "value": 100
          }
        },
        "435d4cabc63d4ce587a1d8590b793359": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b7b80513044f4ada9f55578ab32697fd",
            "placeholder": "​",
            "style": "IPY_MODEL_dd63de0f24f948c5bb9ad9e537f96e40",
            "value": " 100/100 [00:00&lt;00:00, 2676.95 examples/s]"
          }
        },
        "a81ce26c129d476591af5e2dc34d1730": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3904bb68392149029b77d4e2c766620a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "126bf658920c4afaa81620216286b4af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "29652c3442b0405cb09f23171f1af53e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22e35192e85e42028e0aa9b647c9f5fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b7b80513044f4ada9f55578ab32697fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd63de0f24f948c5bb9ad9e537f96e40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "26c7014c8b5b462fb2d69846742287ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0d55c0f804ab4f30ba2a72c7fe6c9fbf",
              "IPY_MODEL_75294f86ab4444b48f66ecce68dd721d",
              "IPY_MODEL_f0c6f11b5070433bb16067d65513e844"
            ],
            "layout": "IPY_MODEL_d53d85a8ddb84a86b821b81037621f39"
          }
        },
        "0d55c0f804ab4f30ba2a72c7fe6c9fbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06d1deb6bb7b49f7a779ece734361af7",
            "placeholder": "​",
            "style": "IPY_MODEL_abe90b35ca4c496b86f961d39735be73",
            "value": "Generating train split: "
          }
        },
        "75294f86ab4444b48f66ecce68dd721d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a0e698746294b738a70dc1a7182e86c",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_016888f0dc6140c7992ae6d2f2984965",
            "value": 1
          }
        },
        "f0c6f11b5070433bb16067d65513e844": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71d2f8d3238b485ebc084309645fa034",
            "placeholder": "​",
            "style": "IPY_MODEL_d09d30160cb1403bba84a40acfb49c78",
            "value": " 200/0 [00:00&lt;00:00, 8251.23 examples/s]"
          }
        },
        "d53d85a8ddb84a86b821b81037621f39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06d1deb6bb7b49f7a779ece734361af7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "abe90b35ca4c496b86f961d39735be73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3a0e698746294b738a70dc1a7182e86c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "016888f0dc6140c7992ae6d2f2984965": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "71d2f8d3238b485ebc084309645fa034": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d09d30160cb1403bba84a40acfb49c78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6a884ba704534f66b4ea06fee49a9755": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b124b01bd6464fc2ab947f248a1ee265",
              "IPY_MODEL_52182d5e0a47410bb7a6476f04c3d89e",
              "IPY_MODEL_44fdcbfcead74027a00ab40df938ff8b"
            ],
            "layout": "IPY_MODEL_046ae726716e4e3fb9c7d9a91397f282"
          }
        },
        "b124b01bd6464fc2ab947f248a1ee265": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e7adde370da4a3684f78df5aa92e086",
            "placeholder": "​",
            "style": "IPY_MODEL_7e887ef21a084a9fbbd3171def60d7ed",
            "value": "Converting train dataset to ChatML: 100%"
          }
        },
        "52182d5e0a47410bb7a6476f04c3d89e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e8988fa14614a7891ad7dd2d48ee7ec",
            "max": 200,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4820adf77f7a42e7baffc541f7086ebf",
            "value": 200
          }
        },
        "44fdcbfcead74027a00ab40df938ff8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_459a70aa61d34616bf293ed24ecb1d4f",
            "placeholder": "​",
            "style": "IPY_MODEL_585e24f4da0848fb99c6f6a60ff695c3",
            "value": " 200/200 [00:00&lt;00:00, 5548.39 examples/s]"
          }
        },
        "046ae726716e4e3fb9c7d9a91397f282": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e7adde370da4a3684f78df5aa92e086": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e887ef21a084a9fbbd3171def60d7ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4e8988fa14614a7891ad7dd2d48ee7ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4820adf77f7a42e7baffc541f7086ebf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "459a70aa61d34616bf293ed24ecb1d4f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "585e24f4da0848fb99c6f6a60ff695c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6791c2b7724f4cc3a1fd714838f28412": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5331e6e182e341e88e9005ab2deef986",
              "IPY_MODEL_d5c31b762f9445ae98345d5f1e937c3d",
              "IPY_MODEL_eec2b6e051c2459c8d7256a66ac96d50"
            ],
            "layout": "IPY_MODEL_6285aec7e81e4566bedc28cadff44c4a"
          }
        },
        "5331e6e182e341e88e9005ab2deef986": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_baaaf3b1b297472d8da9d7726b1d7bec",
            "placeholder": "​",
            "style": "IPY_MODEL_7ce2f713594a4854b85ae5d11cd484fb",
            "value": "Adding EOS to train dataset: 100%"
          }
        },
        "d5c31b762f9445ae98345d5f1e937c3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf549ed91a274a1b86f25139542dede8",
            "max": 200,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9e1b933eaa634586935e84f1e8abe781",
            "value": 200
          }
        },
        "eec2b6e051c2459c8d7256a66ac96d50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_beb917b64e7049ecaca5931be663ed59",
            "placeholder": "​",
            "style": "IPY_MODEL_34f2a509663544f8a5fa282df9520ff2",
            "value": " 200/200 [00:00&lt;00:00, 5812.59 examples/s]"
          }
        },
        "6285aec7e81e4566bedc28cadff44c4a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "baaaf3b1b297472d8da9d7726b1d7bec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ce2f713594a4854b85ae5d11cd484fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cf549ed91a274a1b86f25139542dede8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e1b933eaa634586935e84f1e8abe781": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "beb917b64e7049ecaca5931be663ed59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34f2a509663544f8a5fa282df9520ff2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d65040eb96af450cbf65953254e6dbcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bca15887a83f40929efb489a3e7d8c9c",
              "IPY_MODEL_a764d35670044e23a353bcfb0049c36d",
              "IPY_MODEL_6621de102bee44898e788c4a73bfdf83"
            ],
            "layout": "IPY_MODEL_95b60879e80e4d97966cbd24564b6c99"
          }
        },
        "bca15887a83f40929efb489a3e7d8c9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2450026d7b1743cd8419434a939b9541",
            "placeholder": "​",
            "style": "IPY_MODEL_53d1f66bf5614926a4d74c035d85f05a",
            "value": "Tokenizing train dataset: 100%"
          }
        },
        "a764d35670044e23a353bcfb0049c36d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82959c4c90e6485694b9c22b7dfe6b4c",
            "max": 200,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aa14e53c3e624980a6e7d62ae4cf1682",
            "value": 200
          }
        },
        "6621de102bee44898e788c4a73bfdf83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_afde729678cf4694b87c10ba73e748d8",
            "placeholder": "​",
            "style": "IPY_MODEL_c772025c55ed441f82d90d06d01abb92",
            "value": " 200/200 [00:01&lt;00:00, 203.78 examples/s]"
          }
        },
        "95b60879e80e4d97966cbd24564b6c99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2450026d7b1743cd8419434a939b9541": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53d1f66bf5614926a4d74c035d85f05a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "82959c4c90e6485694b9c22b7dfe6b4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa14e53c3e624980a6e7d62ae4cf1682": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "afde729678cf4694b87c10ba73e748d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c772025c55ed441f82d90d06d01abb92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0f46c20cb4aa4e7a97bdcd009e72439c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_67cf449fdb20440da175060044121477",
              "IPY_MODEL_1d0680d89ea148eb9062c8d2ec1c18e1",
              "IPY_MODEL_1aba49649c1344a687e5c28d2f8c1015"
            ],
            "layout": "IPY_MODEL_ccf5aad63b5141b597238b58d6df23fd"
          }
        },
        "67cf449fdb20440da175060044121477": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d56dcbd7b27048bcba83553588f38ffe",
            "placeholder": "​",
            "style": "IPY_MODEL_efe3b96ac3d84566a3b564666b9c1d21",
            "value": "Truncating train dataset: 100%"
          }
        },
        "1d0680d89ea148eb9062c8d2ec1c18e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eebffaea8bfa49f58893c20ba90eb87b",
            "max": 200,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_082d7fdbb5b14c6fb25122ec820e60ef",
            "value": 200
          }
        },
        "1aba49649c1344a687e5c28d2f8c1015": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e2499a5c5ff4351b325e8457cc7a71d",
            "placeholder": "​",
            "style": "IPY_MODEL_c1797e5dfeaa4ff9aae1cdc775d3f086",
            "value": " 200/200 [00:00&lt;00:00, 8015.72 examples/s]"
          }
        },
        "ccf5aad63b5141b597238b58d6df23fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d56dcbd7b27048bcba83553588f38ffe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "efe3b96ac3d84566a3b564666b9c1d21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eebffaea8bfa49f58893c20ba90eb87b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "082d7fdbb5b14c6fb25122ec820e60ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0e2499a5c5ff4351b325e8457cc7a71d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1797e5dfeaa4ff9aae1cdc775d3f086": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErGluGmFqEci",
        "outputId": "3db7d1f3-591f-47ed-f86e-86e4a3284be5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/348.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m337.9/348.0 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m348.0/348.0 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/207.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m92.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.5/144.5 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install  -U -q trl peft math_verify\n",
        "# Tested with transformers==4.47.1, trl==0.14.0, datasets==3.2.0, peft==0.14.0, accelerate==1.2.1, math_verify==0.3.3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n"
      ],
      "metadata": {
        "id": "U4P1a6YBqOKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset_id = \"AI-MO/NuminaMath-TIR\"\n",
        "train_dataset, test_dataset = load_dataset(dataset_id, split=[\"train[:5%]\", \"test[:5%]\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301,
          "referenced_widgets": [
            "cb1921306ae64ef198df4544e79d4918",
            "4d8d81e0389d4cfe8f1ec832dfb822df",
            "128885171f9543c283e83f1fdc2e1cee",
            "78c965cb33e54f518dfcb5342d7ef969",
            "b9717ed4ae0b43eebf1d4b9b3eb9841e",
            "6000d5ad5143458bbba4e7c4d11465fd",
            "f71d70acba0143bdbfae388e18a0a2ef",
            "06a8289bb4e7491588ee876537cebc93",
            "a4408536e45244bfb937e10a9d0d3fe8",
            "a336d43b8b014cc9aea08609df742431",
            "15baa1367faa4bb694f53dc6a76cc096",
            "b9ffb48c0ff14e39af235c5bfb1bb7eb",
            "b01dd4545d0045a4bcc26ff5a1dff3d5",
            "aabb54edec334789a9e77384b61a2455",
            "fb29a83efebf4c12bdb36187fe9b8b9f",
            "ea4496d9cdda4ac19fda0c41b34d607b",
            "28c36fa858cd49749281f7f70d582505",
            "97579dd87ea64cd59b3029b9d821e36c",
            "9aeac10078f94da29cfe7052b6054d38",
            "6dfe36cec84d46be8b4a06418d456629",
            "2a18a7589ff34703a4bc33f15b643071",
            "09f7236c602e4581921af9b8f03001fe",
            "3eb04fc58ab44a40949e9511c4c25417",
            "caad1b4e1519403cb855c5b5fd0fa735",
            "200573ab1f814d988e128eba43e7e001",
            "234336b2a5d34089946cddab0d0bc930",
            "55f85ce4885e4296a670413ae0e37708",
            "56a30f91680546e38ebfaf25ee16dd9c",
            "944e143b03254b2483b01d7bbdbe1299",
            "342d6197ffb6478aa36b08f9a887e05d",
            "a5e2a429315345bb94340153b2463edc",
            "dc258216ca4a42888d7bc6249917e970",
            "bc02ff06a2834903a4ba8eead439077a",
            "123a4ff192394a479cdd2e2a232b7c36",
            "f847df33e7bb40fabe667148ef831af9",
            "f42d1dc03fe543d99af507d92bb49edc",
            "77fe904174304adeb0edb7cced2c561b",
            "84f8f536421b4c84869e1e1c79d0df49",
            "876759e08ed14d1184311ec6cfd04ed0",
            "51cca855eca8497d87964df281478ef8",
            "6dfd00330cb64fd081a18adc8718e171",
            "f49cb23031114641b05a79f20d546a91",
            "b736529a7ef2445db7e42c1f5d763a82",
            "b0810095e4d14f9997540cc9a7a87c9a",
            "27b9e17be73744cda1a9a6f232a870ee",
            "9c71bb156b9c425183a2235e277be14f",
            "56137c3f50044ab2826cb3c45422cd2b",
            "30a84f0374e44f16b22da827e969ed2b",
            "10c7b950df3d409b945361d7f7fc5e11",
            "ac05924d03814993a21a9d9468e99aea",
            "475cc6a7a4094bdab6729e1b9619a89e",
            "938808caca1f462e9b0eb5d27abcec2e",
            "95bc4e8e76dd410783f904b9bd06a071",
            "82ea55b2d06b4a4a89d5a8702fcfb680",
            "ce3b8763d91c477da577022043f2f5bd"
          ]
        },
        "id": "EtAOfjHhqUPb",
        "outputId": "8bbcf8f9-5917-4e85-ca6e-37a970874d52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/2.43k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cb1921306ae64ef198df4544e79d4918"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00001.parquet:   0%|          | 0.00/147M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b9ffb48c0ff14e39af235c5bfb1bb7eb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "test-00000-of-00001.parquet:   0%|          | 0.00/215k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3eb04fc58ab44a40949e9511c4c25417"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/72441 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "123a4ff192394a479cdd2e2a232b7c36"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/99 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "27b9e17be73744cda1a9a6f232a870ee"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lI35CGVLqVz_",
        "outputId": "40f8ed18-7071-49a1-d2c7-fd16bf252572"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['problem', 'solution', 'messages'],\n",
            "    num_rows: 3622\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SYSTEM_PROMPT = (\n",
        "    \"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant \"\n",
        "    \"first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning \"\n",
        "    \"process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., \"\n",
        "    \"<think> reasoning process here </think><answer> answer here </answer>\"\n",
        ")\n",
        "\n",
        "\n",
        "def make_conversation(example):\n",
        "    return {\n",
        "        \"prompt\": [\n",
        "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "            {\"role\": \"user\", \"content\": example[\"problem\"]},\n",
        "        ],\n",
        "    }\n",
        "\n",
        "\n",
        "train_dataset = train_dataset.map(make_conversation)\n",
        "test_dataset = test_dataset.map(make_conversation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "798b8ef3f3ec4f82982673cd3af4ad93",
            "b63a9b0b53b04b3f88c44e900c302c3c",
            "36f9e28a2bce4f7594098364840c44cd",
            "528d1547ea7c4449939f92d03acde0df",
            "5c7b86492565452393b6dd8e5a2ff216",
            "54e0d4d738a3404db15738a42819e85c",
            "609853e5bd1547b8b28d6a80d0f4e8ba",
            "c596279f7b274e3281951575d39ea106",
            "d5e0c9f1f7a84208a8631b06ead5a710",
            "a5fc0b67dbac48d1ab21d131e0bd9abc",
            "113e0b95e84e48aa948257788419a445",
            "b44c75a72dcf40419975b42379cd2452",
            "2eb35ea1e6784e38bf5c8647cbb35775",
            "1f0620475fc44c20b033ad4cf8422fac",
            "0d35c6ff25154dd08f21a562d8d747db",
            "aa39681f075a465a9d22e3406ab76775",
            "7cadebb3f5c74381bf222933af4d3a3a",
            "25cdb322c88341e4b0f921a9aec87014",
            "d99dc32a167e49ad85a369b213bc0a94",
            "53110aa4254c46d3b60da27a00a2dc70",
            "f3a29670465c4ad08cbfdec472824d84",
            "67124d81db6c45239f5fd7d33c44c1fb"
          ]
        },
        "id": "029Dmku2qapL",
        "outputId": "f8e67a1c-2d85-4b38-8bbe-28e917da41c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3622 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "798b8ef3f3ec4f82982673cd3af4ad93"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b44c75a72dcf40419975b42379cd2452"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = train_dataset.remove_columns([\"messages\", \"problem\"])\n",
        "print(train_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sC9hWftPqgOU",
        "outputId": "85aac141-1ac3-4897-cc6c-f1cc6b11156c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['solution', 'prompt'],\n",
            "    num_rows: 3622\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "model_id = \"Qwen/Qwen2-0.5B-Instruct\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130,
          "referenced_widgets": [
            "32a564ac6b08404ab5280b8753b2051a",
            "68e84ee2761c46888aa3e4062c14bfe4",
            "b9ca785e6d5c4b5c819f85c575425362",
            "e7d22f83d6054a33a8d2714ba9895449",
            "c719d6b162364eeeb6fa396c7436a073",
            "ad2a35b006d246e9bcec994a12f3638f",
            "bcb98601d80b4669baaa0843f046c9ac",
            "9ccdd48e2be347daa5c158a7e8712ff7",
            "f17ad365ca6f44499bb8aa9c0b78d75b",
            "34f19cbff346461aa114ed3e239367f2",
            "1aa875dde21b46a6b19ec78370a4cda7",
            "cdb7a92b8eac4fbf828fbd2619fd83bb",
            "708d3a5904db4ac1ad86757429d0df97",
            "e4ecbfbdcacf48cfa760777d418aa4f3",
            "1803021b8fcd43da8656ce492ed0280c",
            "e24d30dd77e24c84b21d5245992eaba9",
            "1c92948efe8242de8723e81c3ad95413",
            "2911698e2e5743698a96893259b54b51",
            "07d7f4dd7cbc4c96a3ffc196440655aa",
            "ef26cde9271b4edcbb1a2e9615b1cbef",
            "b2a7fc4932ef4061b9c443cde0381eb6",
            "e75eb6a232dd47bc90db1fd80e1cea04",
            "2baaeec2bda942c2af4a92a6b4830da4",
            "d52fca5057b54ce3a1f291d91e795099",
            "f2a52b52b98b4194ba6614a604358fd0",
            "d349f0eac647402383b7bf349024cc6c",
            "643ea45236a54e95a1a943c358fb19cc",
            "31092e224db64f40bd1b22fe90eabc64",
            "a1b1f41f5b1f4e2aaadee6585ead9ea4",
            "bf2b64c48ceb43e5bc33ac9422343f08",
            "a61dd72a85914e99926f7499b6ad935d",
            "5a98e352114144379c146ea78bd4911a",
            "11a81f05880b42fa84145f17770b3916"
          ]
        },
        "id": "mTNmF1Rtqjw2",
        "outputId": "9650a39e-b502-49fb-a822-d2c86ff20359"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/659 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "32a564ac6b08404ab5280b8753b2051a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/988M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cdb7a92b8eac4fbf828fbd2619fd83bb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2baaeec2bda942c2af4a92a6b4830da4"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-SGGQ0GqwCD",
        "outputId": "149d2129-b8d4-42a6-ee93-d8a72dac4daa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 540,672 || all params: 494,573,440 || trainable%: 0.1093\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "\n",
        "def format_reward(completions, **kwargs):\n",
        "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
        "    pattern = r\"^<think>.*?</think>\\s*<answer>.*?</answer>$\"\n",
        "    completion_contents = [completion[0][\"content\"] for completion in completions]\n",
        "    matches = [re.match(pattern, content) for content in completion_contents]\n",
        "    rewards_list = [1.0 if match else 0.0 for match in matches]\n",
        "    return [1.0 if match else 0.0 for match in matches]"
      ],
      "metadata": {
        "id": "P4wL1QnEq2VK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from math_verify import LatexExtractionConfig, parse, verify\n",
        "\n",
        "\n",
        "def accuracy_reward(completions, **kwargs):\n",
        "    \"\"\"Reward function that checks if the completion is the same as the ground truth.\"\"\"\n",
        "    solutions = kwargs[\"solution\"]\n",
        "    completion_contents = [completion[0][\"content\"] for completion in completions]\n",
        "    rewards = []\n",
        "    for content, solution in zip(completion_contents, solutions):\n",
        "        gold_parsed = parse(solution, extraction_mode=\"first_match\", extraction_config=[LatexExtractionConfig()])\n",
        "        answer_parsed = parse(content, extraction_mode=\"first_match\", extraction_config=[LatexExtractionConfig()])\n",
        "        if len(gold_parsed) != 0:\n",
        "            try:\n",
        "                rewards.append(float(verify(answer_parsed, gold_parsed)))\n",
        "            except Exception:\n",
        "                rewards.append(0.0)\n",
        "        else:\n",
        "            rewards.append(1.0)\n",
        "    return rewards"
      ],
      "metadata": {
        "id": "v2szJGXmq-iH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import GRPOConfig\n",
        "\n",
        "# Configure training arguments using GRPOConfig\n",
        "training_args = GRPOConfig(\n",
        "    output_dir=\"Qwen2-0.5B-GRPO-test\",\n",
        "    learning_rate=1e-5,\n",
        "    remove_unused_columns=False,  # to access the solution column in accuracy_reward\n",
        "    gradient_accumulation_steps=16,\n",
        "    num_train_epochs=1,\n",
        "    bf16=True,\n",
        "    # Parameters that control de data preprocessing\n",
        "    max_completion_length=64,  # default: 256\n",
        "    num_generations=4,  # default: 8\n",
        "    max_prompt_length=128,  # default: 512\n",
        "    # Parameters related to reporting and saving\n",
        "    report_to=[\"tensorboard\"],\n",
        "    logging_steps=10,\n",
        "    push_to_hub=False,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=10,\n",
        ")"
      ],
      "metadata": {
        "id": "9FvtrFB8rEAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import GRPOTrainer\n",
        "\n",
        "trainer = GRPOTrainer(\n",
        "    model=model, reward_funcs=[format_reward, accuracy_reward], args=training_args, train_dataset=train_dataset\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvQ_ByLkrNwH",
        "outputId": "caffb984-ce94-48e5-da96-c5010bf10df4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-FuRZovMrVh-",
        "outputId": "4b1d8c69-29d6-47be-9d0a-247bd42c3015"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10' max='113' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 10/113 12:31 < 2:41:19, 0.01 it/s, Epoch 0.08/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:math_verify.grader:Error during comparison\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 809, in compare_single_extraction_wrapper\n",
            "    return compare_single_extraction(g, t)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/utils.py\", line 51, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 789, in compare_single_extraction\n",
            "    return sympy_expr_eq(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 667, in sympy_expr_eq\n",
            "    return sympy_compare_relational(gold, pred, float_rounding, numeric_precision)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 344, in sympy_compare_relational\n",
            "    if sympy_solve_and_compare(gold, pred, float_rounding, numeric_precision):\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 274, in sympy_solve_and_compare\n",
            "    solved_gold = list(ordered(solve(gold, gold.free_symbols)))\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sympy/core/sorting.py\", line 298, in ordered\n",
            "    d[None].extend(seq)\n",
            "TypeError: 'LessThan' object is not iterable\n",
            "ERROR:math_verify.grader:Error during comparison\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 809, in compare_single_extraction_wrapper\n",
            "    return compare_single_extraction(g, t)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/utils.py\", line 51, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 789, in compare_single_extraction\n",
            "    return sympy_expr_eq(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 667, in sympy_expr_eq\n",
            "    return sympy_compare_relational(gold, pred, float_rounding, numeric_precision)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 344, in sympy_compare_relational\n",
            "    if sympy_solve_and_compare(gold, pred, float_rounding, numeric_precision):\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 275, in sympy_solve_and_compare\n",
            "    solved_pred = list(ordered(solve(pred, pred.free_symbols)))\n",
            "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sympy/solvers/solvers.py\", line 957, in solve\n",
            "    raise ValueError(\"can only solve for one symbol at a time\")\n",
            "ValueError: can only solve for one symbol at a time\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-3435b262f1ae>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2244\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2245\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2246\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2247\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2558\u001b[0m                     )\n\u001b[1;32m   2559\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2560\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2562\u001b[0m                     if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3728\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3730\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3731\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_sagemaker_mp_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3732\u001b[0m             \u001b[0mloss_mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmp_forward_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/trl/extras/profiling.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mprofiling_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/trl/trainer/grpo_trainer.py\u001b[0m in \u001b[0;36m_prepare_inputs\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_iterations\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 647\u001b[0;31m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate_and_score_completions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    648\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffered_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/trl/trainer/grpo_trainer.py\u001b[0m in \u001b[0;36m_generate_and_score_completions\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    717\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_wrapped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgather_deepspeed3_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mds3_gather_for_generation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             ) as unwrapped_model:\n\u001b[0;32m--> 719\u001b[0;31m                 prompt_completion_ids = unwrapped_model.generate(\n\u001b[0m\u001b[1;32m    720\u001b[0m                     \u001b[0mprompt_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneration_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1873\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_peft_forward_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1874\u001b[0m                     \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_peft_forward_args\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1875\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1876\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1877\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[1;32m   2463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2464\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2465\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2466\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2467\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3432\u001b[0m                 \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3433\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3434\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3436\u001b[0m             \u001b[0;31m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_requested_to_return_tuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_configured_to_return_tuple\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_top_level_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         outputs: BaseModelOutputWithPast = self.model(\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_requested_to_return_tuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_configured_to_return_tuple\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_top_level_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    547\u001b[0m                 )\n\u001b[1;32m    548\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    550\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         hidden_states, self_attn_weights = self.self_attn(\n\u001b[0m\u001b[1;32m    263\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mquery_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mkey_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mvalue_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0mcos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mposition_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/layer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_dora\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mactive_adapter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlora_B\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlora_A\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mscaling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIdentity\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/dropout.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1423\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"dropout probability has to be between 0 and 1, but got {p}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m     return (\n\u001b[0;32m-> 1425\u001b[0;31m         \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1426\u001b[0m     )\n\u001b[1;32m   1427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#add Data Parallel and Model parallel pipeline? - cant use anything can just increase batch size\n",
        "\n"
      ],
      "metadata": {
        "id": "DW3K4wKSsjXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import GRPOConfig\n",
        "\n",
        "training_args = GRPOConfig(\n",
        "    # 1) Mixed precision\n",
        "    bf16=True,              # or fp16=True on non-bf16 cards\n",
        "\n",
        "    # 2) Bigger per‐GPU batch, fewer accumulations\n",
        "    per_device_train_batch_size=16,\n",
        "    gradient_accumulation_steps=16,\n",
        "\n",
        "    # 3) Leaner reward eval\n",
        "    num_generations=2,\n",
        "    max_prompt_length=128,\n",
        "    max_completion_length=64,\n",
        "\n",
        "    # 4) Lighten I/O\n",
        "    logging_steps=100,\n",
        "    save_strategy=\"epoch\",\n",
        "\n",
        "    # keep your other args…\n",
        "    learning_rate=1e-5,\n",
        "    num_train_epochs=1,\n",
        "    remove_unused_columns=False,\n",
        "    report_to=[\"tensorboard\"],\n",
        "    push_to_hub=False,\n",
        ")\n"
      ],
      "metadata": {
        "id": "hI5rWu91sm9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import GRPOTrainer\n",
        "\n",
        "trainer = GRPOTrainer(\n",
        "    model=model, reward_funcs=[format_reward, accuracy_reward], args=training_args, train_dataset=train_dataset\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGjlc4cav4tC",
        "outputId": "eebcb66e-6e45-48f3-ed78-2d7e4693b23e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hITE3Lxmv7MP",
        "outputId": "4e625b5d-5267-464b-d1f6-8714c2ee4e2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='28' max='28' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [28/28 1:01:58, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:math_verify.grader:Error during comparison\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 809, in compare_single_extraction_wrapper\n",
            "    return compare_single_extraction(g, t)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/utils.py\", line 51, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 789, in compare_single_extraction\n",
            "    return sympy_expr_eq(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 667, in sympy_expr_eq\n",
            "    return sympy_compare_relational(gold, pred, float_rounding, numeric_precision)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 344, in sympy_compare_relational\n",
            "    if sympy_solve_and_compare(gold, pred, float_rounding, numeric_precision):\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 275, in sympy_solve_and_compare\n",
            "    solved_pred = list(ordered(solve(pred, pred.free_symbols)))\n",
            "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sympy/solvers/solvers.py\", line 957, in solve\n",
            "    raise ValueError(\"can only solve for one symbol at a time\")\n",
            "ValueError: can only solve for one symbol at a time\n",
            "ERROR:math_verify.grader:Error during comparison\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 809, in compare_single_extraction_wrapper\n",
            "    return compare_single_extraction(g, t)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/utils.py\", line 51, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 789, in compare_single_extraction\n",
            "    return sympy_expr_eq(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 667, in sympy_expr_eq\n",
            "    return sympy_compare_relational(gold, pred, float_rounding, numeric_precision)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 344, in sympy_compare_relational\n",
            "    if sympy_solve_and_compare(gold, pred, float_rounding, numeric_precision):\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 284, in sympy_solve_and_compare\n",
            "    for g, p in zip(sorted(solved_gold), sorted(solved_pred))\n",
            "                                         ^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: '<' not supported between instances of 'dict' and 'dict'\n",
            "ERROR:math_verify.grader:Error during comparison\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 809, in compare_single_extraction_wrapper\n",
            "    return compare_single_extraction(g, t)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/utils.py\", line 51, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 789, in compare_single_extraction\n",
            "    return sympy_expr_eq(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 667, in sympy_expr_eq\n",
            "    return sympy_compare_relational(gold, pred, float_rounding, numeric_precision)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 344, in sympy_compare_relational\n",
            "    if sympy_solve_and_compare(gold, pred, float_rounding, numeric_precision):\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 274, in sympy_solve_and_compare\n",
            "    solved_gold = list(ordered(solve(gold, gold.free_symbols)))\n",
            "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sympy/solvers/solvers.py\", line 1170, in solve\n",
            "    solution = _solve(f[0], *symbols, **flags)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sympy/solvers/solvers.py\", line 1729, in _solve\n",
            "    raise NotImplementedError('\\n'.join([msg, not_impl_msg % f]))\n",
            "NotImplementedError: multiple generators [tan(theta), theta]\n",
            "No algorithms are implemented to solve equation -theta + tan(theta)\n",
            "ERROR:math_verify.grader:Timeout during comparison\n",
            "ERROR:math_verify.grader:Error during comparison\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 809, in compare_single_extraction_wrapper\n",
            "    return compare_single_extraction(g, t)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/utils.py\", line 51, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 789, in compare_single_extraction\n",
            "    return sympy_expr_eq(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 667, in sympy_expr_eq\n",
            "    return sympy_compare_relational(gold, pred, float_rounding, numeric_precision)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 344, in sympy_compare_relational\n",
            "    if sympy_solve_and_compare(gold, pred, float_rounding, numeric_precision):\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 275, in sympy_solve_and_compare\n",
            "    solved_pred = list(ordered(solve(pred, pred.free_symbols)))\n",
            "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sympy/solvers/solvers.py\", line 1170, in solve\n",
            "    solution = _solve(f[0], *symbols, **flags)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sympy/solvers/solvers.py\", line 1729, in _solve\n",
            "    raise NotImplementedError('\\n'.join([msg, not_impl_msg % f]))\n",
            "NotImplementedError: multiple generators [x, f(x)]\n",
            "No algorithms are implemented to solve equation (-x**2 - 1774*x - 235) + f(x)\n",
            "ERROR:math_verify.grader:Error during comparison\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 809, in compare_single_extraction_wrapper\n",
            "    return compare_single_extraction(g, t)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/utils.py\", line 51, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 789, in compare_single_extraction\n",
            "    return sympy_expr_eq(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 667, in sympy_expr_eq\n",
            "    return sympy_compare_relational(gold, pred, float_rounding, numeric_precision)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 344, in sympy_compare_relational\n",
            "    if sympy_solve_and_compare(gold, pred, float_rounding, numeric_precision):\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 274, in sympy_solve_and_compare\n",
            "    solved_gold = list(ordered(solve(gold, gold.free_symbols)))\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sympy/core/sorting.py\", line 298, in ordered\n",
            "    d[None].extend(seq)\n",
            "TypeError: 'LessThan' object is not iterable\n",
            "ERROR:math_verify.grader:Error during comparison\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 809, in compare_single_extraction_wrapper\n",
            "    return compare_single_extraction(g, t)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/utils.py\", line 51, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 789, in compare_single_extraction\n",
            "    return sympy_expr_eq(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 667, in sympy_expr_eq\n",
            "    return sympy_compare_relational(gold, pred, float_rounding, numeric_precision)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 344, in sympy_compare_relational\n",
            "    if sympy_solve_and_compare(gold, pred, float_rounding, numeric_precision):\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 275, in sympy_solve_and_compare\n",
            "    solved_pred = list(ordered(solve(pred, pred.free_symbols)))\n",
            "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sympy/solvers/solvers.py\", line 1170, in solve\n",
            "    solution = _solve(f[0], *symbols, **flags)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sympy/solvers/solvers.py\", line 1729, in _solve\n",
            "    raise NotImplementedError('\\n'.join([msg, not_impl_msg % f]))\n",
            "NotImplementedError: multiple generators [x, P(x)]\n",
            "No algorithms are implemented to solve equation -x + P(x)\n",
            "ERROR:math_verify.grader:Error during comparison\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 809, in compare_single_extraction_wrapper\n",
            "    return compare_single_extraction(g, t)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/utils.py\", line 51, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 789, in compare_single_extraction\n",
            "    return sympy_expr_eq(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 667, in sympy_expr_eq\n",
            "    return sympy_compare_relational(gold, pred, float_rounding, numeric_precision)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 344, in sympy_compare_relational\n",
            "    if sympy_solve_and_compare(gold, pred, float_rounding, numeric_precision):\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 275, in sympy_solve_and_compare\n",
            "    solved_pred = list(ordered(solve(pred, pred.free_symbols)))\n",
            "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sympy/solvers/solvers.py\", line 1170, in solve\n",
            "    solution = _solve(f[0], *symbols, **flags)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sympy/solvers/solvers.py\", line 1729, in _solve\n",
            "    raise NotImplementedError('\\n'.join([msg, not_impl_msg % f]))\n",
            "NotImplementedError: multiple generators [x, f(x)]\n",
            "No algorithms are implemented to solve equation -x + f(x)\n",
            "ERROR:math_verify.grader:Error during comparison\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 809, in compare_single_extraction_wrapper\n",
            "    return compare_single_extraction(g, t)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/utils.py\", line 51, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 789, in compare_single_extraction\n",
            "    return sympy_expr_eq(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 667, in sympy_expr_eq\n",
            "    return sympy_compare_relational(gold, pred, float_rounding, numeric_precision)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 344, in sympy_compare_relational\n",
            "    if sympy_solve_and_compare(gold, pred, float_rounding, numeric_precision):\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 274, in sympy_solve_and_compare\n",
            "    solved_gold = list(ordered(solve(gold, gold.free_symbols)))\n",
            "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sympy/solvers/solvers.py\", line 1170, in solve\n",
            "    solution = _solve(f[0], *symbols, **flags)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sympy/solvers/solvers.py\", line 1377, in _solve\n",
            "    raise NotImplementedError(not_impl_msg % f)\n",
            "NotImplementedError: No algorithms are implemented to solve equation (-m**2 - 2*n**2) + (f(m)**2 + 2*f(n)**2)\n",
            "ERROR:math_verify.grader:Error during comparison\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 809, in compare_single_extraction_wrapper\n",
            "    return compare_single_extraction(g, t)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/utils.py\", line 51, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 789, in compare_single_extraction\n",
            "    return sympy_expr_eq(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 667, in sympy_expr_eq\n",
            "    return sympy_compare_relational(gold, pred, float_rounding, numeric_precision)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 344, in sympy_compare_relational\n",
            "    if sympy_solve_and_compare(gold, pred, float_rounding, numeric_precision):\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 274, in sympy_solve_and_compare\n",
            "    solved_gold = list(ordered(solve(gold, gold.free_symbols)))\n",
            "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sympy/solvers/solvers.py\", line 1170, in solve\n",
            "    solution = _solve(f[0], *symbols, **flags)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sympy/solvers/solvers.py\", line 1377, in _solve\n",
            "    raise NotImplementedError(not_impl_msg % f)\n",
            "NotImplementedError: No algorithms are implemented to solve equation (-x*f(x) + y*f(y)) + g(x, y)\n",
            "ERROR:math_verify.grader:Timeout during comparison\n",
            "ERROR:math_verify.grader:Error during comparison\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 809, in compare_single_extraction_wrapper\n",
            "    return compare_single_extraction(g, t)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/utils.py\", line 51, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 789, in compare_single_extraction\n",
            "    return sympy_expr_eq(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 667, in sympy_expr_eq\n",
            "    return sympy_compare_relational(gold, pred, float_rounding, numeric_precision)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 344, in sympy_compare_relational\n",
            "    if sympy_solve_and_compare(gold, pred, float_rounding, numeric_precision):\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 275, in sympy_solve_and_compare\n",
            "    solved_pred = list(ordered(solve(pred, pred.free_symbols)))\n",
            "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sympy/solvers/solvers.py\", line 1170, in solve\n",
            "    solution = _solve(f[0], *symbols, **flags)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sympy/solvers/solvers.py\", line 1729, in _solve\n",
            "    raise NotImplementedError('\\n'.join([msg, not_impl_msg % f]))\n",
            "NotImplementedError: multiple generators [exp(x), y(x)]\n",
            "No algorithms are implemented to solve equation y(x) - exp(2*x)\n",
            "ERROR:math_verify.grader:Error during comparison\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 809, in compare_single_extraction_wrapper\n",
            "    return compare_single_extraction(g, t)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/utils.py\", line 51, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 789, in compare_single_extraction\n",
            "    return sympy_expr_eq(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 667, in sympy_expr_eq\n",
            "    return sympy_compare_relational(gold, pred, float_rounding, numeric_precision)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 344, in sympy_compare_relational\n",
            "    if sympy_solve_and_compare(gold, pred, float_rounding, numeric_precision):\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 274, in sympy_solve_and_compare\n",
            "    solved_gold = list(ordered(solve(gold, gold.free_symbols)))\n",
            "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sympy/solvers/solvers.py\", line 957, in solve\n",
            "    raise ValueError(\"can only solve for one symbol at a time\")\n",
            "ValueError: can only solve for one symbol at a time\n",
            "ERROR:math_verify.grader:Error during comparison\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 809, in compare_single_extraction_wrapper\n",
            "    return compare_single_extraction(g, t)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/utils.py\", line 51, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 789, in compare_single_extraction\n",
            "    return sympy_expr_eq(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 667, in sympy_expr_eq\n",
            "    return sympy_compare_relational(gold, pred, float_rounding, numeric_precision)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 344, in sympy_compare_relational\n",
            "    if sympy_solve_and_compare(gold, pred, float_rounding, numeric_precision):\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 275, in sympy_solve_and_compare\n",
            "    solved_pred = list(ordered(solve(pred, pred.free_symbols)))\n",
            "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sympy/solvers/solvers.py\", line 950, in solve\n",
            "    fi = Add(fi.lhs, -fi.rhs, evaluate=False)\n",
            "                     ^^^^^^^\n",
            "TypeError: bad operand type for unary -: 'Tuple'\n",
            "ERROR:math_verify.grader:Error during comparison\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 809, in compare_single_extraction_wrapper\n",
            "    return compare_single_extraction(g, t)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/utils.py\", line 51, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 789, in compare_single_extraction\n",
            "    return sympy_expr_eq(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 667, in sympy_expr_eq\n",
            "    return sympy_compare_relational(gold, pred, float_rounding, numeric_precision)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 344, in sympy_compare_relational\n",
            "    if sympy_solve_and_compare(gold, pred, float_rounding, numeric_precision):\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 274, in sympy_solve_and_compare\n",
            "    solved_gold = list(ordered(solve(gold, gold.free_symbols)))\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sympy/core/sorting.py\", line 298, in ordered\n",
            "    d[None].extend(seq)\n",
            "TypeError: 'LessThan' object is not iterable\n",
            "ERROR:math_verify.grader:Error during comparison\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 809, in compare_single_extraction_wrapper\n",
            "    return compare_single_extraction(g, t)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/utils.py\", line 51, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 789, in compare_single_extraction\n",
            "    return sympy_expr_eq(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 667, in sympy_expr_eq\n",
            "    return sympy_compare_relational(gold, pred, float_rounding, numeric_precision)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 344, in sympy_compare_relational\n",
            "    if sympy_solve_and_compare(gold, pred, float_rounding, numeric_precision):\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 274, in sympy_solve_and_compare\n",
            "    solved_gold = list(ordered(solve(gold, gold.free_symbols)))\n",
            "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sympy/solvers/solvers.py\", line 1170, in solve\n",
            "    solution = _solve(f[0], *symbols, **flags)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sympy/solvers/solvers.py\", line 1729, in _solve\n",
            "    raise NotImplementedError('\\n'.join([msg, not_impl_msg % f]))\n",
            "NotImplementedError: multiple generators [P(x), P(x**3)]\n",
            "No algorithms are implemented to solve equation -P(x)**3 + P(x**3)\n",
            "ERROR:math_verify.grader:Error during comparison\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 809, in compare_single_extraction_wrapper\n",
            "    return compare_single_extraction(g, t)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/utils.py\", line 51, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 789, in compare_single_extraction\n",
            "    return sympy_expr_eq(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 667, in sympy_expr_eq\n",
            "    return sympy_compare_relational(gold, pred, float_rounding, numeric_precision)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 344, in sympy_compare_relational\n",
            "    if sympy_solve_and_compare(gold, pred, float_rounding, numeric_precision):\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 274, in sympy_solve_and_compare\n",
            "    solved_gold = list(ordered(solve(gold, gold.free_symbols)))\n",
            "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sympy/solvers/solvers.py\", line 1170, in solve\n",
            "    solution = _solve(f[0], *symbols, **flags)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sympy/solvers/solvers.py\", line 1729, in _solve\n",
            "    raise NotImplementedError('\\n'.join([msg, not_impl_msg % f]))\n",
            "NotImplementedError: \n",
            "No algorithms are implemented to solve equation P(x) - 2\n",
            "ERROR:math_verify.grader:Error during comparison\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 809, in compare_single_extraction_wrapper\n",
            "    return compare_single_extraction(g, t)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/utils.py\", line 51, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 789, in compare_single_extraction\n",
            "    return sympy_expr_eq(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 662, in sympy_expr_eq\n",
            "    if sympy_str_eq(gold, pred):\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 362, in sympy_str_eq\n",
            "    raise ValueError(\"Can't evaluate nan or zoo\")\n",
            "ValueError: Can't evaluate nan or zoo\n",
            "ERROR:math_verify.grader:Error during comparison\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 809, in compare_single_extraction_wrapper\n",
            "    return compare_single_extraction(g, t)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/utils.py\", line 51, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 789, in compare_single_extraction\n",
            "    return sympy_expr_eq(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 667, in sympy_expr_eq\n",
            "    return sympy_compare_relational(gold, pred, float_rounding, numeric_precision)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 344, in sympy_compare_relational\n",
            "    if sympy_solve_and_compare(gold, pred, float_rounding, numeric_precision):\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 278, in sympy_solve_and_compare\n",
            "    return all(\n",
            "           ^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 282, in <genexpr>\n",
            "    for (g_k, g_v), (p_k, p_v) in zip(sorted(g.items()), sorted(p.items()))\n",
            "                                                                ^^^^^^^\n",
            "AttributeError: 'Integer' object has no attribute 'items'\n",
            "ERROR:math_verify.grader:Error during comparison\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 809, in compare_single_extraction_wrapper\n",
            "    return compare_single_extraction(g, t)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/utils.py\", line 51, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 789, in compare_single_extraction\n",
            "    return sympy_expr_eq(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 667, in sympy_expr_eq\n",
            "    return sympy_compare_relational(gold, pred, float_rounding, numeric_precision)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 344, in sympy_compare_relational\n",
            "    if sympy_solve_and_compare(gold, pred, float_rounding, numeric_precision):\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 278, in sympy_solve_and_compare\n",
            "    return all(\n",
            "           ^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 282, in <genexpr>\n",
            "    for (g_k, g_v), (p_k, p_v) in zip(sorted(g.items()), sorted(p.items()))\n",
            "                                                                ^^^^^^^\n",
            "AttributeError: 'Rational' object has no attribute 'items'\n",
            "ERROR:math_verify.grader:Error during comparison\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 809, in compare_single_extraction_wrapper\n",
            "    return compare_single_extraction(g, t)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/utils.py\", line 51, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 789, in compare_single_extraction\n",
            "    return sympy_expr_eq(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 667, in sympy_expr_eq\n",
            "    return sympy_compare_relational(gold, pred, float_rounding, numeric_precision)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 344, in sympy_compare_relational\n",
            "    if sympy_solve_and_compare(gold, pred, float_rounding, numeric_precision):\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 278, in sympy_solve_and_compare\n",
            "    return all(\n",
            "           ^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 282, in <genexpr>\n",
            "    for (g_k, g_v), (p_k, p_v) in zip(sorted(g.items()), sorted(p.items()))\n",
            "                                                                ^^^^^^^\n",
            "AttributeError: 'Rational' object has no attribute 'items'\n",
            "ERROR:math_verify.grader:Error during comparison\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 809, in compare_single_extraction_wrapper\n",
            "    return compare_single_extraction(g, t)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/utils.py\", line 51, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 789, in compare_single_extraction\n",
            "    return sympy_expr_eq(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 667, in sympy_expr_eq\n",
            "    return sympy_compare_relational(gold, pred, float_rounding, numeric_precision)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 344, in sympy_compare_relational\n",
            "    if sympy_solve_and_compare(gold, pred, float_rounding, numeric_precision):\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 278, in sympy_solve_and_compare\n",
            "    return all(\n",
            "           ^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 282, in <genexpr>\n",
            "    for (g_k, g_v), (p_k, p_v) in zip(sorted(g.items()), sorted(p.items()))\n",
            "                                             ^^^^^^^\n",
            "AttributeError: 'Zero' object has no attribute 'items'\n",
            "ERROR:math_verify.grader:Error during comparison\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 809, in compare_single_extraction_wrapper\n",
            "    return compare_single_extraction(g, t)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/utils.py\", line 51, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 789, in compare_single_extraction\n",
            "    return sympy_expr_eq(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 667, in sympy_expr_eq\n",
            "    return sympy_compare_relational(gold, pred, float_rounding, numeric_precision)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 344, in sympy_compare_relational\n",
            "    if sympy_solve_and_compare(gold, pred, float_rounding, numeric_precision):\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 275, in sympy_solve_and_compare\n",
            "    solved_pred = list(ordered(solve(pred, pred.free_symbols)))\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sympy/core/sorting.py\", line 298, in ordered\n",
            "    d[None].extend(seq)\n",
            "TypeError: 'StrictLessThan' object is not iterable\n",
            "ERROR:math_verify.grader:Error during comparison\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 809, in compare_single_extraction_wrapper\n",
            "    return compare_single_extraction(g, t)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/utils.py\", line 51, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 789, in compare_single_extraction\n",
            "    return sympy_expr_eq(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 667, in sympy_expr_eq\n",
            "    return sympy_compare_relational(gold, pred, float_rounding, numeric_precision)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 344, in sympy_compare_relational\n",
            "    if sympy_solve_and_compare(gold, pred, float_rounding, numeric_precision):\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/math_verify/grader.py\", line 274, in sympy_solve_and_compare\n",
            "    solved_gold = list(ordered(solve(gold, gold.free_symbols)))\n",
            "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sympy/solvers/solvers.py\", line 1170, in solve\n",
            "    solution = _solve(f[0], *symbols, **flags)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sympy/solvers/solvers.py\", line 1729, in _solve\n",
            "    raise NotImplementedError('\\n'.join([msg, not_impl_msg % f]))\n",
            "NotImplementedError: multiple generators [x, log(x + 1)]\n",
            "No algorithms are implemented to solve equation -x/(x + 1) + log(x + 1, 10)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=28, training_loss=0.007950196308749062, metrics={'train_runtime': 3854.5149, 'train_samples_per_second': 0.94, 'train_steps_per_second': 0.007, 'total_flos': 0.0, 'train_loss': 0.007950196308749062})"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "6wRh-GUxv9cy",
        "outputId": "dced0613-074f-4878-9f05-bfeba3d6424d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "expected str, bytes or os.PathLike object, not PeftModelForCausalLM",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-85d56dedf1cf>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(self, output_dir, _internal_call)\u001b[0m\n\u001b[1;32m   3900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3901\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_save\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3902\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3904\u001b[0m         \u001b[0;31m# Push to the Hub when `save_model` is called by the user.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(self, output_dir, state_dict)\u001b[0m\n\u001b[1;32m   3981\u001b[0m         \u001b[0;31m# If we are executing this function, we are the process zero, so we don't check for that.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3982\u001b[0m         \u001b[0moutput_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_dir\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_dir\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3983\u001b[0;31m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3984\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Saving model checkpoint to {output_dir}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/posixpath.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(p)\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not PeftModelForCausalLM"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Zip up the model directory\n",
        "import shutil\n",
        "shutil.make_archive(\"my_model\", \"zip\", training_args.output_dir)\n",
        "\n",
        "# 2. Download the zip file to your local machine\n",
        "from google.colab import files\n",
        "files.download(\"my_model.zip\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "d3C5FTzDGaL3",
        "outputId": "e33fcd99-b5dc-4604-e178-bef85c1ffc7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_fac88f2a-b615-4cde-8bf2-24f3f61c930f\", \"my_model.zip\", 15994992)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reloading the model from drive"
      ],
      "metadata": {
        "id": "hdnmlrrBjE8O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Mount Google Drive (if not already mounted)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Set the path to your model folder\n",
        "model_path = '/content/drive/MyDrive/GPRO_3b/my_model'\n",
        "\n",
        "# 3. Load tokenizer and model (adjust load_in_8bit/device_map if you quantized it)\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_path,\n",
        "    device_map='auto',       # or {\"\": 0} to force GPU 0\n",
        "    load_in_8bit=False        # if you used bitsandbytes quantization\n",
        ")\n",
        "\n",
        "# 4. Create a text-generation pipeline\n",
        "gen = pipeline(\n",
        "    'text-generation',\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    # device=0                  # GPU device index\n",
        ")\n",
        "\n",
        "# 5. Run a prompt through your model\n",
        "prompt = \"Explain Group Relative Policy Optimization in simple terms:\"\n",
        "outputs = gen(prompt, max_new_tokens=100, do_sample=True, temperature=0.7)\n",
        "\n",
        "# 6. Print the generated text\n",
        "print(outputs[0]['generated_text'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358,
          "referenced_widgets": [
            "95f5e7e8469642b78032d327c2f9a2df",
            "85adca88285d499bb54932de3d9a377f",
            "b21b9ee3fbe747a793f910218f6cb810",
            "37c97612b40b46cd9b8e9231660eadce",
            "1529f9beffc0455eadc5a0b8785c8837",
            "870c4ba4ca2f45cfab86a1abcdcb4a57",
            "568581c813c34bd490b6df999e4696f1",
            "f2b5ba17010f4dfd98b5783f8260c6f4",
            "522b5cde0bd44f0d9426865b8a0ad13b",
            "0b3ff88c2e1f428aa25a99524f51eb3e",
            "bc91f19570484652884865b78dbcd92a",
            "888474474c164133a11433ae88803da3",
            "9907ae87e4de4061852191def067c0df",
            "53d585d87bc1477aa3009ac4e4a6f3b9",
            "e61f84148bda49458048a65deb8ac337",
            "9792c465d6ad4f7a9b67205776740b6f",
            "640b0663afc8454db44bea4092fedff2",
            "43b5f375adcc4b7d9a6d37115ff7fdfc",
            "8162b3a32223454fbb1af6f229a1fc06",
            "aa9c3ab287014d64b6384783e257ea9b",
            "a3fba33e9c124748874cf7b844392693",
            "cb21234130b443abb61c0b39142a14c7",
            "6e3c847411c24e03849fde5b6c166ba8",
            "2815ac160df441099c47078a85979550",
            "fd0a7e0e4a294b2ba681dde7aa359b6d",
            "957f6e5dd2574e6a9e8990bf55b97ce2",
            "dac7b94099c34ecab2a57f068652dfc4",
            "fa4baa38361c4cc6a5a14d2c6cc83207",
            "f41646ddd8fa4b9f96408f7193b159a2",
            "6d3218c34e754df9899cff897607faf9",
            "57b3c28c9ec1428089e577ba5f5e15c3",
            "4f5f35bf822f45608f0f340b2a4826c8",
            "362588e4197d4ce0bc8524f13b0b6cd0"
          ]
        },
        "id": "gvs53dH_HH_C",
        "outputId": "820505ef-9c78-4640-9e90-4e409f60e7dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/659 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "95f5e7e8469642b78032d327c2f9a2df"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/988M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "888474474c164133a11433ae88803da3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6e3c847411c24e03849fde5b6c166ba8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/peft/config.py:162: UserWarning: Unexpected keyword arguments ['alpha_pattern', 'bias', 'corda_config', 'eva_config', 'exclude_modules', 'fan_in_fan_out', 'init_lora_weights', 'layer_replication', 'layers_pattern', 'layers_to_transform', 'loftq_config', 'lora_alpha', 'lora_bias', 'lora_dropout', 'megatron_config', 'megatron_core', 'modules_to_save', 'r', 'rank_pattern', 'target_modules', 'trainable_token_indices', 'use_dora', 'use_rslora'] for class PeftConfig, these are ignored. This probably means that you're loading a configuration file that was saved using a higher version of the library and additional parameters have been introduced since. It is highly recommended to upgrade the PEFT version before continuing (e.g. by running `pip install -U peft`).\n",
            "  warnings.warn(\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explain Group Relative Policy Optimization in simple terms: Group relative policy optimization refers to a technique used in the field of computer science and software engineering where a group of individuals or entities work together to develop, maintain, and improve a piece of software or hardware. It involves assigning tasks to different members of the group based on their expertise and availability, and then using machine learning algorithms to optimize the performance of the software or hardware.\n",
            "For example, if you were working on developing a new mobile app for your company, you might assign one person to design the user\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt =\"In 1988, a person's age was equal to the sum of the digits of their birth year. How old was this person?\"\n",
        "outputs = gen(prompt, max_new_tokens=1000, do_sample=True, temperature=0.7)\n",
        "\n",
        "# 6. Print the generated text\n",
        "print(outputs[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlhPR00PjD9e",
        "outputId": "746a848e-7269-4a28-8966-5d92ec7f21d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In 1988, a person's age was equal to the sum of the digits of their birth year. How old was this person?\r\n",
            "\r\n",
            "I know that $10^{2n} - 1$ is divisible by $4$, but I don't know how to prove it.\n",
            "\n",
            "\r\n",
            "\r\n",
            "This is just a special case of Fermat's little theorem. It states that if $a$ and $b$ are coprime integers then $a^p \\equiv a \\mod p$ for all primes $p$. \r\n",
            "\r\n",
            "So in your problem, we have that $10^n-1$ is divisible by 4 because 10 and 4 are relatively prime (since they are both odd). But $10^n=2\\cdot5^n$ so $5|2\\cdot5^n$ which means that $5|10^n-1$ because $5$ divides any power of 10 except for $5^n$. So $10^n-1$ is divisible by 5 as well.\r\n",
            "\r\n",
            "Therefore $10^{2n}-1$ is also divisible by 5, so $10^{2n}=1+5k$ where k is an integer. Then we can see that $10^{2n}-1=(1+5k)-1=5k$ and therefore $10^{2n}=5k$ or $k=\\frac{1}{2}\\cdot10^{2n}$ which is clearly less than 10. So the only possible value of $k$ is $\\boxed{\\frac{1}{2}}$. \n",
            "\n",
            "EDIT: Fixed typo. Sorry! :blush:\n",
            "\n",
            "\r\n",
            "\r\n",
            "I'm not sure what you're asking, but here's my solution:\r\n",
            "\r\n",
            "Given that $10^n-1 = 5k$, where $k$ is some integer, we want to find the smallest positive integer $n$ such that $10^n = 5k + 1$.\r\n",
            "By trial and error, we find that $n = 10$ works since $10^1 = 5*2$ and $10^2 = 2*5*2$.\r\n",
            "Thus, the answer is $10$.\r\n",
            "\r\n",
            "EDIT: I think there may be a small mistake in my reasoning. I'll try to fix it later. If anyone has any other questions, please ask!\n",
            "\n",
            "You're correct, and you've made a very good point about using the fact that $10^n = 5k$ instead of $10^n = 2*5^n$. However, I think your method is still valid. You could also use induction on n. For example, when n=1, the equation holds true. When n=k, you get the formula 10^(2k) - 1 = 5k. Then for n=k+1, you get 10^(2(k+1)) - 1 = 5k+1, which again holds true. Therefore, the answer is 10. \n",
            "\n",
            "The proof of Fermat's Little Theorem is pretty easy. Just use $a^p \\equiv a \\mod p$ for any integer $a$ and $p$ prime number. \n",
            "If you need help with this, PM me. \r\n",
            "Also, I believe there is no way to make the question more clear without giving away the answer. \r\n",
            "\r\n",
            "@below, thanks for catching that. I fixed it now. :D\n",
            "\n",
            "Oh okay. Thanks for the reminder. :)\r\n",
            "\r\n",
            "Edit: Okay, I think I got it now.  Let $10^{2n} - 1 = 5k$ and let $10^n = 5m + 1$ for some integer $m$. Then we have that $10^{2n} - 1 = 5m + 1 = 5k$, so $10^{2n} = 6k + 1 = 5m + 1$. This implies that $10^{2n} = 5m$, so $m = \\frac{1}{2} * 10^{2n}$. Since $10^{2n} - 1 = 5k$, we have that $m = \\frac{1}{2} * 10^{2n} = 5k$ and thus $10^{2n} = 5k + 1$, as desired. \r\n",
            "\r\n",
            "Thank you very much for all the help! :) \r\n",
            "\r\n",
            "@below, I'm glad I helped!  :P \r\n",
            "\r\n",
            "Thanks again! :) \r\n",
            "\r\n",
            "I hope you guys enjoyed solving this one too!  :D \r\n",
            "\r\n",
            "EDIT: Oh, and thank you for the corrections. I really appreciate them! :) \r\n",
            "\r\n",
            "And yes, I do believe that there is no way to solve the problem more clearly without giving away the answer. :) \r\n",
            "\r\n",
            "Anyway, I hope you all enjoy solving the next one too! :D \r\n",
            "\r\n",
            "EDIT: Well, I actually did\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# GPRO_3b_ec2.py\n",
        "\n",
        "# Installation instructions for EC2 (run these in your environment before executing the script):\n",
        "# pip install -U trl peft math_verify transformers datasets huggingface_hub accelerate torch\n",
        "# Ensure CUDA is available and compatible with your torch version if using GPU.\n",
        "\n",
        "import os\n",
        "import re\n",
        "import shutil # For managing directories if needed\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from datasets import load_dataset\n",
        "from huggingface_hub import login\n",
        "from peft import LoraConfig, get_peft_model, PeftModel\n",
        "from math_verify import LatexExtractionConfig, parse, verify\n",
        "from trl import GRPOConfig, GRPOTrainer\n",
        "\n",
        "# --- Configuration ---\n",
        "# Hugging Face Login (recommended: set HUGGING_FACE_HUB_TOKEN environment variable)\n",
        "try:\n",
        "    login_token = os.environ.get(\"HUGGING_FACE_HUB_TOKEN\")\n",
        "    if login_token:\n",
        "        login(token=login_token, add_to_git_credential=False) # add_to_git_credential=False for non-interactive\n",
        "        print(\"Hugging Face Hub login successful using HUGGING_FACE_HUB_TOKEN.\")\n",
        "    else:\n",
        "        print(\"HUGGING_FACE_HUB_TOKEN not found. Attempting interactive login if needed by model download.\")\n",
        "        # login() # Or just let operations that need it fail/prompt.\n",
        "except Exception as e:\n",
        "    print(f\"Hugging Face Hub login attempt issue. Ensure you are logged in if private models are used. Error: {e}\")\n",
        "\n",
        "# Dataset and Model IDs\n",
        "dataset_id = \"AI-MO/NuminaMath-TIR\"\n",
        "model_id = \"Qwen/Qwen2-0.5B-Instruct\" # Base model for fine-tuning\n",
        "\n",
        "# Output directory for the trained model\n",
        "TRAINED_MODEL_DIR = \"Qwen2-0.5B-GRPO-NuminaMath-Finetuned\"\n",
        "\n",
        "# System prompt for the task\n",
        "SYSTEM_PROMPT = (\n",
        "    \"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant \"\n",
        "    \"first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning \"\n",
        "    \"process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., \"\n",
        "    \"<think> reasoning process here </think><answer> answer here </answer>\"\n",
        ")\n",
        "\n",
        "# --- Dataset Preparation ---\n",
        "print(f\"Loading dataset: {dataset_id}\")\n",
        "# Using a smaller portion for demonstration. Adjust or remove slicing for full training.\n",
        "# E.g., split=[\"train\", \"test\"] for full dataset.\n",
        "# For EC2, manage data size based on instance capabilities and time.\n",
        "train_dataset_raw = load_dataset(dataset_id, split=\"train[:1%]\") # Use a small slice for faster run\n",
        "# test_dataset_raw = load_dataset(dataset_id, split=\"test[:1%]\") # Test set not directly used by GRPOTrainer.train, but good for eval\n",
        "\n",
        "print(f\"Loaded raw train dataset: {train_dataset_raw}\")\n",
        "\n",
        "def make_conversation_and_prepare_columns(example):\n",
        "    \"\"\"Formats the problem into a chat prompt and ensures 'solution' is available.\"\"\"\n",
        "    return {\n",
        "        \"prompt\": [ # This will be the main input for the GRPOTrainer\n",
        "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "            {\"role\": \"user\", \"content\": example[\"problem\"]},\n",
        "        ],\n",
        "        \"solution\": example[\"solution\"] # Keep solution for the accuracy reward function\n",
        "    }\n",
        "\n",
        "train_dataset = train_dataset_raw.map(make_conversation_and_prepare_columns)\n",
        "\n",
        "# Remove original columns that are now incorporated into 'prompt' or are not needed\n",
        "# 'problem' is in 'prompt', 'solution' is kept, 'messages' (original format) is not needed.\n",
        "train_dataset = train_dataset.remove_columns([\"problem\", \"messages\"])\n",
        "\n",
        "print(f\"Processed train dataset: {train_dataset}\")\n",
        "print(f\"Train dataset features: {train_dataset.features}\")\n",
        "if len(train_dataset) == 0:\n",
        "    raise ValueError(\"Train dataset is empty. Check data loading and slicing.\")\n",
        "\n",
        "\n",
        "# --- Model Loading and PEFT Setup ---\n",
        "print(f\"Loading base model: {model_id}\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=\"auto\",  # Automatically selects precision (bfloat16, float16, or float32)\n",
        "    device_map=\"auto\",   # Automatically distributes model across available devices (GPU/CPU)\n",
        ")\n",
        "\n",
        "# LoRA Configuration\n",
        "lora_config = LoraConfig(\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"], # Modules to apply LoRA to. Check model architecture if changing.\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "print(\"Trainable parameters after PEFT setup:\")\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# --- Reward Functions ---\n",
        "def format_reward(completions, **kwargs):\n",
        "    \"\"\"Reward function that checks if the completion has the specified <think>...</think><answer>...</answer> format.\"\"\"\n",
        "    pattern = r\"^<think>.*?</think>\\s*<answer>.*?</answer>$\"\n",
        "    # Completions is a list of lists of dicts: [[{\"role\": \"assistant\", \"content\": \"...\"}], ...]\n",
        "    # We need the content string.\n",
        "    completion_contents = [comp[0][\"content\"] for comp in completions if comp and comp[0] and \"content\" in comp[0]]\n",
        "    rewards_list = [1.0 if re.match(pattern, content, re.DOTALL) else 0.0 for content in completion_contents]\n",
        "    return rewards_list\n",
        "\n",
        "def accuracy_reward(completions, **kwargs):\n",
        "    \"\"\"Reward function that checks if the answer in the completion matches the ground truth solution.\"\"\"\n",
        "    # 'solution' column from the dataset is passed via kwargs by GRPOTrainer if remove_unused_columns=False\n",
        "    solutions = kwargs[\"solution\"] # This will be a list of solutions, one for each prompt in the batch\n",
        "\n",
        "    # Completions is List[List[Dict[str, str]]], where outer list corresponds to prompts,\n",
        "    # inner list corresponds to `num_generations` per prompt.\n",
        "    # We need to align generated completions with their original solutions.\n",
        "    # GRPOTrainer calls reward_funcs with `completions` shaped (batch_size * num_generations, 1, dict)\n",
        "    # and `solution` will be of length `batch_size`. We need to repeat/align `solution`.\n",
        "    # The GRPOTrainer's documentation/source should clarify how kwargs are batched.\n",
        "    # Assuming `solution` corresponds to the prompts, and we have `num_generations` for each.\n",
        "    # Let's assume for now `completions` and `solutions` are aligned by GRPOTrainer or this needs adjustment.\n",
        "    # The provided example implies completions and solutions are already aligned or broadcasted.\n",
        "    # `completions` is (batch_size * num_generations), `solutions` is (batch_size)\n",
        "    # Let `num_generations` be from `training_args`.\n",
        "\n",
        "    num_gens_per_prompt = kwargs.get(\"num_generations\", 1) # Get from training_args if possible or infer\n",
        "    if \"num_generations\" not in kwargs and trainer_args: # Access from global if defined\n",
        "        num_gens_per_prompt = trainer_args.num_generations\n",
        "\n",
        "    expanded_solutions = []\n",
        "    for sol in solutions:\n",
        "        expanded_solutions.extend([sol] * num_gens_per_prompt)\n",
        "\n",
        "    completion_contents = [comp[0][\"content\"] for comp in completions if comp and comp[0] and \"content\" in comp[0]]\n",
        "    rewards = []\n",
        "\n",
        "    for content, solution_text in zip(completion_contents, expanded_solutions):\n",
        "        gold_parsed = parse(solution_text, extraction_mode=\"first_match\", extraction_config=[LatexExtractionConfig()])\n",
        "        # Extract from <answer> tag in generated content\n",
        "        answer_match = re.search(r\"<answer>(.*?)</answer>\", content, re.DOTALL)\n",
        "        if answer_match:\n",
        "            answer_text = answer_match.group(1).strip()\n",
        "            answer_parsed = parse(answer_text, extraction_mode=\"first_match\", extraction_config=[LatexExtractionConfig()])\n",
        "            if len(gold_parsed) != 0 and len(answer_parsed) != 0:\n",
        "                try:\n",
        "                    rewards.append(float(verify(answer_parsed, gold_parsed)))\n",
        "                except Exception:\n",
        "                    rewards.append(0.0) # Error during verification\n",
        "            elif len(gold_parsed) == 0 and len(answer_parsed) == 0 : # Both empty (e.g. non-math textual answer)\n",
        "                 rewards.append(1.0 if answer_text == solution_text else 0.0) # Simple string match for non-parsable\n",
        "            elif len(gold_parsed) == 0 and len(answer_parsed) != 0 : # Gold is empty, answer is not\n",
        "                rewards.append(0.0)\n",
        "            else: # Gold is not empty, answer is empty or unparsable\n",
        "                rewards.append(0.0)\n",
        "        else:\n",
        "            rewards.append(0.0) # No <answer> tag\n",
        "    return rewards\n",
        "\n",
        "# --- Training Arguments and Trainer ---\n",
        "print(\"Configuring training arguments...\")\n",
        "# Conditional mixed precision based on hardware support\n",
        "use_bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
        "use_fp16 = torch.cuda.is_available() and not use_bf16\n",
        "\n",
        "# Global placeholder for training_args to be accessible by reward function if needed for num_generations\n",
        "trainer_args = None\n",
        "\n",
        "training_args = GRPOConfig(\n",
        "    output_dir=TRAINED_MODEL_DIR,\n",
        "    learning_rate=1e-5,       # Typical learning rate for fine-tuning\n",
        "    remove_unused_columns=False, # Crucial for accessing 'solution' in accuracy_reward\n",
        "    gradient_accumulation_steps=4, # Adjust based on VRAM. Original: 16. Reduced for wider compatibility.\n",
        "    num_train_epochs=1,        # Number of training epochs\n",
        "    bf16=use_bf16,             # Use bfloat16 if supported\n",
        "    fp16=use_fp16,             # Use float16 if bfloat16 not supported but CUDA is available\n",
        "\n",
        "    # per_device_train_batch_size: From TrainingArguments, defaults to 8.\n",
        "    # This is the number of prompts processed for loss computation by one device.\n",
        "    per_device_train_batch_size=2, # Adjust based on VRAM. Original: 8 (default). Reduced.\n",
        "\n",
        "    # batch_size (for GRPO generation step): From GRPOConfig, defaults to 1.\n",
        "    # Number of prompts to generate completions for simultaneously.\n",
        "    # batch_size = 1, # Default in GRPOConfig\n",
        "\n",
        "    # Parameters controlling data preprocessing and generation during training\n",
        "    max_completion_length=128, # Max length of generated completions. Original: 64. Increased for math.\n",
        "    num_generations=2,         # Number of completions to generate per prompt. Original: 4. Reduced.\n",
        "    max_prompt_length=256,     # Max length of prompts. Original: 128. Increased.\n",
        "\n",
        "    # Reporting and saving\n",
        "    report_to=\"tensorboard\",   # For EC2, consider \"none\" or ensure TensorBoard is set up.\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",     # Save at the end of each epoch. Original: \"steps\"\n",
        "    # save_steps=10,           # Used if save_strategy=\"steps\". Original: 10\n",
        "    # save_total_limit=1,      # Optional: limit the number of saved checkpoints\n",
        "    push_to_hub=False,         # Set to True to push to Hugging Face Hub\n",
        "    # hub_model_id=\"your-username/your-model-name\", # Required if push_to_hub=True\n",
        ")\n",
        "trainer_args = training_args # Make it accessible globally for reward function\n",
        "\n",
        "tokenizer_for_training = AutoTokenizer.from_pretrained(model_id)\n",
        "if tokenizer_for_training.pad_token_id is None:\n",
        "    tokenizer_for_training.pad_token_id = tokenizer_for_training.eos_token_id\n",
        "    model.config.pad_token_id = model.config.eos_token_id # Ensure model config also updated\n",
        "\n",
        "trainer = GRPOTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer_for_training, # Pass tokenizer to GRPOTrainer\n",
        "    reward_funcs=[format_reward, accuracy_reward],\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        ")\n",
        "\n",
        "print(\"Starting training...\")\n",
        "trainer.train()\n",
        "print(\"Training finished.\")\n",
        "\n",
        "# --- Save Model ---\n",
        "print(f\"Saving fine-tuned model (adapter) and tokenizer to {TRAINED_MODEL_DIR}...\")\n",
        "trainer.save_model(TRAINED_MODEL_DIR) # Saves adapter & tokenizer\n",
        "# tokenizer_for_training.save_pretrained(TRAINED_MODEL_DIR) # Trainer should do this\n",
        "print(f\"Model and tokenizer saved to {TRAINED_MODEL_DIR}.\")\n",
        "\n",
        "# Optional: Zip the model directory if needed for transfer\n",
        "# shutil.make_archive(f\"{TRAINED_MODEL_DIR}_archive\", \"zip\", TRAINED_MODEL_DIR)\n",
        "# print(f\"Model archived to {TRAINED_MODEL_DIR}_archive.zip\")\n",
        "\n",
        "\n",
        "# --- Inference with the Fine-tuned Model ---\n",
        "print(\"\\n--- Starting Inference with the Fine-tuned Model ---\")\n",
        "\n",
        "# Load the fine-tuned model (base model + adapter) and tokenizer\n",
        "# AutoModelForCausalLM.from_pretrained on a PEFT-saved directory\n",
        "# should load the base model and apply the adapter.\n",
        "print(f\"Loading fine-tuned model from {TRAINED_MODEL_DIR} for inference...\")\n",
        "try:\n",
        "    inference_model = AutoModelForCausalLM.from_pretrained(\n",
        "        TRAINED_MODEL_DIR,\n",
        "        torch_dtype=\"auto\", # Use appropriate dtype\n",
        "        device_map=\"auto\",  # Load on available device\n",
        "    )\n",
        "    # Tokenizer should have been saved by trainer.save_model()\n",
        "    inference_tokenizer = AutoTokenizer.from_pretrained(TRAINED_MODEL_DIR)\n",
        "    print(\"Successfully loaded fine-tuned model and tokenizer for inference.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model directly from {TRAINED_MODEL_DIR}: {e}\")\n",
        "    print(\"Attempting fallback: loading base model and then attaching adapter...\")\n",
        "    base_model_for_inference = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id, # Base model ID\n",
        "        torch_dtype=\"auto\",\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "    inference_model = PeftModel.from_pretrained(base_model_for_inference, TRAINED_MODEL_DIR)\n",
        "    # Optional: merge LoRA weights for potentially faster inference if not done automatically.\n",
        "    # This replaces the model with the merged version.\n",
        "    # inference_model = inference_model.merge_and_unload()\n",
        "    inference_tokenizer = AutoTokenizer.from_pretrained(model_id) # Fallback to base tokenizer\n",
        "    print(\"Fallback loading complete.\")\n",
        "\n",
        "# Ensure pad_token_id is set for the inference tokenizer\n",
        "if inference_tokenizer.pad_token_id is None:\n",
        "    inference_tokenizer.pad_token_id = inference_tokenizer.eos_token_id\n",
        "\n",
        "# Determine device for pipeline (GPU if available, else CPU)\n",
        "pipeline_device = 0 if torch.cuda.is_available() else -1\n",
        "\n",
        "print(f\"Creating text-generation pipeline on device: {'cuda:0' if pipeline_device == 0 else 'cpu'}\")\n",
        "gen_pipeline = pipeline(\n",
        "    'text-generation',\n",
        "    model=inference_model,\n",
        "    tokenizer=inference_tokenizer,\n",
        "    device=pipeline_device\n",
        ")\n",
        "\n",
        "# Test prompts\n",
        "prompts_for_testing = [\n",
        "    \"Explain Group Relative Policy Optimization in simple terms.\",\n",
        "    \"In 1988, a person's age was equal to the sum of the digits of their birth year. How old was this person?\"\n",
        "]\n",
        "\n",
        "for user_prompt_text in prompts_for_testing:\n",
        "    print(f\"\\nUser Prompt: {user_prompt_text}\")\n",
        "\n",
        "    # Format the prompt using the chat template, including the system prompt used during training\n",
        "    messages_for_inference = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": user_prompt_text},\n",
        "    ]\n",
        "\n",
        "    if inference_tokenizer.chat_template:\n",
        "        formatted_model_input = inference_tokenizer.apply_chat_template(\n",
        "            messages_for_inference,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True  # Crucial for instruct models to know it's their turn\n",
        "        )\n",
        "    else:\n",
        "        # Fallback if no chat template (should not happen for Qwen2-Instruct)\n",
        "        print(\"Warning: Tokenizer does not have a chat template. Using basic formatting.\")\n",
        "        formatted_model_input = f\"{SYSTEM_PROMPT}\\nUser: {user_prompt_text}\\nAssistant:\"\n",
        "\n",
        "    print(f\"--- Formatted Input to Model ---\\n{formatted_model_input}\\n-------------------------------\")\n",
        "\n",
        "    # Generate text\n",
        "    outputs = gen_pipeline(\n",
        "        formatted_model_input,\n",
        "        max_new_tokens=512,  # Max tokens for the generated response\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        #eos_token_id=inference_tokenizer.eos_token_id, # Pipeline usually handles this\n",
        "        #pad_token_id=inference_tokenizer.pad_token_id # Pipeline usually handles this\n",
        "    )\n",
        "\n",
        "    generated_full_text = outputs[0]['generated_text']\n",
        "\n",
        "    # Extract only the newly generated assistant's reply\n",
        "    # The `formatted_model_input` is what we sent.\n",
        "    # The `generated_full_text` contains this input + the model's generation.\n",
        "    assistant_reply = generated_full_text\n",
        "    if generated_full_text.startswith(formatted_model_input):\n",
        "        assistant_reply = generated_full_text[len(formatted_model_input):].strip()\n",
        "    else:\n",
        "        # If the pipeline behaves differently or input string had subtle changes (e.g. whitespace)\n",
        "        # try to find the last part based on assistant markers if possible.\n",
        "        # For Qwen2, this would be after \"<|im_start|>assistant\\n\"\n",
        "        # This is a heuristic if the direct prefix strip fails.\n",
        "        assistant_marker = None\n",
        "        if \"qwen2\" in inference_tokenizer.name_or_path.lower():\n",
        "             assistant_marker = \"<|im_start|>assistant\" # Don't include \\n as it might vary slightly\n",
        "\n",
        "        if assistant_marker:\n",
        "            last_occurrence_idx = generated_full_text.rfind(assistant_marker)\n",
        "            if last_occurrence_idx != -1:\n",
        "                # Find the end of the marker (e.g., after the newline)\n",
        "                end_of_marker_idx = generated_full_text.find(\"\\n\", last_occurrence_idx)\n",
        "                if end_of_marker_idx != -1:\n",
        "                     assistant_reply = generated_full_text[end_of_marker_idx+1:].strip()\n",
        "\n",
        "    # Clean up trailing special tokens like <|im_end|>\n",
        "    if assistant_reply.endswith(inference_tokenizer.eos_token) or \\\n",
        "       (hasattr(inference_tokenizer, 'special_tokens_map') and \\\n",
        "        'im_end_token' in inference_tokenizer.special_tokens_map and \\\n",
        "        assistant_reply.endswith(inference_tokenizer.special_tokens_map['im_end_token'])):\n",
        "\n",
        "        if assistant_reply.endswith(inference_tokenizer.eos_token):\n",
        "            assistant_reply = assistant_reply[:-len(inference_tokenizer.eos_token)].strip()\n",
        "\n",
        "        if hasattr(inference_tokenizer, 'special_tokens_map') and \\\n",
        "           'im_end_token' in inference_tokenizer.special_tokens_map and \\\n",
        "           assistant_reply.endswith(inference_tokenizer.special_tokens_map['im_end_token']):\n",
        "           assistant_reply = assistant_reply[:-len(inference_tokenizer.special_tokens_map['im_end_token'])].strip()\n",
        "\n",
        "\n",
        "    print(f\"--- Assistant's Reply ---\\n{assistant_reply}\\n-------------------------\")\n",
        "\n",
        "print(\"\\nScript finished.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "c2wgDWzskKYV",
        "outputId": "2b815fed-38d3-4faa-cf5c-46ce66297613"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'math_verify'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-6fab68b03145>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhuggingface_hub\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpeft\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLoraConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_peft_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPeftModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmath_verify\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLatexExtractionConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtrl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGRPOConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGRPOTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'math_verify'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# GPRO_3b_ec2.py\n",
        "\n",
        "# Installation instructions for EC2 (run these in your environment before executing the script):\n",
        "# pip install -U trl peft math_verify transformers datasets huggingface_hub accelerate torch\n",
        "# Ensure CUDA is available and compatible with your torch version if using GPU.\n",
        "\n",
        "import os\n",
        "import re\n",
        "import shutil # For managing directories if needed\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from datasets import load_dataset\n",
        "from huggingface_hub import login\n",
        "from peft import LoraConfig, get_peft_model, PeftModel\n",
        "from math_verify import LatexExtractionConfig, parse, verify\n",
        "from trl import GRPOConfig, GRPOTrainer\n",
        "\n",
        "# --- Configuration ---\n",
        "# Hugging Face Login (recommended: set HUGGING_FACE_HUB_TOKEN environment variable)\n",
        "try:\n",
        "    login_token = os.environ.get(\"HUGGING_FACE_HUB_TOKEN\")\n",
        "    if login_token:\n",
        "        login(token=login_token, add_to_git_credential=False) # add_to_git_credential=False for non-interactive\n",
        "        print(\"Hugging Face Hub login successful using HUGGING_FACE_HUB_TOKEN.\")\n",
        "    else:\n",
        "        print(\"HUGGING_FACE_HUB_TOKEN not found. Attempting interactive login if needed by model download.\")\n",
        "        # login() # Or just let operations that need it fail/prompt.\n",
        "except Exception as e:\n",
        "    print(f\"Hugging Face Hub login attempt issue. Ensure you are logged in if private models are used. Error: {e}\")\n",
        "\n",
        "# Dataset and Model IDs\n",
        "dataset_id = \"AI-MO/NuminaMath-TIR\"\n",
        "model_id = \"Qwen/Qwen2-0.5B-Instruct\" # Base model for fine-tuning\n",
        "\n",
        "# Output directory for the trained model\n",
        "TRAINED_MODEL_DIR = \"Qwen2-0.5B-GRPO-NuminaMath-Finetuned\"\n",
        "\n",
        "# System prompt for the task\n",
        "SYSTEM_PROMPT = (\n",
        "    \"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant \"\n",
        "    \"first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning \"\n",
        "    \"process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., \"\n",
        "    \"<think> reasoning process here </think><answer> answer here </answer>\"\n",
        ")\n",
        "\n",
        "# --- Dataset Preparation ---\n",
        "print(f\"Loading dataset: {dataset_id}\")\n",
        "# Using a smaller portion for demonstration. Adjust or remove slicing for full training.\n",
        "# E.g., split=[\"train\", \"test\"] for full dataset.\n",
        "# For EC2, manage data size based on instance capabilities and time.\n",
        "train_dataset_raw = load_dataset(dataset_id, split=\"train[:1%]\") # Use a small slice for faster run\n",
        "# test_dataset_raw = load_dataset(dataset_id, split=\"test[:1%]\") # Test set not directly used by GRPOTrainer.train, but good for eval\n",
        "\n",
        "print(f\"Loaded raw train dataset: {train_dataset_raw}\")\n",
        "\n",
        "def make_conversation_and_prepare_columns(example):\n",
        "    \"\"\"Formats the problem into a chat prompt and ensures 'solution' is available.\"\"\"\n",
        "    return {\n",
        "        \"prompt\": [ # This will be the main input for the GRPOTrainer\n",
        "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "            {\"role\": \"user\", \"content\": example[\"problem\"]},\n",
        "        ],\n",
        "        \"solution\": example[\"solution\"] # Keep solution for the accuracy reward function\n",
        "    }\n",
        "\n",
        "train_dataset = train_dataset_raw.map(make_conversation_and_prepare_columns)\n",
        "\n",
        "# Remove original columns that are now incorporated into 'prompt' or are not needed\n",
        "# 'problem' is in 'prompt', 'solution' is kept, 'messages' (original format) is not needed.\n",
        "train_dataset = train_dataset.remove_columns([\"problem\", \"messages\"])\n",
        "\n",
        "print(f\"Processed train dataset: {train_dataset}\")\n",
        "print(f\"Train dataset features: {train_dataset.features}\")\n",
        "if len(train_dataset) == 0:\n",
        "    raise ValueError(\"Train dataset is empty. Check data loading and slicing.\")\n",
        "\n",
        "\n",
        "# --- Model Loading and PEFT Setup ---\n",
        "print(f\"Loading base model: {model_id}\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=\"auto\",  # Automatically selects precision (bfloat16, float16, or float32)\n",
        "    device_map=\"auto\",   # Automatically distributes model across available devices (GPU/CPU)\n",
        ")\n",
        "\n",
        "# LoRA Configuration\n",
        "lora_config = LoraConfig(\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"], # Modules to apply LoRA to. Check model architecture if changing.\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "print(\"Trainable parameters after PEFT setup:\")\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# --- Reward Functions ---\n",
        "def format_reward(completions, **kwargs):\n",
        "    \"\"\"Reward function that checks if the completion has the specified <think>...</think><answer>...</answer> format.\"\"\"\n",
        "    pattern = r\"^<think>.*?</think>\\s*<answer>.*?</answer>$\"\n",
        "    # Completions is a list of lists of dicts: [[{\"role\": \"assistant\", \"content\": \"...\"}], ...]\n",
        "    # We need the content string.\n",
        "    completion_contents = [comp[0][\"content\"] for comp in completions if comp and comp[0] and \"content\" in comp[0]]\n",
        "    rewards_list = [1.0 if re.match(pattern, content, re.DOTALL) else 0.0 for content in completion_contents]\n",
        "    return rewards_list\n",
        "\n",
        "def accuracy_reward(completions, **kwargs):\n",
        "    \"\"\"Reward function that checks if the answer in the completion matches the ground truth solution.\"\"\"\n",
        "    # 'solution' column from the dataset is passed via kwargs by GRPOTrainer if remove_unused_columns=False\n",
        "    solutions = kwargs[\"solution\"] # This will be a list of solutions, one for each prompt in the batch\n",
        "\n",
        "    # Completions is List[List[Dict[str, str]]], where outer list corresponds to prompts,\n",
        "    # inner list corresponds to `num_generations` per prompt.\n",
        "    # We need to align generated completions with their original solutions.\n",
        "    # GRPOTrainer calls reward_funcs with `completions` shaped (batch_size * num_generations, 1, dict)\n",
        "    # and `solution` will be of length `batch_size`. We need to repeat/align `solution`.\n",
        "    # The GRPOTrainer's documentation/source should clarify how kwargs are batched.\n",
        "    # Assuming `solution` corresponds to the prompts, and we have `num_generations` for each.\n",
        "    # Let's assume for now `completions` and `solutions` are aligned by GRPOTrainer or this needs adjustment.\n",
        "    # The provided example implies completions and solutions are already aligned or broadcasted.\n",
        "    # `completions` is (batch_size * num_generations), `solutions` is (batch_size)\n",
        "    # Let `num_generations` be from `training_args`.\n",
        "\n",
        "    num_gens_per_prompt = kwargs.get(\"num_generations\", 1) # Get from training_args if possible or infer\n",
        "    if \"num_generations\" not in kwargs and trainer_args: # Access from global if defined\n",
        "        num_gens_per_prompt = trainer_args.num_generations\n",
        "\n",
        "    expanded_solutions = []\n",
        "    for sol in solutions:\n",
        "        expanded_solutions.extend([sol] * num_gens_per_prompt)\n",
        "\n",
        "    completion_contents = [comp[0][\"content\"] for comp in completions if comp and comp[0] and \"content\" in comp[0]]\n",
        "    rewards = []\n",
        "\n",
        "    for content, solution_text in zip(completion_contents, expanded_solutions):\n",
        "        gold_parsed = parse(solution_text, extraction_mode=\"first_match\", extraction_config=[LatexExtractionConfig()])\n",
        "        # Extract from <answer> tag in generated content\n",
        "        answer_match = re.search(r\"<answer>(.*?)</answer>\", content, re.DOTALL)\n",
        "        if answer_match:\n",
        "            answer_text = answer_match.group(1).strip()\n",
        "            answer_parsed = parse(answer_text, extraction_mode=\"first_match\", extraction_config=[LatexExtractionConfig()])\n",
        "            if len(gold_parsed) != 0 and len(answer_parsed) != 0:\n",
        "                try:\n",
        "                    rewards.append(float(verify(answer_parsed, gold_parsed)))\n",
        "                except Exception:\n",
        "                    rewards.append(0.0) # Error during verification\n",
        "            elif len(gold_parsed) == 0 and len(answer_parsed) == 0 : # Both empty (e.g. non-math textual answer)\n",
        "                 rewards.append(1.0 if answer_text == solution_text else 0.0) # Simple string match for non-parsable\n",
        "            elif len(gold_parsed) == 0 and len(answer_parsed) != 0 : # Gold is empty, answer is not\n",
        "                rewards.append(0.0)\n",
        "            else: # Gold is not empty, answer is empty or unparsable\n",
        "                rewards.append(0.0)\n",
        "        else:\n",
        "            rewards.append(0.0) # No <answer> tag\n",
        "    return rewards\n",
        "\n",
        "# --- Training Arguments and Trainer ---\n",
        "print(\"Configuring training arguments...\")\n",
        "# Conditional mixed precision based on hardware support\n",
        "use_bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
        "use_fp16 = torch.cuda.is_available() and not use_bf16\n",
        "\n",
        "# Global placeholder for training_args to be accessible by reward function if needed for num_generations\n",
        "trainer_args = None\n",
        "\n",
        "training_args = GRPOConfig(\n",
        "    output_dir=TRAINED_MODEL_DIR,\n",
        "    learning_rate=1e-5,       # Typical learning rate for fine-tuning\n",
        "    remove_unused_columns=False, # Crucial for accessing 'solution' in accuracy_reward\n",
        "    gradient_accumulation_steps=4, # Adjust based on VRAM. Original: 16. Reduced for wider compatibility.\n",
        "    num_train_epochs=1,        # Number of training epochs\n",
        "    bf16=use_bf16,             # Use bfloat16 if supported\n",
        "    fp16=use_fp16,             # Use float16 if bfloat16 not supported but CUDA is available\n",
        "\n",
        "    # per_device_train_batch_size: From TrainingArguments, defaults to 8.\n",
        "    # This is the number of prompts processed for loss computation by one device.\n",
        "    per_device_train_batch_size=2, # Adjust based on VRAM. Original: 8 (default). Reduced.\n",
        "\n",
        "    # batch_size (for GRPO generation step): From GRPOConfig, defaults to 1.\n",
        "    # Number of prompts to generate completions for simultaneously.\n",
        "    # batch_size = 1, # Default in GRPOConfig\n",
        "\n",
        "    # Parameters controlling data preprocessing and generation during training\n",
        "    max_completion_length=128, # Max length of generated completions. Original: 64. Increased for math.\n",
        "    num_generations=2,         # Number of completions to generate per prompt. Original: 4. Reduced.\n",
        "    max_prompt_length=256,     # Max length of prompts. Original: 128. Increased.\n",
        "\n",
        "    # Reporting and saving\n",
        "    report_to=\"tensorboard\",   # For EC2, consider \"none\" or ensure TensorBoard is set up.\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",     # Save at the end of each epoch. Original: \"steps\"\n",
        "    # save_steps=10,           # Used if save_strategy=\"steps\". Original: 10\n",
        "    # save_total_limit=1,      # Optional: limit the number of saved checkpoints\n",
        "    push_to_hub=False,         # Set to True to push to Hugging Face Hub\n",
        "    # hub_model_id=\"your-username/your-model-name\", # Required if push_to_hub=True\n",
        ")\n",
        "trainer_args = training_args # Make it accessible globally for reward function\n",
        "\n",
        "tokenizer_for_training = AutoTokenizer.from_pretrained(model_id)\n",
        "if tokenizer_for_training.pad_token_id is None:\n",
        "    tokenizer_for_training.pad_token_id = tokenizer_for_training.eos_token_id\n",
        "    model.config.pad_token_id = model.config.eos_token_id # Ensure model config also updated\n",
        "\n",
        "trainer = GRPOTrainer(\n",
        "    model=model,\n",
        "    processing_class=tokenizer_for_training, # Pass tokenizer to GRPOTrainer\n",
        "    reward_funcs=[format_reward, accuracy_reward],\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        ")\n",
        "\n",
        "print(\"Starting training...\")\n",
        "trainer.train()\n",
        "print(\"Training finished.\")\n",
        "\n",
        "# --- Save Model ---\n",
        "print(f\"Saving fine-tuned model (adapter) and tokenizer to {TRAINED_MODEL_DIR}...\")\n",
        "trainer.save_model(TRAINED_MODEL_DIR) # Saves adapter & tokenizer\n",
        "# tokenizer_for_training.save_pretrained(TRAINED_MODEL_DIR) # Trainer should do this\n",
        "print(f\"Model and tokenizer saved to {TRAINED_MODEL_DIR}.\")\n",
        "\n",
        "# Optional: Zip the model directory if needed for transfer\n",
        "# shutil.make_archive(f\"{TRAINED_MODEL_DIR}_archive\", \"zip\", TRAINED_MODEL_DIR)\n",
        "# print(f\"Model archived to {TRAINED_MODEL_DIR}_archive.zip\")\n",
        "\n",
        "\n",
        "# --- Inference with the Fine-tuned Model ---\n",
        "print(\"\\n--- Starting Inference with the Fine-tuned Model ---\")\n",
        "\n",
        "# Load the fine-tuned model (base model + adapter) and tokenizer\n",
        "# AutoModelForCausalLM.from_pretrained on a PEFT-saved directory\n",
        "# should load the base model and apply the adapter.\n",
        "print(f\"Loading fine-tuned model from {TRAINED_MODEL_DIR} for inference...\")\n",
        "try:\n",
        "    inference_model = AutoModelForCausalLM.from_pretrained(\n",
        "        TRAINED_MODEL_DIR,\n",
        "        torch_dtype=\"auto\", # Use appropriate dtype\n",
        "        device_map=\"auto\",  # Load on available device\n",
        "    )\n",
        "    # Tokenizer should have been saved by trainer.save_model()\n",
        "    inference_tokenizer = AutoTokenizer.from_pretrained(TRAINED_MODEL_DIR)\n",
        "    print(\"Successfully loaded fine-tuned model and tokenizer for inference.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model directly from {TRAINED_MODEL_DIR}: {e}\")\n",
        "    print(\"Attempting fallback: loading base model and then attaching adapter...\")\n",
        "    base_model_for_inference = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id, # Base model ID\n",
        "        torch_dtype=\"auto\",\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "    inference_model = PeftModel.from_pretrained(base_model_for_inference, TRAINED_MODEL_DIR)\n",
        "    # Optional: merge LoRA weights for potentially faster inference if not done automatically.\n",
        "    # This replaces the model with the merged version.\n",
        "    # inference_model = inference_model.merge_and_unload()\n",
        "    inference_tokenizer = AutoTokenizer.from_pretrained(model_id) # Fallback to base tokenizer\n",
        "    print(\"Fallback loading complete.\")\n",
        "\n",
        "# Ensure pad_token_id is set for the inference tokenizer\n",
        "if inference_tokenizer.pad_token_id is None:\n",
        "    inference_tokenizer.pad_token_id = inference_tokenizer.eos_token_id\n",
        "\n",
        "# Determine device for pipeline (GPU if available, else CPU)\n",
        "pipeline_device = 0 if torch.cuda.is_available() else -1\n",
        "\n",
        "print(f\"Creating text-generation pipeline on device: {'cuda:0' if pipeline_device == 0 else 'cpu'}\")\n",
        "gen_pipeline = pipeline(\n",
        "    'text-generation',\n",
        "    model=inference_model,\n",
        "    tokenizer=inference_tokenizer,\n",
        "    device=pipeline_device\n",
        ")\n",
        "\n",
        "# Test prompts\n",
        "prompts_for_testing = [\n",
        "    \"Explain Group Relative Policy Optimization in simple terms.\",\n",
        "    \"In 1988, a person's age was equal to the sum of the digits of their birth year. How old was this person?\"\n",
        "]\n",
        "\n",
        "for user_prompt_text in prompts_for_testing:\n",
        "    print(f\"\\nUser Prompt: {user_prompt_text}\")\n",
        "\n",
        "    # Format the prompt using the chat template, including the system prompt used during training\n",
        "    messages_for_inference = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": user_prompt_text},\n",
        "    ]\n",
        "\n",
        "    if inference_tokenizer.chat_template:\n",
        "        formatted_model_input = inference_tokenizer.apply_chat_template(\n",
        "            messages_for_inference,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True  # Crucial for instruct models to know it's their turn\n",
        "        )\n",
        "    else:\n",
        "        # Fallback if no chat template (should not happen for Qwen2-Instruct)\n",
        "        print(\"Warning: Tokenizer does not have a chat template. Using basic formatting.\")\n",
        "        formatted_model_input = f\"{SYSTEM_PROMPT}\\nUser: {user_prompt_text}\\nAssistant:\"\n",
        "\n",
        "    print(f\"--- Formatted Input to Model ---\\n{formatted_model_input}\\n-------------------------------\")\n",
        "\n",
        "    # Generate text\n",
        "    outputs = gen_pipeline(\n",
        "        formatted_model_input,\n",
        "        max_new_tokens=512,  # Max tokens for the generated response\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        #eos_token_id=inference_tokenizer.eos_token_id, # Pipeline usually handles this\n",
        "        #pad_token_id=inference_tokenizer.pad_token_id # Pipeline usually handles this\n",
        "    )\n",
        "\n",
        "    generated_full_text = outputs[0]['generated_text']\n",
        "\n",
        "    # Extract only the newly generated assistant's reply\n",
        "    # The `formatted_model_input` is what we sent.\n",
        "    # The `generated_full_text` contains this input + the model's generation.\n",
        "    assistant_reply = generated_full_text\n",
        "    if generated_full_text.startswith(formatted_model_input):\n",
        "        assistant_reply = generated_full_text[len(formatted_model_input):].strip()\n",
        "    else:\n",
        "        # If the pipeline behaves differently or input string had subtle changes (e.g. whitespace)\n",
        "        # try to find the last part based on assistant markers if possible.\n",
        "        # For Qwen2, this would be after \"<|im_start|>assistant\\n\"\n",
        "        # This is a heuristic if the direct prefix strip fails.\n",
        "        assistant_marker = None\n",
        "        if \"qwen2\" in inference_tokenizer.name_or_path.lower():\n",
        "             assistant_marker = \"<|im_start|>assistant\" # Don't include \\n as it might vary slightly\n",
        "\n",
        "        if assistant_marker:\n",
        "            last_occurrence_idx = generated_full_text.rfind(assistant_marker)\n",
        "            if last_occurrence_idx != -1:\n",
        "                # Find the end of the marker (e.g., after the newline)\n",
        "                end_of_marker_idx = generated_full_text.find(\"\\n\", last_occurrence_idx)\n",
        "                if end_of_marker_idx != -1:\n",
        "                     assistant_reply = generated_full_text[end_of_marker_idx+1:].strip()\n",
        "\n",
        "    # Clean up trailing special tokens like <|im_end|>\n",
        "    if assistant_reply.endswith(inference_tokenizer.eos_token) or \\\n",
        "       (hasattr(inference_tokenizer, 'special_tokens_map') and \\\n",
        "        'im_end_token' in inference_tokenizer.special_tokens_map and \\\n",
        "        assistant_reply.endswith(inference_tokenizer.special_tokens_map['im_end_token'])):\n",
        "\n",
        "        if assistant_reply.endswith(inference_tokenizer.eos_token):\n",
        "            assistant_reply = assistant_reply[:-len(inference_tokenizer.eos_token)].strip()\n",
        "\n",
        "        if hasattr(inference_tokenizer, 'special_tokens_map') and \\\n",
        "           'im_end_token' in inference_tokenizer.special_tokens_map and \\\n",
        "           assistant_reply.endswith(inference_tokenizer.special_tokens_map['im_end_token']):\n",
        "           assistant_reply = assistant_reply[:-len(inference_tokenizer.special_tokens_map['im_end_token'])].strip()\n",
        "\n",
        "\n",
        "    print(f\"--- Assistant's Reply ---\\n{assistant_reply}\\n-------------------------\")\n",
        "\n",
        "print(\"\\nScript finished.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 729
        },
        "id": "MNWx4qQqeS-S",
        "outputId": "501f5ed5-3aa3-4136-f7df-ed738e9e3c8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HUGGING_FACE_HUB_TOKEN not found. Attempting interactive login if needed by model download.\n",
            "Loading dataset: AI-MO/NuminaMath-TIR\n",
            "Loaded raw train dataset: Dataset({\n",
            "    features: ['problem', 'solution', 'messages'],\n",
            "    num_rows: 724\n",
            "})\n",
            "Processed train dataset: Dataset({\n",
            "    features: ['solution', 'prompt'],\n",
            "    num_rows: 724\n",
            "})\n",
            "Train dataset features: {'solution': Value(dtype='string', id=None), 'prompt': [{'content': Value(dtype='string', id=None), 'role': Value(dtype='string', id=None)}]}\n",
            "Loading base model: Qwen/Qwen2-0.5B-Instruct\n",
            "Trainable parameters after PEFT setup:\n",
            "trainable params: 540,672 || all params: 494,573,440 || trainable%: 0.1093\n",
            "Configuring training arguments...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5' max='181' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  5/181 00:33 < 32:16, 0.09 it/s, Epoch 0.02/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-cac4844317a1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training finished.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2244\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2245\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2246\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2247\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2558\u001b[0m                     )\n\u001b[1;32m   2559\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2560\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2562\u001b[0m                     if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3728\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3730\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3731\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_sagemaker_mp_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3732\u001b[0m             \u001b[0mloss_mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmp_forward_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/trl/extras/profiling.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mprofiling_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/trl/trainer/grpo_trainer.py\u001b[0m in \u001b[0;36m_prepare_inputs\u001b[0;34m(self, accumulated_local_batch)\u001b[0m\n\u001b[1;32m    897\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mgenerate_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffered_inputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m                 \u001b[0;31m# self._buffered_inputs=None can occur when resuming from a checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m                 \u001b[0maccumulated_local_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate_and_score_completions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccumulated_local_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    900\u001b[0m                 self._buffered_inputs = split_tensor_dict(\n\u001b[1;32m    901\u001b[0m                     \u001b[0maccumulated_local_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/trl/trainer/grpo_trainer.py\u001b[0m in \u001b[0;36m_generate_and_score_completions\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    972\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_wrapped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgather_deepspeed3_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mds3_gather_for_generation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m             ) as unwrapped_model:\n\u001b[0;32m--> 974\u001b[0;31m                 prompt_completion_ids = unwrapped_model.generate(\n\u001b[0m\u001b[1;32m    975\u001b[0m                     \u001b[0mprompt_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneration_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1873\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_peft_forward_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1874\u001b[0m                     \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_peft_forward_args\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1875\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1876\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1877\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[1;32m   2463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2464\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2465\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2466\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2467\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3432\u001b[0m                 \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3433\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3434\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3436\u001b[0m             \u001b[0;31m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_requested_to_return_tuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_configured_to_return_tuple\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_top_level_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         outputs: BaseModelOutputWithPast = self.model(\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_requested_to_return_tuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_configured_to_return_tuple\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_top_level_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    547\u001b[0m                 )\n\u001b[1;32m    548\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    550\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         hidden_states, self_attn_weights = self.self_attn(\n\u001b[0m\u001b[1;32m    263\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0mcos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mposition_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0mquery_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_rotary_pos_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mapply_rotary_pos_emb\u001b[0;34m(q, k, cos, sin, position_ids, unsqueeze_dim)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mcos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munsqueeze_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0msin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munsqueeze_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mq_embed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcos\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrotate_half\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0mk_embed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcos\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrotate_half\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mq_embed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_embed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mrotate_half\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%capture\n",
        "# !pip install transformers torch accelerate sentencepiece\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import numpy as np\n",
        "import re\n",
        "import random\n",
        "\n",
        "# --- Model Setup ---\n",
        "MODEL_NAME = \"Qwen/Qwen1.5-0.5B-Chat\" # Using a 0.5B Qwen Chat model\n",
        "\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        torch_dtype=\"auto\", # Use bfloat16 if available, otherwise float16\n",
        "        device_map=\"auto\"   # Automatically use GPU if available\n",
        "    )\n",
        "    print(f\"Successfully loaded model and tokenizer: {MODEL_NAME}\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"Model is on GPU: {model.device}\")\n",
        "    else:\n",
        "        print(\"Model is on CPU. Execution might be slow.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")\n",
        "    print(\"Please ensure you have internet access and the model name is correct.\")\n",
        "    print(\"If you are repeatedly facing issues, try restarting the Colab runtime.\")\n",
        "    model = None # Set model to None if loading fails\n",
        "\n",
        "\n",
        "# --- DSL Definition and Parser ---\n",
        "# Operands: A[r,c], B[r,c], M<id>\n",
        "# C[r,c] = <operand> OR M<id> = <operand1> <op> <operand2>\n",
        "\n",
        "# Regex for parsing DSL lines\n",
        "# M0 = A[0,0] + B[0,0]  OR  M1 = M0 * A[1,1] OR C[0,0] = M1\n",
        "ASSIGN_INTERMEDIATE_REGEX = re.compile(r\"^\\s*(M\\d+)\\s*=\\s*([A-Z]\\[\\d,\\d\\]|M\\d+)\\s*([+\\-*])\\s*([A-Z]\\[\\d,\\d\\]|M\\d+)\\s*$\")\n",
        "ASSIGN_OUTPUT_REGEX = re.compile(r\"^\\s*(C\\[\\d,\\d\\])\\s*=\\s*([A-Z]\\[\\d,\\d\\]|M\\d+)\\s*$\")\n",
        "ASSIGN_INTERMEDIATE_SINGLE_REGEX = re.compile(r\"^\\s*(M\\d+)\\s*=\\s*([A-Z]\\[\\d,\\d\\]|M\\d+)\\s*$\") # M0 = A[0,0]\n",
        "\n",
        "def parse_dsl_step(line):\n",
        "    \"\"\"Parses a single DSL step string.\"\"\"\n",
        "    match_inter = ASSIGN_INTERMEDIATE_REGEX.match(line)\n",
        "    if match_inter:\n",
        "        return {\"type\": \"intermediate_op\", \"out\": match_inter.group(1), \"op1\": match_inter.group(2),\n",
        "                \"op_type\": match_inter.group(3), \"op2\": match_inter.group(4)}\n",
        "\n",
        "    match_output = ASSIGN_OUTPUT_REGEX.match(line)\n",
        "    if match_output:\n",
        "        return {\"type\": \"output_assign\", \"out_cell\": match_output.group(1), \"source\": match_output.group(2)}\n",
        "\n",
        "    match_inter_single = ASSIGN_INTERMEDIATE_SINGLE_REGEX.match(line)\n",
        "    if match_inter_single: # For M0 = A[0,0] type assignments\n",
        "        return {\"type\": \"intermediate_assign_single\", \"out\": match_inter_single.group(1), \"source\": match_inter_single.group(2)}\n",
        "\n",
        "    return None # Parse error\n",
        "\n",
        "# --- Algorithm Executor ---\n",
        "class MatrixMathState:\n",
        "    def __init__(self, a, b):\n",
        "        if not (a.shape == (2,2) and b.shape == (2,2)):\n",
        "            raise ValueError(\"Matrices A and B must be 2x2.\")\n",
        "        self.matrix_a = a\n",
        "        self.matrix_b = b\n",
        "        self.expected_c = np.matmul(a, b)\n",
        "        self.intermediate_vars = {} # Store M0, M1, ...\n",
        "        self.output_c = np.full((2,2), np.nan) # Initialize with NaNs\n",
        "        self.errors = []\n",
        "        self.dsl_steps_executed = []\n",
        "\n",
        "    def get_operand_value(self, operand_str):\n",
        "        if operand_str.startswith(\"M\"):\n",
        "            if operand_str in self.intermediate_vars:\n",
        "                return self.intermediate_vars[operand_str]\n",
        "            else:\n",
        "                self.errors.append(f\"Error: Intermediate variable {operand_str} not found.\")\n",
        "                return None\n",
        "        elif operand_str.startswith(\"A[\"):\n",
        "            try:\n",
        "                r, c = map(int, operand_str[2:-1].split(','))\n",
        "                return self.matrix_a[r,c]\n",
        "            except:\n",
        "                self.errors.append(f\"Error: Invalid format or index for matrix A operand: {operand_str}\")\n",
        "                return None\n",
        "        elif operand_str.startswith(\"B[\"):\n",
        "            try:\n",
        "                r, c = map(int, operand_str[2:-1].split(','))\n",
        "                return self.matrix_b[r,c]\n",
        "            except:\n",
        "                self.errors.append(f\"Error: Invalid format or index for matrix B operand: {operand_str}\")\n",
        "                return None\n",
        "        self.errors.append(f\"Error: Unknown operand type: {operand_str}\")\n",
        "        return None\n",
        "\n",
        "    def execute_step(self, parsed_step, original_line=\"\"):\n",
        "        if parsed_step is None:\n",
        "            self.errors.append(f\"Parse Error for line: {original_line}\")\n",
        "            return False\n",
        "\n",
        "        op_type = parsed_step[\"type\"]\n",
        "        success = False\n",
        "\n",
        "        if op_type == \"intermediate_op\":\n",
        "            val1 = self.get_operand_value(parsed_step[\"op1\"])\n",
        "            val2 = self.get_operand_value(parsed_step[\"op2\"])\n",
        "            if val1 is not None and val2 is not None:\n",
        "                op = parsed_step[\"op_type\"]\n",
        "                if op == '+': result = val1 + val2\n",
        "                elif op == '-': result = val1 - val2\n",
        "                elif op == '*': result = val1 * val2\n",
        "                else:\n",
        "                    self.errors.append(f\"Unknown arithmetic operator: {op}\")\n",
        "                    return False\n",
        "                self.intermediate_vars[parsed_step[\"out\"]] = result\n",
        "                success = True\n",
        "        elif op_type == \"intermediate_assign_single\":\n",
        "            val_source = self.get_operand_value(parsed_step[\"source\"])\n",
        "            if val_source is not None:\n",
        "                self.intermediate_vars[parsed_step[\"out\"]] = val_source\n",
        "                success = True\n",
        "        elif op_type == \"output_assign\":\n",
        "            val_source = self.get_operand_value(parsed_step[\"source\"])\n",
        "            if val_source is not None:\n",
        "                try:\n",
        "                    cell_str = parsed_step[\"out_cell\"] # C[r,c]\n",
        "                    r, c = map(int, cell_str[2:-1].split(','))\n",
        "                    if 0 <= r < 2 and 0 <= c < 2:\n",
        "                        self.output_c[r,c] = val_source\n",
        "                        success = True\n",
        "                    else:\n",
        "                        self.errors.append(f\"Error: Output cell C[{r},{c}] out of bounds.\")\n",
        "                except Exception as e:\n",
        "                    self.errors.append(f\"Error parsing output cell {parsed_step['out_cell']}: {e}\")\n",
        "        else:\n",
        "            self.errors.append(f\"Unknown parsed step type: {op_type}\")\n",
        "\n",
        "        if success:\n",
        "            self.dsl_steps_executed.append(original_line if original_line else str(parsed_step))\n",
        "        return success\n",
        "\n",
        "    def is_c_complete(self):\n",
        "        return not np.isnan(self.output_c).any()\n",
        "\n",
        "    def verify_c(self):\n",
        "        if not self.is_c_complete():\n",
        "            return False\n",
        "        return np.allclose(self.output_c, self.expected_c)\n",
        "\n",
        "# --- LLM Interaction Logic ---\n",
        "def get_llm_step_suggestion(current_prompt_messages):\n",
        "    if model is None or tokenizer is None:\n",
        "        return \"Error: Model not loaded.\"\n",
        "\n",
        "    # The Qwen1.5-Chat model expects a list of messages\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        current_prompt_messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=50, # Max length of the DSL step string\n",
        "        pad_token_id=tokenizer.eos_token_id, # Important for stopping criteria\n",
        "        do_sample=True, # Enable sampling for more diverse outputs\n",
        "        temperature=0.7, # Control randomness; lower is more deterministic\n",
        "        top_k=50,\n",
        "        top_p=0.95\n",
        "    )\n",
        "    response_ids = outputs[0][input_ids.shape[-1]:] # Get only the generated tokens\n",
        "    llm_output = tokenizer.decode(response_ids, skip_special_tokens=True).strip()\n",
        "    return llm_output\n",
        "\n",
        "def generate_matrix_mult_algorithm(matrix_a_str, matrix_b_str, max_llm_steps=20):\n",
        "    \"\"\"\n",
        "    Iteratively prompts the LLM to generate a full algorithm.\n",
        "    \"\"\"\n",
        "    state = MatrixMathState(eval(matrix_a_str), eval(matrix_b_str)) # eval is generally unsafe, but used here for simplicity with np.array strings\n",
        "\n",
        "    # System message to set the context for the chat model\n",
        "    system_message = f\"\"\"You are an AI assistant helping to generate steps for a 2x2 matrix multiplication: C = A * B.\n",
        "Your task is to output ONLY ONE valid DSL step at a time to compute the matrix C.\n",
        "Matrices A and B are 2x2.\n",
        "Valid DSL Step Formats:\n",
        "1. Define an intermediate variable M<id> (e.g., M0, M1, M2, ...):\n",
        "   M<id> = <operand1> + <operand2>\n",
        "   M<id> = <operand1> - <operand2>\n",
        "   M<id> = <operand1> * <operand2>\n",
        "   M<id> = <operand>  (e.g. M0 = A[0,0])\n",
        "   Operands can be A[r,c], B[r,c], or a previously defined M<id>.\n",
        "2. Assign to an output cell C[r,c]:\n",
        "   C[r,c] = <operand> (where operand is an M<id> or A[r,c] or B[r,c])\n",
        "\n",
        "Example of a valid first step: M0 = A[0,0] * B[0,0]\n",
        "Another example: M1 = A[0,1] + A[1,1]\n",
        "Then, for example: C[0,0] = M0 (if M0 was defined as a product for C[0,0])\n",
        "\n",
        "Do not output any explanations, just the single DSL line.\n",
        "Focus on correctness and completing all C[0,0], C[0,1], C[1,0], C[1,1].\n",
        "\"\"\"\n",
        "\n",
        "    # Initialize conversation history with the system message\n",
        "    conversation_history = [{\"role\": \"system\", \"content\": system_message}]\n",
        "\n",
        "    for step_num in range(max_llm_steps):\n",
        "        if state.is_c_complete():\n",
        "            break\n",
        "\n",
        "        current_state_summary = f\"Current algorithm state:\\nMatrices:\\nA = {matrix_a_str}\\nB = {matrix_b_str}\\n\"\n",
        "        current_state_summary += \"Executed Steps:\\n\" + (\"\\n\".join(state.dsl_steps_executed) if state.dsl_steps_executed else \"None yet.\") + \"\\n\"\n",
        "        current_state_summary += f\"Intermediate Variables (M values): {state.intermediate_vars}\\n\"\n",
        "        current_state_summary += f\"Output Matrix C (NaN means not yet computed):\\n{state.output_c}\\n\"\n",
        "        current_state_summary += \"Provide the NEXT SINGLE DSL step:\"\n",
        "\n",
        "        # Add user message to conversation\n",
        "        user_message = {\"role\": \"user\", \"content\": current_state_summary}\n",
        "        prompt_messages_for_llm = conversation_history + [user_message]\n",
        "\n",
        "        llm_dsl_line = get_llm_step_suggestion(prompt_messages_for_llm)\n",
        "\n",
        "        if llm_dsl_line.startswith(\"Error:\"): # Handle model loading error\n",
        "            state.errors.append(llm_dsl_line)\n",
        "            break # Stop if model isn't working\n",
        "\n",
        "        # Add assistant's response (the DSL line) to history for next turn\n",
        "        assistant_message = {\"role\": \"assistant\", \"content\": llm_dsl_line}\n",
        "        conversation_history.append(user_message) # Add user's prompt that led to this\n",
        "        conversation_history.append(assistant_message) # Add assistant's reply\n",
        "\n",
        "\n",
        "        parsed = parse_dsl_step(llm_dsl_line)\n",
        "\n",
        "        if parsed:\n",
        "            if not state.execute_step(parsed, llm_dsl_line):\n",
        "                # Error already logged in state.execute_step\n",
        "                # Decide if we want to stop on first execution error, or let LLM try to recover\n",
        "                # For this test, let's log and continue, LLM might output something different next\n",
        "                print(f\"  LLM Output (Step {step_num+1}): '{llm_dsl_line}' -> Parsed, but execution failed. Error: {state.errors[-1] if state.errors else 'Unknown exec error'}\")\n",
        "            else:\n",
        "                 print(f\"  LLM Output (Step {step_num+1}): '{llm_dsl_line}' -> Executed successfully.\")\n",
        "        else:\n",
        "            state.errors.append(f\"LLM output could not be parsed: '{llm_dsl_line}'\")\n",
        "            print(f\"  LLM Output (Step {step_num+1}): '{llm_dsl_line}' -> Parse Error.\")\n",
        "            # Optionally, break if LLM consistently gives unparsable output\n",
        "\n",
        "    return state\n",
        "\n",
        "# --- Main Test Loop ---\n",
        "if model is None:\n",
        "    print(\"\\nSkipping main test loop as model failed to load.\")\n",
        "else:\n",
        "    print(\"\\n--- Starting 2x2 Matrix Multiplication Test ---\")\n",
        "    num_tests = 10\n",
        "    correct_algorithms = 0\n",
        "\n",
        "    for i in range(num_tests):\n",
        "        print(f\"\\n--- Test Case {i+1}/{num_tests} ---\")\n",
        "        # Generate random 2x2 matrices with integer values for simplicity\n",
        "        # Using np.array representation directly in strings for the prompt\n",
        "        # This avoids issues with LLM misinterpreting list of lists syntax sometimes\n",
        "        mat_a_np = np.random.randint(-5, 6, size=(2,2))\n",
        "        mat_b_np = np.random.randint(-5, 6, size=(2,2))\n",
        "\n",
        "        # Convert numpy arrays to string representations for the prompt\n",
        "        # e.g., \"np.array([[1, 2], [3, 4]])\"\n",
        "        mat_a_str = f\"np.array({mat_a_np.tolist()})\"\n",
        "        mat_b_str = f\"np.array({mat_b_np.tolist()})\"\n",
        "\n",
        "        print(f\"Matrix A:\\n{mat_a_np}\")\n",
        "        print(f\"Matrix B:\\n{mat_b_np}\")\n",
        "        expected_c = np.matmul(mat_a_np, mat_b_np)\n",
        "        print(f\"Expected C:\\n{expected_c}\")\n",
        "\n",
        "        final_state = generate_matrix_mult_algorithm(mat_a_str, mat_b_str)\n",
        "\n",
        "        print(\"\\nGenerated DSL Algorithm:\")\n",
        "        if final_state.dsl_steps_executed:\n",
        "            for step_line in final_state.dsl_steps_executed:\n",
        "                print(step_line)\n",
        "        else:\n",
        "            print(\"No valid DSL steps were executed.\")\n",
        "\n",
        "        if final_state.errors:\n",
        "            print(\"\\nErrors encountered during generation/execution:\")\n",
        "            for err in final_state.errors:\n",
        "                print(f\"- {err}\")\n",
        "\n",
        "        print(f\"\\nFinal Computed C (by LLM algorithm):\\n{final_state.output_c}\")\n",
        "\n",
        "        if final_state.is_c_complete():\n",
        "            if final_state.verify_c():\n",
        "                print(\"Result: CORRECT\")\n",
        "                correct_algorithms += 1\n",
        "            else:\n",
        "                print(\"Result: INCORRECT (C is complete but values are wrong)\")\n",
        "        else:\n",
        "            print(\"Result: INCOMPLETE (C was not fully computed)\")\n",
        "\n",
        "    print(f\"\\n--- Test Summary ---\")\n",
        "    print(f\"Total correct algorithms: {correct_algorithms}/{num_tests} ({correct_algorithms/num_tests*100:.2f}%)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "c2c4ce6f7ba44a83ad30cbf93e517a39",
            "5ce2aebb0b624bed8fd5644aa4083f12",
            "4965de5bb3dc45e1bfb680683163bf39",
            "371cbe5ccc394f42b250aa3dfdc5c0d8",
            "a8a0a110d5b449009494feeeb4b1c33d",
            "459f4d4327ad40d1945071ab25d68b47",
            "5e69bafe78f44f4e8985318aa9037fd3",
            "335620f25fbe41b8a0cd9c68be4fb056",
            "40500fe3a021491ea73e7df3d59f6ef9",
            "53de218bb9f0496bb8d796190259a484",
            "0cab8bf53e2e43be84c4a3a6314ac7b4",
            "a77e2933c65345f9b203ab982c3b0ed2",
            "4fbbe512dae647a6ae4ea1efa8af3801",
            "ac2a03537f344e09904650df2c76be5c",
            "17ae356def2f4f1295af8f14f700fa6b",
            "78639a2ed4374a848f8356501b950d9f",
            "1055f60bb0394ce9b9bf29a97676ed72",
            "298ab97f90da435ab8b90d7b33c0e6d6",
            "fc9c09ac71bd443499aeef5b122f946c",
            "e512f80800144e5c8bb61185f0a7296d",
            "1df98dc84ba34ed8ab52bc14b66bbf12",
            "10360a5bcdaf4e05b2214233f5ee1082",
            "b2e5e901a73542ac8a023cef2bedc3ae",
            "fef911f0bf3c4a4ea0181076e624117d",
            "af6ffa6caa084e04bcf48327ecff633b",
            "d1f0e8f42e504b5cbfe43ee9c3aec5b7",
            "f5e0adb86f634fb9b3db097be18c6edc",
            "4202c658a0774594a6929d1b4bc83828",
            "378825f09e9c419ca48f7cc3dd6b5adc",
            "c10cc846da474dafaa7504c7389fb0b0",
            "8c91baf8340840e7ae8d58132356b579",
            "35d7d2f4101a49b9bd2bd7284f09db95",
            "726f6a65d47b440c9914e758a809b077",
            "15ec1574b3934fe3b7bd25d2af54b2bd",
            "de5d85bf4ab04a268ebfc91e51752e7f",
            "9bf7b5392fff40f6a9259a59ff3ff2da",
            "4e9c7e5e5b564ab1b511b73a7db884af",
            "c4c2e1091d4541e8affb6358b32c167b",
            "d7fe4713309b4aa99a83c4dc646ba9b4",
            "f4638bc4e6a64629a0649d165f5314c4",
            "7532414a4bbb46cc8370698aa5fc3168",
            "835c31cc178040cca9dd62409d49d3f3",
            "23314a2426d24dbab592054be867761b",
            "365c41cf78784c6e968b507485e74003",
            "00c6084f152743ab9c615dde1e3bcbbb",
            "9399b95574274358bd23a9a6ea0907a7",
            "1b7f628d3f9449f7b18d99cea23a4cbe",
            "66b03eee840f4fee919330f4b827fc89",
            "f99c4a9f3e044ccfa1b127bb59685648",
            "9a923b488c1e46e3809ae1f0e29ae628",
            "b7c0146733414eab9669088d609c79a7",
            "60e7fe8d95a04b1db4c0c8ca05d59a25",
            "2c9ca585bc4f46928b14e2226aed85b0",
            "a2a02f7fa0f2451eb84a4789cee60cef",
            "dced3593c4864e85b0952b60cf111974",
            "8ffeaec3d81747eeafd701505c8e2e38",
            "ea33c278c6684419ae1195ac8626fb01",
            "d64a5c9f4d5644db8e58176e6f002235",
            "55287b9957e14725af071b4abdd472be",
            "a0ce9fe9de114aeeb047bc7d5a80b2e8",
            "f4593a0ca90344f8a1b0e759a376afac",
            "105c64c6e7224d9d85242339548b86aa",
            "72c520d7b39d4d45a318e460b18fcdc1",
            "8fa6adc6b7b64c0db191dccc437c51f2",
            "e3f3c18d54eb48459f3325e5a4a424e5",
            "c56f014dd292477d91bf60af19614d99",
            "259acd8944d14a20a146cca6cca21e13",
            "667af96db43542d1aaa31e4b17a3be21",
            "bc35496d55994b71aca014d5a41cbbc3",
            "7d12cfab7edd40f5a4ba2b8bc257ace3",
            "1f220c2297b848fc9a9cf344debb4722",
            "9f48bf2484074471988847e34177e6d8",
            "9e9d848e25554fb5983cc198e447e406",
            "349ac936c6294a0dbf69db6992b2e8ea",
            "ec822aad480d496da18c53d9e57586ad",
            "5634a99f3c1c45d8806042b4576f3478",
            "06972232ea164adb94866ffcaedeacd5"
          ]
        },
        "id": "5deuifA3fB4i",
        "outputId": "83bf60c1-9bb2-45d6-aba8-55cc0c501d7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c2c4ce6f7ba44a83ad30cbf93e517a39"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a77e2933c65345f9b203ab982c3b0ed2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b2e5e901a73542ac8a023cef2bedc3ae"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "15ec1574b3934fe3b7bd25d2af54b2bd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/661 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "00c6084f152743ab9c615dde1e3bcbbb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.24G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8ffeaec3d81747eeafd701505c8e2e38"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/206 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "259acd8944d14a20a146cca6cca21e13"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded model and tokenizer: Qwen/Qwen1.5-0.5B-Chat\n",
            "Model is on GPU: cuda:0\n",
            "\n",
            "--- Starting 2x2 Matrix Multiplication Test ---\n",
            "\n",
            "--- Test Case 1/10 ---\n",
            "Matrix A:\n",
            "[[-1 -1]\n",
            " [-4 -4]]\n",
            "Matrix B:\n",
            "[[ 2  5]\n",
            " [ 1 -4]]\n",
            "Expected C:\n",
            "[[ -3  -1]\n",
            " [-12  -4]]\n",
            "  LLM Output (Step 1): 'Sure! Here's the next DSL step to compute the matrix C:\n",
            "```\n",
            "C = np.dot(A, B)\n",
            "```' -> Parse Error.\n",
            "  LLM Output (Step 2): 'Here's the next DSL step to compute the matrix C:\n",
            "```\n",
            "M = np.linalg.norm(C)\n",
            "C = A @ B\n",
            "```' -> Parse Error.\n",
            "  LLM Output (Step 3): 'Here's the next DSL step to compute the matrix C:\n",
            "```\n",
            "M = np.linalg.norm(np.dot(A, B))\n",
            "C = A @ B\n",
            "```' -> Parse Error.\n",
            "  LLM Output (Step 4): 'Here's the next DSL step to compute the matrix C:\n",
            "```\n",
            "M = np.linalg.norm(np.dot(A, B))\n",
            "C = A @ B\n",
            "```' -> Parse Error.\n",
            "  LLM Output (Step 5): 'Here's the next DSL step to compute the matrix C:\n",
            "```\n",
            "M = np.linalg.norm(np.dot(A, B))\n",
            "C = A @ B\n",
            "```' -> Parse Error.\n",
            "  LLM Output (Step 6): 'Here's the next DSL step to compute the matrix C:\n",
            "```\n",
            "M = np.linalg.norm(np.dot(A, B))\n",
            "C = A @ B\n",
            "```' -> Parse Error.\n",
            "  LLM Output (Step 7): 'Here's the next DSL step to compute the matrix C:\n",
            "```\n",
            "M = np.linalg.norm(np.dot(A, B))\n",
            "C = A @ B\n",
            "```' -> Parse Error.\n",
            "  LLM Output (Step 8): 'Here's the next DSL step to compute the matrix C:\n",
            "```\n",
            "M = np.linalg.norm(np.dot(A, B))\n",
            "C = A @ B\n",
            "```' -> Parse Error.\n",
            "  LLM Output (Step 9): 'Here's the next DSL step to compute the matrix C:\n",
            "```\n",
            "M = np.linalg.norm(np.dot(A, B))\n",
            "C = A @ B\n",
            "```' -> Parse Error.\n",
            "  LLM Output (Step 10): 'Here's the next DSL step to compute the matrix C:\n",
            "```\n",
            "M = np.linalg.norm(np.dot(A, B))\n",
            "C = A @ B\n",
            "```' -> Parse Error.\n",
            "  LLM Output (Step 11): 'Here's the next DSL step to compute the matrix C:\n",
            "```\n",
            "M = np.linalg.norm(np.dot(A, B))\n",
            "C = A @ B\n",
            "```' -> Parse Error.\n",
            "  LLM Output (Step 12): 'Here's the next DSL step to compute the matrix C:\n",
            "```\n",
            "M = np.linalg.norm(np.dot(A, B))\n",
            "C = A @ B\n",
            "```' -> Parse Error.\n",
            "  LLM Output (Step 13): 'Here's the next DSL step to compute the matrix C:\n",
            "```\n",
            "M = np.linalg.norm(np.dot(A, B))\n",
            "C = A @ B\n",
            "```' -> Parse Error.\n",
            "  LLM Output (Step 14): 'Here's the next DSL step to compute the matrix C:\n",
            "```\n",
            "M = np.linalg.norm(np.dot(A, B))\n",
            "C = A @ B\n",
            "```' -> Parse Error.\n",
            "  LLM Output (Step 15): 'Here's the next DSL step to compute the matrix C:\n",
            "```\n",
            "M = np.linalg.norm(np.dot(A, B))\n",
            "C = A @ B\n",
            "```' -> Parse Error.\n",
            "  LLM Output (Step 16): 'Here's the next DSL step to compute the matrix C:\n",
            "```\n",
            "M = np.linalg.norm(np.dot(A, B))\n",
            "C = A @ B\n",
            "```' -> Parse Error.\n",
            "  LLM Output (Step 17): 'Here's the next DSL step to compute the matrix C:\n",
            "```\n",
            "M = np.linalg.norm(np.dot(A, B))\n",
            "C = A @ B\n",
            "```' -> Parse Error.\n",
            "  LLM Output (Step 18): 'Here's the next DSL step to compute the matrix C:\n",
            "```\n",
            "M = np.linalg.norm(np.dot(A, B))\n",
            "C = A @ B\n",
            "```' -> Parse Error.\n",
            "  LLM Output (Step 19): 'Here's the next DSL step to compute the matrix C:\n",
            "```\n",
            "M = np.linalg.norm(np.dot(A, B))\n",
            "C = A @ B\n",
            "```' -> Parse Error.\n",
            "  LLM Output (Step 20): 'Here's the next DSL step to compute the matrix C:\n",
            "```\n",
            "M = np.linalg.norm(np.dot(A, B))\n",
            "C = A @ B\n",
            "```' -> Parse Error.\n",
            "\n",
            "Generated DSL Algorithm:\n",
            "No valid DSL steps were executed.\n",
            "\n",
            "Errors encountered during generation/execution:\n",
            "- LLM output could not be parsed: 'Sure! Here's the next DSL step to compute the matrix C:\n",
            "```\n",
            "C = np.dot(A, B)\n",
            "```'\n",
            "- LLM output could not be parsed: 'Here's the next DSL step to compute the matrix C:\n",
            "```\n",
            "M = np.linalg.norm(C)\n",
            "C = A @ B\n",
            "```'\n",
            "- LLM output could not be parsed: 'Here's the next DSL step to compute the matrix C:\n",
            "```\n",
            "M = np.linalg.norm(np.dot(A, B))\n",
            "C = A @ B\n",
            "```'\n",
            "- LLM output could not be parsed: 'Here's the next DSL step to compute the matrix C:\n",
            "```\n",
            "M = np.linalg.norm(np.dot(A, B))\n",
            "C = A @ B\n",
            "```'\n",
            "- LLM output could not be parsed: 'Here's the next DSL step to compute the matrix C:\n",
            "```\n",
            "M = np.linalg.norm(np.dot(A, B))\n",
            "C = A @ B\n",
            "```'\n",
            "- LLM output could not be parsed: 'Here's the next DSL step to compute the matrix C:\n",
            "```\n",
            "M = np.linalg.norm(np.dot(A, B))\n",
            "C = A @ B\n",
            "```'\n",
            "- LLM output could not be parsed: 'Here's the next DSL step to compute the matrix C:\n",
            "```\n",
            "M = np.linalg.norm(np.dot(A, B))\n",
            "C = A @ B\n",
            "```'\n",
            "- LLM output could not be parsed: 'Here's the next DSL step to compute the matrix C:\n",
            "```\n",
            "M = np.linalg.norm(np.dot(A, B))\n",
            "C = A @ B\n",
            "```'\n",
            "- LLM output could not be parsed: 'Here's the next DSL step to compute the matrix C:\n",
            "```\n",
            "M = np.linalg.norm(np.dot(A, B))\n",
            "C = A @ B\n",
            "```'\n",
            "- LLM output could not be parsed: 'Here's the next DSL step to compute the matrix C:\n",
            "```\n",
            "M = np.linalg.norm(np.dot(A, B))\n",
            "C = A @ B\n",
            "```'\n",
            "- LLM output could not be parsed: 'Here's the next DSL step to compute the matrix C:\n",
            "```\n",
            "M = np.linalg.norm(np.dot(A, B))\n",
            "C = A @ B\n",
            "```'\n",
            "- LLM output could not be parsed: 'Here's the next DSL step to compute the matrix C:\n",
            "```\n",
            "M = np.linalg.norm(np.dot(A, B))\n",
            "C = A @ B\n",
            "```'\n",
            "- LLM output could not be parsed: 'Here's the next DSL step to compute the matrix C:\n",
            "```\n",
            "M = np.linalg.norm(np.dot(A, B))\n",
            "C = A @ B\n",
            "```'\n",
            "- LLM output could not be parsed: 'Here's the next DSL step to compute the matrix C:\n",
            "```\n",
            "M = np.linalg.norm(np.dot(A, B))\n",
            "C = A @ B\n",
            "```'\n",
            "- LLM output could not be parsed: 'Here's the next DSL step to compute the matrix C:\n",
            "```\n",
            "M = np.linalg.norm(np.dot(A, B))\n",
            "C = A @ B\n",
            "```'\n",
            "- LLM output could not be parsed: 'Here's the next DSL step to compute the matrix C:\n",
            "```\n",
            "M = np.linalg.norm(np.dot(A, B))\n",
            "C = A @ B\n",
            "```'\n",
            "- LLM output could not be parsed: 'Here's the next DSL step to compute the matrix C:\n",
            "```\n",
            "M = np.linalg.norm(np.dot(A, B))\n",
            "C = A @ B\n",
            "```'\n",
            "- LLM output could not be parsed: 'Here's the next DSL step to compute the matrix C:\n",
            "```\n",
            "M = np.linalg.norm(np.dot(A, B))\n",
            "C = A @ B\n",
            "```'\n",
            "- LLM output could not be parsed: 'Here's the next DSL step to compute the matrix C:\n",
            "```\n",
            "M = np.linalg.norm(np.dot(A, B))\n",
            "C = A @ B\n",
            "```'\n",
            "- LLM output could not be parsed: 'Here's the next DSL step to compute the matrix C:\n",
            "```\n",
            "M = np.linalg.norm(np.dot(A, B))\n",
            "C = A @ B\n",
            "```'\n",
            "\n",
            "Final Computed C (by LLM algorithm):\n",
            "[[nan nan]\n",
            " [nan nan]]\n",
            "Result: INCOMPLETE (C was not fully computed)\n",
            "\n",
            "--- Test Case 2/10 ---\n",
            "Matrix A:\n",
            "[[-4 -3]\n",
            " [ 2  1]]\n",
            "Matrix B:\n",
            "[[ 2 -1]\n",
            " [ 3 -3]]\n",
            "Expected C:\n",
            "[[-17  13]\n",
            " [  7  -5]]\n",
            "  LLM Output (Step 1): 'Current algorithm state:\n",
            "Matrices:\n",
            "A = np.array([-4, -3], [2, 1]])\n",
            "B = np.array([[2, -1], [3, -3]])\n",
            "\n",
            "Execute Steps:\n",
            "- Assign intermediate variables M_0 =' -> Parse Error.\n",
            "  LLM Output (Step 2): 'Current algorithm state:\n",
            "Matrices:\n",
            "A = np.array([-4, -3], [2, 1]])\n",
            "B = np.array([[2, -1], [3, -3]])\n",
            "\n",
            "Execute Steps:\n",
            "- Assign intermediate variables M_1 =' -> Parse Error.\n",
            "  LLM Output (Step 3): 'Current algorithm state:\n",
            "Matrices:\n",
            "A = np.array([-4, -3], [2, 1]])\n",
            "B = np.array([[2, -1], [3, -3]])\n",
            "\n",
            "Execute Steps:\n",
            "- Assign intermediate variables M_2 =' -> Parse Error.\n",
            "  LLM Output (Step 4): 'Current algorithm state:\n",
            "Matrices:\n",
            "A = np.array([-4, -3], [2, 1]])\n",
            "\n",
            "Execute Steps:\n",
            "- Assign intermediate variables M_3 =' -> Parse Error.\n",
            "  LLM Output (Step 5): 'Current algorithm state:\n",
            "Matrices:\n",
            "A = np.array([-4, -3], [2, 1]])\n",
            "\n",
            "Execute Steps:\n",
            "- Assign intermediate variables M_4 =' -> Parse Error.\n",
            "  LLM Output (Step 6): 'Current algorithm state:\n",
            "Matrices:\n",
            "A = np.array([-4, -3], [2, 1]])\n",
            "\n",
            "Execute Steps:\n",
            "- Assign intermediate variables M_5 =' -> Parse Error.\n",
            "  LLM Output (Step 7): 'Current algorithm state:\n",
            "Matrices:\n",
            "A = np.array([-4, -3], [2, 1]])\n",
            "\n",
            "Execute Steps:\n",
            "- Assign intermediate variables M_6 =' -> Parse Error.\n",
            "  LLM Output (Step 8): 'Current algorithm state:\n",
            "Matrices:\n",
            "A = np.array([-4, -3], [2, 1]])\n",
            "\n",
            "Execute Steps:\n",
            "- Assign intermediate variables M_7 =' -> Parse Error.\n",
            "  LLM Output (Step 9): 'Current algorithm state:\n",
            "Matrices:\n",
            "A = np.array([-4, -3], [2, 1]])\n",
            "\n",
            "Execute Steps:\n",
            "- Assign intermediate variables M_8 =' -> Parse Error.\n",
            "  LLM Output (Step 10): 'Current algorithm state:\n",
            "Matrices:\n",
            "A = np.array([-4, -3], [2, 1]])\n",
            "\n",
            "Execute Steps:\n",
            "- Assign intermediate variables M_9 =' -> Parse Error.\n",
            "  LLM Output (Step 11): 'Current algorithm state:\n",
            "Matrices:\n",
            "A = np.array([-4, -3], [2, 1]])\n",
            "\n",
            "Execute Steps:\n",
            "- Assign intermediate variables M_10 =' -> Parse Error.\n",
            "  LLM Output (Step 12): 'Current algorithm state:\n",
            "Matrices:\n",
            "A = np.array([[-4, -3], [2, 1]])\n",
            "\n",
            "Execute Steps:\n",
            "- Assign intermediate variables M_11 =' -> Parse Error.\n",
            "  LLM Output (Step 13): 'Current algorithm state:\n",
            "Matrices:\n",
            "A = np.array([[-4, -3], [2, 1]])\n",
            "\n",
            "Execute Steps:\n",
            "- Assign intermediate variables M_12 =' -> Parse Error.\n",
            "  LLM Output (Step 14): 'Current algorithm state:\n",
            "Matrices:\n",
            "A = np.array([[-4, -3], [2, 1]])\n",
            "\n",
            "Execute Steps:\n",
            "- Assign intermediate variables M_13 =' -> Parse Error.\n",
            "  LLM Output (Step 15): 'Current algorithm state:\n",
            "Matrices:\n",
            "A = np.array([[-4, -3], [2, 1]])\n",
            "\n",
            "Execute Steps:\n",
            "- Assign intermediate variables M_14 =' -> Parse Error.\n",
            "  LLM Output (Step 16): 'Current algorithm state:\n",
            "Matrices:\n",
            "A = np.array([[-4, -3], [2, 1]])\n",
            "\n",
            "Execute Steps:\n",
            "- Assign intermediate variables M_15 =' -> Parse Error.\n",
            "  LLM Output (Step 17): 'Current algorithm state:\n",
            "Matrices:\n",
            "A = np.array([[-4, -3], [2, 1]])\n",
            "\n",
            "Execute Steps:\n",
            "- Assign intermediate variables M_16 =' -> Parse Error.\n",
            "  LLM Output (Step 18): 'Current algorithm state:\n",
            "Matrices:\n",
            "A = np.array([[-4, -3], [2, 1]])\n",
            "\n",
            "Execute Steps:\n",
            "- Assign intermediate variables M_17 =' -> Parse Error.\n",
            "  LLM Output (Step 19): 'Current algorithm state:\n",
            "Matrices:\n",
            "A = np.array([[-4, -3], [2, 1]])\n",
            "\n",
            "Execute Steps:\n",
            "- Assign intermediate variables M_18 =' -> Parse Error.\n",
            "  LLM Output (Step 20): 'Current algorithm state:\n",
            "Matrices:\n",
            "A = np.array([[-4, -3], [2, 1]])\n",
            "\n",
            "Execute Steps:\n",
            "- Assign intermediate variables M_19 =' -> Parse Error.\n",
            "\n",
            "Generated DSL Algorithm:\n",
            "No valid DSL steps were executed.\n",
            "\n",
            "Errors encountered during generation/execution:\n",
            "- LLM output could not be parsed: 'Current algorithm state:\n",
            "Matrices:\n",
            "A = np.array([-4, -3], [2, 1]])\n",
            "B = np.array([[2, -1], [3, -3]])\n",
            "\n",
            "Execute Steps:\n",
            "- Assign intermediate variables M_0 ='\n",
            "- LLM output could not be parsed: 'Current algorithm state:\n",
            "Matrices:\n",
            "A = np.array([-4, -3], [2, 1]])\n",
            "B = np.array([[2, -1], [3, -3]])\n",
            "\n",
            "Execute Steps:\n",
            "- Assign intermediate variables M_1 ='\n",
            "- LLM output could not be parsed: 'Current algorithm state:\n",
            "Matrices:\n",
            "A = np.array([-4, -3], [2, 1]])\n",
            "B = np.array([[2, -1], [3, -3]])\n",
            "\n",
            "Execute Steps:\n",
            "- Assign intermediate variables M_2 ='\n",
            "- LLM output could not be parsed: 'Current algorithm state:\n",
            "Matrices:\n",
            "A = np.array([-4, -3], [2, 1]])\n",
            "\n",
            "Execute Steps:\n",
            "- Assign intermediate variables M_3 ='\n",
            "- LLM output could not be parsed: 'Current algorithm state:\n",
            "Matrices:\n",
            "A = np.array([-4, -3], [2, 1]])\n",
            "\n",
            "Execute Steps:\n",
            "- Assign intermediate variables M_4 ='\n",
            "- LLM output could not be parsed: 'Current algorithm state:\n",
            "Matrices:\n",
            "A = np.array([-4, -3], [2, 1]])\n",
            "\n",
            "Execute Steps:\n",
            "- Assign intermediate variables M_5 ='\n",
            "- LLM output could not be parsed: 'Current algorithm state:\n",
            "Matrices:\n",
            "A = np.array([-4, -3], [2, 1]])\n",
            "\n",
            "Execute Steps:\n",
            "- Assign intermediate variables M_6 ='\n",
            "- LLM output could not be parsed: 'Current algorithm state:\n",
            "Matrices:\n",
            "A = np.array([-4, -3], [2, 1]])\n",
            "\n",
            "Execute Steps:\n",
            "- Assign intermediate variables M_7 ='\n",
            "- LLM output could not be parsed: 'Current algorithm state:\n",
            "Matrices:\n",
            "A = np.array([-4, -3], [2, 1]])\n",
            "\n",
            "Execute Steps:\n",
            "- Assign intermediate variables M_8 ='\n",
            "- LLM output could not be parsed: 'Current algorithm state:\n",
            "Matrices:\n",
            "A = np.array([-4, -3], [2, 1]])\n",
            "\n",
            "Execute Steps:\n",
            "- Assign intermediate variables M_9 ='\n",
            "- LLM output could not be parsed: 'Current algorithm state:\n",
            "Matrices:\n",
            "A = np.array([-4, -3], [2, 1]])\n",
            "\n",
            "Execute Steps:\n",
            "- Assign intermediate variables M_10 ='\n",
            "- LLM output could not be parsed: 'Current algorithm state:\n",
            "Matrices:\n",
            "A = np.array([[-4, -3], [2, 1]])\n",
            "\n",
            "Execute Steps:\n",
            "- Assign intermediate variables M_11 ='\n",
            "- LLM output could not be parsed: 'Current algorithm state:\n",
            "Matrices:\n",
            "A = np.array([[-4, -3], [2, 1]])\n",
            "\n",
            "Execute Steps:\n",
            "- Assign intermediate variables M_12 ='\n",
            "- LLM output could not be parsed: 'Current algorithm state:\n",
            "Matrices:\n",
            "A = np.array([[-4, -3], [2, 1]])\n",
            "\n",
            "Execute Steps:\n",
            "- Assign intermediate variables M_13 ='\n",
            "- LLM output could not be parsed: 'Current algorithm state:\n",
            "Matrices:\n",
            "A = np.array([[-4, -3], [2, 1]])\n",
            "\n",
            "Execute Steps:\n",
            "- Assign intermediate variables M_14 ='\n",
            "- LLM output could not be parsed: 'Current algorithm state:\n",
            "Matrices:\n",
            "A = np.array([[-4, -3], [2, 1]])\n",
            "\n",
            "Execute Steps:\n",
            "- Assign intermediate variables M_15 ='\n",
            "- LLM output could not be parsed: 'Current algorithm state:\n",
            "Matrices:\n",
            "A = np.array([[-4, -3], [2, 1]])\n",
            "\n",
            "Execute Steps:\n",
            "- Assign intermediate variables M_16 ='\n",
            "- LLM output could not be parsed: 'Current algorithm state:\n",
            "Matrices:\n",
            "A = np.array([[-4, -3], [2, 1]])\n",
            "\n",
            "Execute Steps:\n",
            "- Assign intermediate variables M_17 ='\n",
            "- LLM output could not be parsed: 'Current algorithm state:\n",
            "Matrices:\n",
            "A = np.array([[-4, -3], [2, 1]])\n",
            "\n",
            "Execute Steps:\n",
            "- Assign intermediate variables M_18 ='\n",
            "- LLM output could not be parsed: 'Current algorithm state:\n",
            "Matrices:\n",
            "A = np.array([[-4, -3], [2, 1]])\n",
            "\n",
            "Execute Steps:\n",
            "- Assign intermediate variables M_19 ='\n",
            "\n",
            "Final Computed C (by LLM algorithm):\n",
            "[[nan nan]\n",
            " [nan nan]]\n",
            "Result: INCOMPLETE (C was not fully computed)\n",
            "\n",
            "--- Test Case 3/10 ---\n",
            "Matrix A:\n",
            "[[ 2 -3]\n",
            " [ 1  1]]\n",
            "Matrix B:\n",
            "[[ 2 -1]\n",
            " [ 3  5]]\n",
            "Expected C:\n",
            "[[ -5 -17]\n",
            " [  5   4]]\n",
            "  LLM Output (Step 1): 'To solve this problem, we need to define an intermediate variable `M1` and assign it the value `M1`.\n",
            "We also need to assign the result of the previous step `C0` to `C1`.\n",
            "Next, we need to' -> Parse Error.\n",
            "  LLM Output (Step 2): 'Here's the next DSL step to compute the matrix C:\n",
            "```less\n",
            "M1 = A[0,1] + A[1,1]\n",
            "C1 = B[0,0] * M1\n",
            "```\n",
            "\n",
            "Now that we have' -> Parse Error.\n",
            "  LLM Output (Step 3): 'Here's the next DSL step to compute the matrix C:\n",
            "```less\n",
            "M1 = A[0,1] + A[1,1]\n",
            "C1 = B[0,0] * M1\n",
            "```\n",
            "\n",
            "And now we have' -> Parse Error.\n",
            "  LLM Output (Step 4): 'Here's the next DSL step to compute the matrix C:\n",
            "```less\n",
            "M1 = A[0,1] + A[1,1]\n",
            "C1 = B[0,0] * M1\n",
            "```\n",
            "\n",
            "And now we have' -> Parse Error.\n",
            "  LLM Output (Step 5): 'Here's the next DSL step to compute the matrix C:\n",
            "```less\n",
            "M1 = A[0,1] + A[1,1]\n",
            "C1 = B[0,0] * M1\n",
            "```\n",
            "\n",
            "And now we have' -> Parse Error.\n",
            "  LLM Output (Step 6): 'Here's the next DSL step to compute the matrix C:\n",
            "```less\n",
            "M1 = A[0,1] + A[1,1]\n",
            "C1 = B[0,0] * M1\n",
            "```\n",
            "\n",
            "And now we have' -> Parse Error.\n",
            "  LLM Output (Step 7): 'Here's the next DSL step to compute the matrix C:\n",
            "```less\n",
            "M1 = A[0,1] + A[1,1]\n",
            "C1 = B[0,0] * M1\n",
            "```\n",
            "\n",
            "And now we have' -> Parse Error.\n",
            "  LLM Output (Step 8): 'Here's the next DSL step to compute the matrix C:\n",
            "```less\n",
            "M1 = A[0,1] + A[1,1]\n",
            "C1 = B[0,0] * M1\n",
            "```\n",
            "\n",
            "And now we have' -> Parse Error.\n",
            "  LLM Output (Step 9): 'Here's the next DSL step to compute the matrix C:\n",
            "```less\n",
            "M1 = A[0,1] + A[1,1]\n",
            "C1 = B[0,0] * M1\n",
            "```\n",
            "\n",
            "And now we have' -> Parse Error.\n",
            "  LLM Output (Step 10): 'Here's the next DSL step to compute the matrix C:\n",
            "```less\n",
            "M1 = A[0,1] + A[1,1]\n",
            "C1 = B[0,0] * M1\n",
            "```\n",
            "\n",
            "And now we have' -> Parse Error.\n",
            "  LLM Output (Step 11): 'Here's the next DSL step to compute the matrix C:\n",
            "```less\n",
            "M1 = A[0,1] + A[1,1]\n",
            "C1 = B[0,0] * M1\n",
            "```\n",
            "\n",
            "And now' -> Parse Error.\n",
            "  LLM Output (Step 12): 'Here's the next DSL step to compute the matrix C:\n",
            "```less\n",
            "M1 = A[0,1] + A[1,1]\n",
            "C1 = B[0,0] * M1\n",
            "```\n",
            "\n",
            "And now we have' -> Parse Error.\n",
            "  LLM Output (Step 13): 'Here's the next DSL step to compute the matrix C:\n",
            "```less\n",
            "M1 = A[0,1] + A[1,1]\n",
            "C1 = B[0,0] * M1\n",
            "```\n",
            "\n",
            "And now' -> Parse Error.\n",
            "  LLM Output (Step 14): 'Here's the next DSL step to compute the matrix C:\n",
            "```less\n",
            "M1 = A[0,1] + A[1,1]\n",
            "C1 = B[0,0] * M1\n",
            "```\n",
            "\n",
            "And now' -> Parse Error.\n",
            "  LLM Output (Step 15): 'Here's the next DSL step to compute the matrix C:\n",
            "```less\n",
            "M1 = A[0,1] + A[1,1]\n",
            "C1 = B[0,0] * M1\n",
            "```\n",
            "\n",
            "And now' -> Parse Error.\n",
            "  LLM Output (Step 16): 'Here's the next DSL step to compute the matrix C:\n",
            "```less\n",
            "M1 = A[0,1] + A[1,1]\n",
            "C1 = B[0,0] * M1\n",
            "```\n",
            "\n",
            "And now' -> Parse Error.\n",
            "  LLM Output (Step 17): 'Here's the next DSL step to compute the matrix C:\n",
            "```less\n",
            "M1 = A[0,1] + A[1,1]\n",
            "C1 = B[0,0] * M1\n",
            "```\n",
            "\n",
            "And now' -> Parse Error.\n",
            "  LLM Output (Step 18): 'Here's the next DSL step to compute the matrix C:\n",
            "```less\n",
            "M1 = A[0,1] + A[1,1]\n",
            "C1 = B[0,0] * M1\n",
            "```\n",
            "\n",
            "And now' -> Parse Error.\n",
            "  LLM Output (Step 19): 'Here's the next DSL step to compute the matrix C:\n",
            "```less\n",
            "M1 = A[0,1] + A[1,1]\n",
            "C1 = B[0,0] * M1\n",
            "```\n",
            "\n",
            "And now' -> Parse Error.\n",
            "  LLM Output (Step 20): 'Here's the next DSL step to compute the matrix C:\n",
            "```less\n",
            "M1 = A[0,1] + A[1,1]\n",
            "C1 = B[0,0] * M1\n",
            "```\n",
            "\n",
            "And now' -> Parse Error.\n",
            "\n",
            "Generated DSL Algorithm:\n",
            "No valid DSL steps were executed.\n",
            "\n",
            "Errors encountered during generation/execution:\n",
            "- LLM output could not be parsed: 'To solve this problem, we need to define an intermediate variable `M1` and assign it the value `M1`.\n",
            "We also need to assign the result of the previous step `C0` to `C1`.\n",
            "Next, we need to'\n",
            "- LLM output could not be parsed: 'Here's the next DSL step to compute the matrix C:\n",
            "```less\n",
            "M1 = A[0,1] + A[1,1]\n",
            "C1 = B[0,0] * M1\n",
            "```\n",
            "\n",
            "Now that we have'\n",
            "- LLM output could not be parsed: 'Here's the next DSL step to compute the matrix C:\n",
            "```less\n",
            "M1 = A[0,1] + A[1,1]\n",
            "C1 = B[0,0] * M1\n",
            "```\n",
            "\n",
            "And now we have'\n",
            "- LLM output could not be parsed: 'Here's the next DSL step to compute the matrix C:\n",
            "```less\n",
            "M1 = A[0,1] + A[1,1]\n",
            "C1 = B[0,0] * M1\n",
            "```\n",
            "\n",
            "And now we have'\n",
            "- LLM output could not be parsed: 'Here's the next DSL step to compute the matrix C:\n",
            "```less\n",
            "M1 = A[0,1] + A[1,1]\n",
            "C1 = B[0,0] * M1\n",
            "```\n",
            "\n",
            "And now we have'\n",
            "- LLM output could not be parsed: 'Here's the next DSL step to compute the matrix C:\n",
            "```less\n",
            "M1 = A[0,1] + A[1,1]\n",
            "C1 = B[0,0] * M1\n",
            "```\n",
            "\n",
            "And now we have'\n",
            "- LLM output could not be parsed: 'Here's the next DSL step to compute the matrix C:\n",
            "```less\n",
            "M1 = A[0,1] + A[1,1]\n",
            "C1 = B[0,0] * M1\n",
            "```\n",
            "\n",
            "And now we have'\n",
            "- LLM output could not be parsed: 'Here's the next DSL step to compute the matrix C:\n",
            "```less\n",
            "M1 = A[0,1] + A[1,1]\n",
            "C1 = B[0,0] * M1\n",
            "```\n",
            "\n",
            "And now we have'\n",
            "- LLM output could not be parsed: 'Here's the next DSL step to compute the matrix C:\n",
            "```less\n",
            "M1 = A[0,1] + A[1,1]\n",
            "C1 = B[0,0] * M1\n",
            "```\n",
            "\n",
            "And now we have'\n",
            "- LLM output could not be parsed: 'Here's the next DSL step to compute the matrix C:\n",
            "```less\n",
            "M1 = A[0,1] + A[1,1]\n",
            "C1 = B[0,0] * M1\n",
            "```\n",
            "\n",
            "And now we have'\n",
            "- LLM output could not be parsed: 'Here's the next DSL step to compute the matrix C:\n",
            "```less\n",
            "M1 = A[0,1] + A[1,1]\n",
            "C1 = B[0,0] * M1\n",
            "```\n",
            "\n",
            "And now'\n",
            "- LLM output could not be parsed: 'Here's the next DSL step to compute the matrix C:\n",
            "```less\n",
            "M1 = A[0,1] + A[1,1]\n",
            "C1 = B[0,0] * M1\n",
            "```\n",
            "\n",
            "And now we have'\n",
            "- LLM output could not be parsed: 'Here's the next DSL step to compute the matrix C:\n",
            "```less\n",
            "M1 = A[0,1] + A[1,1]\n",
            "C1 = B[0,0] * M1\n",
            "```\n",
            "\n",
            "And now'\n",
            "- LLM output could not be parsed: 'Here's the next DSL step to compute the matrix C:\n",
            "```less\n",
            "M1 = A[0,1] + A[1,1]\n",
            "C1 = B[0,0] * M1\n",
            "```\n",
            "\n",
            "And now'\n",
            "- LLM output could not be parsed: 'Here's the next DSL step to compute the matrix C:\n",
            "```less\n",
            "M1 = A[0,1] + A[1,1]\n",
            "C1 = B[0,0] * M1\n",
            "```\n",
            "\n",
            "And now'\n",
            "- LLM output could not be parsed: 'Here's the next DSL step to compute the matrix C:\n",
            "```less\n",
            "M1 = A[0,1] + A[1,1]\n",
            "C1 = B[0,0] * M1\n",
            "```\n",
            "\n",
            "And now'\n",
            "- LLM output could not be parsed: 'Here's the next DSL step to compute the matrix C:\n",
            "```less\n",
            "M1 = A[0,1] + A[1,1]\n",
            "C1 = B[0,0] * M1\n",
            "```\n",
            "\n",
            "And now'\n",
            "- LLM output could not be parsed: 'Here's the next DSL step to compute the matrix C:\n",
            "```less\n",
            "M1 = A[0,1] + A[1,1]\n",
            "C1 = B[0,0] * M1\n",
            "```\n",
            "\n",
            "And now'\n",
            "- LLM output could not be parsed: 'Here's the next DSL step to compute the matrix C:\n",
            "```less\n",
            "M1 = A[0,1] + A[1,1]\n",
            "C1 = B[0,0] * M1\n",
            "```\n",
            "\n",
            "And now'\n",
            "- LLM output could not be parsed: 'Here's the next DSL step to compute the matrix C:\n",
            "```less\n",
            "M1 = A[0,1] + A[1,1]\n",
            "C1 = B[0,0] * M1\n",
            "```\n",
            "\n",
            "And now'\n",
            "\n",
            "Final Computed C (by LLM algorithm):\n",
            "[[nan nan]\n",
            " [nan nan]]\n",
            "Result: INCOMPLETE (C was not fully computed)\n",
            "\n",
            "--- Test Case 4/10 ---\n",
            "Matrix A:\n",
            "[[-3 -3]\n",
            " [-1  1]]\n",
            "Matrix B:\n",
            "[[-2  1]\n",
            " [-5 -2]]\n",
            "Expected C:\n",
            "[[21  3]\n",
            " [-3 -3]]\n",
            "  LLM Output (Step 1): 'Next single DSL step: M1 = A[0,1] + A[1,1].' -> Parse Error.\n",
            "  LLM Output (Step 2): 'Next single DSL step: M1 = A[0,1] + A[1,1].' -> Parse Error.\n",
            "  LLM Output (Step 3): 'Next single DSL step: M1 = A[0,1] + A[1,1].' -> Parse Error.\n",
            "  LLM Output (Step 4): 'Next single DSL step: M1 = A[0,1] + A[1,1].' -> Parse Error.\n",
            "  LLM Output (Step 5): 'Next single DSL step: M1 = A[0,1] + A[1,1].' -> Parse Error.\n",
            "  LLM Output (Step 6): 'Next single DSL step: M1 = A[0,1] + A[1,1].' -> Parse Error.\n",
            "  LLM Output (Step 7): 'Next single DSL step: M1 = A[0,1] + A[1,1].' -> Parse Error.\n",
            "  LLM Output (Step 8): 'Next single DSL step: M1 = A[0,1] + A[1,1].' -> Parse Error.\n",
            "  LLM Output (Step 9): 'Next single DSL step: M1 = A[0,1] + A[1,1].' -> Parse Error.\n",
            "  LLM Output (Step 10): 'Next single DSL step: M1 = A[0,1] + A[1,1].' -> Parse Error.\n",
            "  LLM Output (Step 11): 'Next single DSL step: M1 = A[0,1] + A[1,1].' -> Parse Error.\n",
            "  LLM Output (Step 12): 'Next single DSL step: M1 = A[0,1] + A[1,1].' -> Parse Error.\n",
            "  LLM Output (Step 13): 'Next single DSL step: M1 = A[0,1] + A[1,1].' -> Parse Error.\n",
            "  LLM Output (Step 14): 'Next single DSL step: M1 = A[0,1] + A[1,1].' -> Parse Error.\n",
            "  LLM Output (Step 15): 'Next single DSL step: M1 = A[0,1] + A[1,1].' -> Parse Error.\n",
            "  LLM Output (Step 16): 'Next single DSL step: M1 = A[0,1] + A[1,1].' -> Parse Error.\n",
            "  LLM Output (Step 17): 'Next single DSL step:' -> Parse Error.\n",
            "  LLM Output (Step 18): 'Next single DSL step:' -> Parse Error.\n",
            "  LLM Output (Step 19): 'Next single DSL step:' -> Parse Error.\n",
            "  LLM Output (Step 20): 'Next single DSL step:' -> Parse Error.\n",
            "\n",
            "Generated DSL Algorithm:\n",
            "No valid DSL steps were executed.\n",
            "\n",
            "Errors encountered during generation/execution:\n",
            "- LLM output could not be parsed: 'Next single DSL step: M1 = A[0,1] + A[1,1].'\n",
            "- LLM output could not be parsed: 'Next single DSL step: M1 = A[0,1] + A[1,1].'\n",
            "- LLM output could not be parsed: 'Next single DSL step: M1 = A[0,1] + A[1,1].'\n",
            "- LLM output could not be parsed: 'Next single DSL step: M1 = A[0,1] + A[1,1].'\n",
            "- LLM output could not be parsed: 'Next single DSL step: M1 = A[0,1] + A[1,1].'\n",
            "- LLM output could not be parsed: 'Next single DSL step: M1 = A[0,1] + A[1,1].'\n",
            "- LLM output could not be parsed: 'Next single DSL step: M1 = A[0,1] + A[1,1].'\n",
            "- LLM output could not be parsed: 'Next single DSL step: M1 = A[0,1] + A[1,1].'\n",
            "- LLM output could not be parsed: 'Next single DSL step: M1 = A[0,1] + A[1,1].'\n",
            "- LLM output could not be parsed: 'Next single DSL step: M1 = A[0,1] + A[1,1].'\n",
            "- LLM output could not be parsed: 'Next single DSL step: M1 = A[0,1] + A[1,1].'\n",
            "- LLM output could not be parsed: 'Next single DSL step: M1 = A[0,1] + A[1,1].'\n",
            "- LLM output could not be parsed: 'Next single DSL step: M1 = A[0,1] + A[1,1].'\n",
            "- LLM output could not be parsed: 'Next single DSL step: M1 = A[0,1] + A[1,1].'\n",
            "- LLM output could not be parsed: 'Next single DSL step: M1 = A[0,1] + A[1,1].'\n",
            "- LLM output could not be parsed: 'Next single DSL step: M1 = A[0,1] + A[1,1].'\n",
            "- LLM output could not be parsed: 'Next single DSL step:'\n",
            "- LLM output could not be parsed: 'Next single DSL step:'\n",
            "- LLM output could not be parsed: 'Next single DSL step:'\n",
            "- LLM output could not be parsed: 'Next single DSL step:'\n",
            "\n",
            "Final Computed C (by LLM algorithm):\n",
            "[[nan nan]\n",
            " [nan nan]]\n",
            "Result: INCOMPLETE (C was not fully computed)\n",
            "\n",
            "--- Test Case 5/10 ---\n",
            "Matrix A:\n",
            "[[ 4 -3]\n",
            " [ 5 -5]]\n",
            "Matrix B:\n",
            "[[ 1 -4]\n",
            " [-5  1]]\n",
            "Expected C:\n",
            "[[ 19 -19]\n",
            " [ 30 -25]]\n",
            "  LLM Output (Step 1): 'To compute the matrix C, we need to assign an intermediate variable M1 to represent the column vector corresponding to the second row in the resulting matrix.\n",
            "Next, we will use the following syntax to assign the value M1 to the matrix C:\n",
            "C' -> Parse Error.\n",
            "  LLM Output (Step 2): 'To compute the matrix C, we need to assign an intermediate variable M2 to represent the column vector corresponding to the third row in the resulting matrix.\n",
            "Next, we will use the following syntax to assign the value M2 to the matrix C:\n",
            "C' -> Parse Error.\n",
            "  LLM Output (Step 3): 'To compute the matrix C, we need to assign an intermediate variable M3 to represent the column vector corresponding to the fourth row in the resulting matrix.\n",
            "Next, we will use the following syntax to assign the value M3 to the matrix C:\n",
            "C' -> Parse Error.\n",
            "  LLM Output (Step 4): 'To compute the matrix C, we need to assign an intermediate variable M4 to represent the column vector corresponding to the fifth row in the resulting matrix.\n",
            "Next, we will use the following syntax to assign the value M4 to the matrix C:\n",
            "C' -> Parse Error.\n",
            "  LLM Output (Step 5): 'To compute the matrix C, we need to assign an intermediate variable M5 to represent the column vector corresponding to the sixth row in the resulting matrix.\n",
            "Next, we will use the following syntax to assign the value M5 to the matrix C:\n",
            "C' -> Parse Error.\n",
            "  LLM Output (Step 6): 'To compute the matrix C, we need to assign an intermediate variable M6 to represent the column vector corresponding to the seventh row in the resulting matrix.\n",
            "Next, we will use the following syntax to assign the value M6 to the matrix C:\n",
            "C' -> Parse Error.\n",
            "  LLM Output (Step 7): 'To compute the matrix C, we need to assign an intermediate variable M7 to represent the column vector corresponding to the eighth row in the resulting matrix.\n",
            "Next, we will use the following syntax to assign the value M7 to the matrix C:\n",
            "C' -> Parse Error.\n",
            "  LLM Output (Step 8): 'To compute the matrix C, we need to assign an intermediate variable M8 to represent the column vector corresponding to the ninth row in the resulting matrix.\n",
            "Next, we will use the following syntax to assign the value M8 to the matrix C:\n",
            "C' -> Parse Error.\n",
            "  LLM Output (Step 9): 'To compute the matrix C, we need to assign an intermediate variable M9 to represent the column vector corresponding to the tenth row in the resulting matrix.\n",
            "Next, we will use the following syntax to assign the value M9 to the matrix C:\n",
            "C' -> Parse Error.\n",
            "  LLM Output (Step 10): 'To compute the matrix C, we need to assign an intermediate variable M10 to represent the column vector corresponding to the eleventh row in the resulting matrix.\n",
            "Next, we will use the following syntax to assign the value M10 to the matrix' -> Parse Error.\n",
            "  LLM Output (Step 11): 'To compute the matrix C, we need to assign an intermediate variable' -> Parse Error.\n",
            "  LLM Output (Step 12): 'To compute' -> Parse Error.\n",
            "  LLM Output (Step 13): 'To compute' -> Parse Error.\n",
            "  LLM Output (Step 14): 'To compute' -> Parse Error.\n",
            "  LLM Output (Step 15): 'To' -> Parse Error.\n",
            "  LLM Output (Step 16): 'To' -> Parse Error.\n",
            "  LLM Output (Step 17): 'To' -> Parse Error.\n",
            "  LLM Output (Step 18): 'To' -> Parse Error.\n",
            "  LLM Output (Step 19): 'To' -> Parse Error.\n",
            "  LLM Output (Step 20): '' -> Parse Error.\n",
            "\n",
            "Generated DSL Algorithm:\n",
            "No valid DSL steps were executed.\n",
            "\n",
            "Errors encountered during generation/execution:\n",
            "- LLM output could not be parsed: 'To compute the matrix C, we need to assign an intermediate variable M1 to represent the column vector corresponding to the second row in the resulting matrix.\n",
            "Next, we will use the following syntax to assign the value M1 to the matrix C:\n",
            "C'\n",
            "- LLM output could not be parsed: 'To compute the matrix C, we need to assign an intermediate variable M2 to represent the column vector corresponding to the third row in the resulting matrix.\n",
            "Next, we will use the following syntax to assign the value M2 to the matrix C:\n",
            "C'\n",
            "- LLM output could not be parsed: 'To compute the matrix C, we need to assign an intermediate variable M3 to represent the column vector corresponding to the fourth row in the resulting matrix.\n",
            "Next, we will use the following syntax to assign the value M3 to the matrix C:\n",
            "C'\n",
            "- LLM output could not be parsed: 'To compute the matrix C, we need to assign an intermediate variable M4 to represent the column vector corresponding to the fifth row in the resulting matrix.\n",
            "Next, we will use the following syntax to assign the value M4 to the matrix C:\n",
            "C'\n",
            "- LLM output could not be parsed: 'To compute the matrix C, we need to assign an intermediate variable M5 to represent the column vector corresponding to the sixth row in the resulting matrix.\n",
            "Next, we will use the following syntax to assign the value M5 to the matrix C:\n",
            "C'\n",
            "- LLM output could not be parsed: 'To compute the matrix C, we need to assign an intermediate variable M6 to represent the column vector corresponding to the seventh row in the resulting matrix.\n",
            "Next, we will use the following syntax to assign the value M6 to the matrix C:\n",
            "C'\n",
            "- LLM output could not be parsed: 'To compute the matrix C, we need to assign an intermediate variable M7 to represent the column vector corresponding to the eighth row in the resulting matrix.\n",
            "Next, we will use the following syntax to assign the value M7 to the matrix C:\n",
            "C'\n",
            "- LLM output could not be parsed: 'To compute the matrix C, we need to assign an intermediate variable M8 to represent the column vector corresponding to the ninth row in the resulting matrix.\n",
            "Next, we will use the following syntax to assign the value M8 to the matrix C:\n",
            "C'\n",
            "- LLM output could not be parsed: 'To compute the matrix C, we need to assign an intermediate variable M9 to represent the column vector corresponding to the tenth row in the resulting matrix.\n",
            "Next, we will use the following syntax to assign the value M9 to the matrix C:\n",
            "C'\n",
            "- LLM output could not be parsed: 'To compute the matrix C, we need to assign an intermediate variable M10 to represent the column vector corresponding to the eleventh row in the resulting matrix.\n",
            "Next, we will use the following syntax to assign the value M10 to the matrix'\n",
            "- LLM output could not be parsed: 'To compute the matrix C, we need to assign an intermediate variable'\n",
            "- LLM output could not be parsed: 'To compute'\n",
            "- LLM output could not be parsed: 'To compute'\n",
            "- LLM output could not be parsed: 'To compute'\n",
            "- LLM output could not be parsed: 'To'\n",
            "- LLM output could not be parsed: 'To'\n",
            "- LLM output could not be parsed: 'To'\n",
            "- LLM output could not be parsed: 'To'\n",
            "- LLM output could not be parsed: 'To'\n",
            "- LLM output could not be parsed: ''\n",
            "\n",
            "Final Computed C (by LLM algorithm):\n",
            "[[nan nan]\n",
            " [nan nan]]\n",
            "Result: INCOMPLETE (C was not fully computed)\n",
            "\n",
            "--- Test Case 6/10 ---\n",
            "Matrix A:\n",
            "[[-4 -3]\n",
            " [-4 -4]]\n",
            "Matrix B:\n",
            "[[ 0  1]\n",
            " [-2 -4]]\n",
            "Expected C:\n",
            "[[ 6  8]\n",
            " [ 8 12]]\n",
            "  LLM Output (Step 1): 'Next Single DSL Step: M1 = A[0,1] + A[1,1]' -> Parse Error.\n",
            "  LLM Output (Step 2): 'Next Single DSL Step: M1 = A[0,1] + A[1,1]' -> Parse Error.\n",
            "  LLM Output (Step 3): 'Next Single DSL Step: M1 = A[0,1] + A[1,1]' -> Parse Error.\n",
            "  LLM Output (Step 4): 'Next Single DSL Step: M1 = A[0,1] + A[1,1]' -> Parse Error.\n",
            "  LLM Output (Step 5): 'Next Single DSL Step: M1 = A[0,1] + A[1,1]' -> Parse Error.\n",
            "  LLM Output (Step 6): 'Next Single DSL Step: M1 = A[0,1] + A[1,1]' -> Parse Error.\n",
            "  LLM Output (Step 7): 'Next Single DSL Step: M1 = A[0,1] + A[1,1]' -> Parse Error.\n",
            "  LLM Output (Step 8): 'Next Single DSL Step:' -> Parse Error.\n",
            "  LLM Output (Step 9): 'Next Single DSL Step:' -> Parse Error.\n",
            "  LLM Output (Step 10): 'Next Single DSL Step:' -> Parse Error.\n",
            "  LLM Output (Step 11): 'Next Single DSL Step:' -> Parse Error.\n",
            "  LLM Output (Step 12): 'Next Single DSL Step:' -> Parse Error.\n",
            "  LLM Output (Step 13): 'Next Single DSL Step:' -> Parse Error.\n",
            "  LLM Output (Step 14): 'Next Single DSL Step:' -> Parse Error.\n",
            "  LLM Output (Step 15): 'Next Single DSL Step:' -> Parse Error.\n",
            "  LLM Output (Step 16): 'Next Single DSL Step:' -> Parse Error.\n",
            "  LLM Output (Step 17): 'Next Single DSL Step:' -> Parse Error.\n",
            "  LLM Output (Step 18): 'Next Single DSL Step:' -> Parse Error.\n",
            "  LLM Output (Step 19): 'Next Single DSL Step:' -> Parse Error.\n",
            "  LLM Output (Step 20): 'Next Single DSL Step:' -> Parse Error.\n",
            "\n",
            "Generated DSL Algorithm:\n",
            "No valid DSL steps were executed.\n",
            "\n",
            "Errors encountered during generation/execution:\n",
            "- LLM output could not be parsed: 'Next Single DSL Step: M1 = A[0,1] + A[1,1]'\n",
            "- LLM output could not be parsed: 'Next Single DSL Step: M1 = A[0,1] + A[1,1]'\n",
            "- LLM output could not be parsed: 'Next Single DSL Step: M1 = A[0,1] + A[1,1]'\n",
            "- LLM output could not be parsed: 'Next Single DSL Step: M1 = A[0,1] + A[1,1]'\n",
            "- LLM output could not be parsed: 'Next Single DSL Step: M1 = A[0,1] + A[1,1]'\n",
            "- LLM output could not be parsed: 'Next Single DSL Step: M1 = A[0,1] + A[1,1]'\n",
            "- LLM output could not be parsed: 'Next Single DSL Step: M1 = A[0,1] + A[1,1]'\n",
            "- LLM output could not be parsed: 'Next Single DSL Step:'\n",
            "- LLM output could not be parsed: 'Next Single DSL Step:'\n",
            "- LLM output could not be parsed: 'Next Single DSL Step:'\n",
            "- LLM output could not be parsed: 'Next Single DSL Step:'\n",
            "- LLM output could not be parsed: 'Next Single DSL Step:'\n",
            "- LLM output could not be parsed: 'Next Single DSL Step:'\n",
            "- LLM output could not be parsed: 'Next Single DSL Step:'\n",
            "- LLM output could not be parsed: 'Next Single DSL Step:'\n",
            "- LLM output could not be parsed: 'Next Single DSL Step:'\n",
            "- LLM output could not be parsed: 'Next Single DSL Step:'\n",
            "- LLM output could not be parsed: 'Next Single DSL Step:'\n",
            "- LLM output could not be parsed: 'Next Single DSL Step:'\n",
            "- LLM output could not be parsed: 'Next Single DSL Step:'\n",
            "\n",
            "Final Computed C (by LLM algorithm):\n",
            "[[nan nan]\n",
            " [nan nan]]\n",
            "Result: INCOMPLETE (C was not fully computed)\n",
            "\n",
            "--- Test Case 7/10 ---\n",
            "Matrix A:\n",
            "[[-3  3]\n",
            " [-5 -3]]\n",
            "Matrix B:\n",
            "[[5 3]\n",
            " [5 5]]\n",
            "Expected C:\n",
            "[[  0   6]\n",
            " [-40 -30]]\n",
            "  LLM Output (Step 1): 'Next DSL Step: C[1,0] = A[1,0]*B[1,0].' -> Parse Error.\n",
            "  LLM Output (Step 2): 'Next DSL Step: C[1,1] = A[1,1]*B[1,1].' -> Parse Error.\n",
            "  LLM Output (Step 3): 'Next DSL Step: C[0,0] = M0 (if M0 was defined as a product for C[0,0])' -> Parse Error.\n",
            "  LLM Output (Step 4): 'Next DSL Step: C[0,1] = M1 (if M1 was defined as a product for C[0,1])' -> Parse Error.\n",
            "  LLM Output (Step 5): 'Next DSL Step: C[1,0] = M1 (if M1 was defined as a product for C[1,0])' -> Parse Error.\n",
            "  LLM Output (Step 6): 'Next DSL Step: C[1,1] = M1 (if M1 was defined as a product for C[1,1])' -> Parse Error.\n",
            "  LLM Output (Step 7): 'Next DSL Step: C[0,0] = M0 (if M0 was defined as a product for C[0,0])' -> Parse Error.\n",
            "  LLM Output (Step 8): 'Next DSL Step: C[0,1] = M1 (if M1 was defined as a product for C[0,1])' -> Parse Error.\n",
            "  LLM Output (Step 9): 'Next DSL Step: C[1,0] = M1 (if M1 was defined as a product for C[1,0])' -> Parse Error.\n",
            "  LLM Output (Step 10): 'Next DSL Step: C[1,1] = M1 (if M1 was defined as a product for C[1,1])' -> Parse Error.\n",
            "  LLM Output (Step 11): 'Next DSL Step: C[0,0] = M0' -> Parse Error.\n",
            "  LLM Output (Step 12): 'Next DSL Step:' -> Parse Error.\n",
            "  LLM Output (Step 13): 'Next DSL Step:' -> Parse Error.\n",
            "  LLM Output (Step 14): 'Next DSL Step:' -> Parse Error.\n",
            "  LLM Output (Step 15): 'Next DSL Step:' -> Parse Error.\n",
            "  LLM Output (Step 16): 'Next DSL Step:' -> Parse Error.\n",
            "  LLM Output (Step 17): 'Next DSL Step:' -> Parse Error.\n",
            "  LLM Output (Step 18): 'Next DSL Step:' -> Parse Error.\n",
            "  LLM Output (Step 19): 'Next DSL Step:' -> Parse Error.\n",
            "  LLM Output (Step 20): 'Next DSL Step:' -> Parse Error.\n",
            "\n",
            "Generated DSL Algorithm:\n",
            "No valid DSL steps were executed.\n",
            "\n",
            "Errors encountered during generation/execution:\n",
            "- LLM output could not be parsed: 'Next DSL Step: C[1,0] = A[1,0]*B[1,0].'\n",
            "- LLM output could not be parsed: 'Next DSL Step: C[1,1] = A[1,1]*B[1,1].'\n",
            "- LLM output could not be parsed: 'Next DSL Step: C[0,0] = M0 (if M0 was defined as a product for C[0,0])'\n",
            "- LLM output could not be parsed: 'Next DSL Step: C[0,1] = M1 (if M1 was defined as a product for C[0,1])'\n",
            "- LLM output could not be parsed: 'Next DSL Step: C[1,0] = M1 (if M1 was defined as a product for C[1,0])'\n",
            "- LLM output could not be parsed: 'Next DSL Step: C[1,1] = M1 (if M1 was defined as a product for C[1,1])'\n",
            "- LLM output could not be parsed: 'Next DSL Step: C[0,0] = M0 (if M0 was defined as a product for C[0,0])'\n",
            "- LLM output could not be parsed: 'Next DSL Step: C[0,1] = M1 (if M1 was defined as a product for C[0,1])'\n",
            "- LLM output could not be parsed: 'Next DSL Step: C[1,0] = M1 (if M1 was defined as a product for C[1,0])'\n",
            "- LLM output could not be parsed: 'Next DSL Step: C[1,1] = M1 (if M1 was defined as a product for C[1,1])'\n",
            "- LLM output could not be parsed: 'Next DSL Step: C[0,0] = M0'\n",
            "- LLM output could not be parsed: 'Next DSL Step:'\n",
            "- LLM output could not be parsed: 'Next DSL Step:'\n",
            "- LLM output could not be parsed: 'Next DSL Step:'\n",
            "- LLM output could not be parsed: 'Next DSL Step:'\n",
            "- LLM output could not be parsed: 'Next DSL Step:'\n",
            "- LLM output could not be parsed: 'Next DSL Step:'\n",
            "- LLM output could not be parsed: 'Next DSL Step:'\n",
            "- LLM output could not be parsed: 'Next DSL Step:'\n",
            "- LLM output could not be parsed: 'Next DSL Step:'\n",
            "\n",
            "Final Computed C (by LLM algorithm):\n",
            "[[nan nan]\n",
            " [nan nan]]\n",
            "Result: INCOMPLETE (C was not fully computed)\n",
            "\n",
            "--- Test Case 8/10 ---\n",
            "Matrix A:\n",
            "[[ 5  1]\n",
            " [-5  4]]\n",
            "Matrix B:\n",
            "[[-3  5]\n",
            " [-2  0]]\n",
            "Expected C:\n",
            "[[-17  25]\n",
            " [  7 -25]]\n",
            "  LLM Output (Step 1): 'To compute the matrix C, we need to define an intermediate variable M<id> and assign it to an output cell C(r,c). Let's start by defining the intermediate variables M0 and M1.\n",
            "\n",
            "The intermediate variable M0 will contain the' -> Parse Error.\n",
            "  LLM Output (Step 2): 'Now that we have defined the intermediate variables M0 and M1, we can assign them to output cells C(r,c) using their corresponding values:\n",
            "\n",
            "M0 = A[0,0]\n",
            "M1 = A[0,1]\n",
            "\n",
            "Next,' -> Parse Error.\n",
            "  LLM Output (Step 3): 'Since there is no current execution of any steps in this context, we don't need to define any intermediate variables for this step. Therefore, we simply provide the current DSL step format for C(0,0), which outputs a NaN value:\n",
            "\n",
            "M' -> Parse Error.\n",
            "  LLM Output (Step 4): 'No further steps are needed to compute the matrix C.\n",
            "\n",
            "If you have a specific formula or question related to matrices or their operations, feel free to ask and I'll do my best to assist you.' -> Parse Error.\n",
            "  LLM Output (Step 5): 'Since there is no current execution of any steps in this context, we don't need to define any intermediate variables for this step. Therefore, we simply provide the current DSL step format for C(0,0), which outputs a NaN value:\n",
            "\n",
            "M' -> Parse Error.\n",
            "  LLM Output (Step 6): 'There is no further steps required to compute the matrix C.\n",
            "\n",
            "If you have a specific formula or question related to matrices or their operations, feel free to ask and I'll do my best to assist you.' -> Parse Error.\n",
            "  LLM Output (Step 7): 'Since there is no current execution of any steps in this context, we don't need to define any intermediate variables for this step. Therefore, we simply provide the current DSL step format for C(0,0), which outputs a NaN value:\n",
            "\n",
            "M' -> Parse Error.\n",
            "  LLM Output (Step 8): 'There is no further steps required to compute the matrix C.\n",
            "\n",
            "If you have a specific formula or question related to matrices or their operations, feel free to ask and I'll do my best to assist you.' -> Parse Error.\n",
            "  LLM Output (Step 9): 'Since there is no current execution of any steps in this context, we don't need to define any intermediate variables for this step. Therefore,' -> Parse Error.\n",
            "  LLM Output (Step 10): 'Since there is no current execution of any steps in this context, we don't need to define any intermediate variables for this step. Therefore,' -> Parse Error.\n",
            "  LLM Output (Step 11): 'Since there is no current execution of any steps in this context, we don't need to define any' -> Parse Error.\n",
            "  LLM Output (Step 12): 'Since there is no current execution of any steps in this context, we don't need to define any intermediate variables for this step.' -> Parse Error.\n",
            "  LLM Output (Step 13): 'Since there is no current execution of any steps in this context, we don't need to define any' -> Parse Error.\n",
            "  LLM Output (Step 14): 'Since there is no current execution of any steps in this context,' -> Parse Error.\n",
            "  LLM Output (Step 15): 'Since there is no current execution of any steps in this context,' -> Parse Error.\n",
            "  LLM Output (Step 16): 'Since there is no current execution of any steps in this context,' -> Parse Error.\n",
            "  LLM Output (Step 17): 'Since' -> Parse Error.\n",
            "  LLM Output (Step 18): 'Since' -> Parse Error.\n",
            "  LLM Output (Step 19): 'Since there is' -> Parse Error.\n",
            "  LLM Output (Step 20): 'Since' -> Parse Error.\n",
            "\n",
            "Generated DSL Algorithm:\n",
            "No valid DSL steps were executed.\n",
            "\n",
            "Errors encountered during generation/execution:\n",
            "- LLM output could not be parsed: 'To compute the matrix C, we need to define an intermediate variable M<id> and assign it to an output cell C(r,c). Let's start by defining the intermediate variables M0 and M1.\n",
            "\n",
            "The intermediate variable M0 will contain the'\n",
            "- LLM output could not be parsed: 'Now that we have defined the intermediate variables M0 and M1, we can assign them to output cells C(r,c) using their corresponding values:\n",
            "\n",
            "M0 = A[0,0]\n",
            "M1 = A[0,1]\n",
            "\n",
            "Next,'\n",
            "- LLM output could not be parsed: 'Since there is no current execution of any steps in this context, we don't need to define any intermediate variables for this step. Therefore, we simply provide the current DSL step format for C(0,0), which outputs a NaN value:\n",
            "\n",
            "M'\n",
            "- LLM output could not be parsed: 'No further steps are needed to compute the matrix C.\n",
            "\n",
            "If you have a specific formula or question related to matrices or their operations, feel free to ask and I'll do my best to assist you.'\n",
            "- LLM output could not be parsed: 'Since there is no current execution of any steps in this context, we don't need to define any intermediate variables for this step. Therefore, we simply provide the current DSL step format for C(0,0), which outputs a NaN value:\n",
            "\n",
            "M'\n",
            "- LLM output could not be parsed: 'There is no further steps required to compute the matrix C.\n",
            "\n",
            "If you have a specific formula or question related to matrices or their operations, feel free to ask and I'll do my best to assist you.'\n",
            "- LLM output could not be parsed: 'Since there is no current execution of any steps in this context, we don't need to define any intermediate variables for this step. Therefore, we simply provide the current DSL step format for C(0,0), which outputs a NaN value:\n",
            "\n",
            "M'\n",
            "- LLM output could not be parsed: 'There is no further steps required to compute the matrix C.\n",
            "\n",
            "If you have a specific formula or question related to matrices or their operations, feel free to ask and I'll do my best to assist you.'\n",
            "- LLM output could not be parsed: 'Since there is no current execution of any steps in this context, we don't need to define any intermediate variables for this step. Therefore,'\n",
            "- LLM output could not be parsed: 'Since there is no current execution of any steps in this context, we don't need to define any intermediate variables for this step. Therefore,'\n",
            "- LLM output could not be parsed: 'Since there is no current execution of any steps in this context, we don't need to define any'\n",
            "- LLM output could not be parsed: 'Since there is no current execution of any steps in this context, we don't need to define any intermediate variables for this step.'\n",
            "- LLM output could not be parsed: 'Since there is no current execution of any steps in this context, we don't need to define any'\n",
            "- LLM output could not be parsed: 'Since there is no current execution of any steps in this context,'\n",
            "- LLM output could not be parsed: 'Since there is no current execution of any steps in this context,'\n",
            "- LLM output could not be parsed: 'Since there is no current execution of any steps in this context,'\n",
            "- LLM output could not be parsed: 'Since'\n",
            "- LLM output could not be parsed: 'Since'\n",
            "- LLM output could not be parsed: 'Since there is'\n",
            "- LLM output could not be parsed: 'Since'\n",
            "\n",
            "Final Computed C (by LLM algorithm):\n",
            "[[nan nan]\n",
            " [nan nan]]\n",
            "Result: INCOMPLETE (C was not fully computed)\n",
            "\n",
            "--- Test Case 9/10 ---\n",
            "Matrix A:\n",
            "[[ 0  1]\n",
            " [-5  2]]\n",
            "Matrix B:\n",
            "[[ 5  5]\n",
            " [-5 -2]]\n",
            "Expected C:\n",
            "[[ -5  -2]\n",
            " [-35 -29]]\n",
            "  LLM Output (Step 1): 'C = A * B' -> Parse Error.\n",
            "  LLM Output (Step 2): 'Next Step: M0 = A[0,0] * B[0,0]' -> Parse Error.\n",
            "  LLM Output (Step 3): 'Next Step: M1 = A[0,1] + A[1,1]' -> Parse Error.\n",
            "  LLM Output (Step 4): 'Next Step: M0 = A[0,0] * B[0,0]' -> Parse Error.\n",
            "  LLM Output (Step 5): 'Next Step: M1 = A[0,1] + A[1,1]' -> Parse Error.\n",
            "  LLM Output (Step 6): 'Next Step: M0 = A[0,0] * B[0,0]' -> Parse Error.\n",
            "  LLM Output (Step 7): 'Next Step: M1 = A[0,1] + A[1,1]' -> Parse Error.\n",
            "  LLM Output (Step 8): 'Next Step:' -> Parse Error.\n",
            "  LLM Output (Step 9): 'Next Step:' -> Parse Error.\n",
            "  LLM Output (Step 10): 'Next Step:' -> Parse Error.\n",
            "  LLM Output (Step 11): 'Next Step:' -> Parse Error.\n",
            "  LLM Output (Step 12): 'Next Step:' -> Parse Error.\n",
            "  LLM Output (Step 13): 'Next Step:' -> Parse Error.\n",
            "  LLM Output (Step 14): 'Next Step:' -> Parse Error.\n",
            "  LLM Output (Step 15): 'Next Step:' -> Parse Error.\n",
            "  LLM Output (Step 16): 'Next Step:' -> Parse Error.\n",
            "  LLM Output (Step 17): 'Next Step:' -> Parse Error.\n",
            "  LLM Output (Step 18): 'Next Step:' -> Parse Error.\n",
            "  LLM Output (Step 19): 'Next Step:' -> Parse Error.\n",
            "  LLM Output (Step 20): 'Next Step:' -> Parse Error.\n",
            "\n",
            "Generated DSL Algorithm:\n",
            "No valid DSL steps were executed.\n",
            "\n",
            "Errors encountered during generation/execution:\n",
            "- LLM output could not be parsed: 'C = A * B'\n",
            "- LLM output could not be parsed: 'Next Step: M0 = A[0,0] * B[0,0]'\n",
            "- LLM output could not be parsed: 'Next Step: M1 = A[0,1] + A[1,1]'\n",
            "- LLM output could not be parsed: 'Next Step: M0 = A[0,0] * B[0,0]'\n",
            "- LLM output could not be parsed: 'Next Step: M1 = A[0,1] + A[1,1]'\n",
            "- LLM output could not be parsed: 'Next Step: M0 = A[0,0] * B[0,0]'\n",
            "- LLM output could not be parsed: 'Next Step: M1 = A[0,1] + A[1,1]'\n",
            "- LLM output could not be parsed: 'Next Step:'\n",
            "- LLM output could not be parsed: 'Next Step:'\n",
            "- LLM output could not be parsed: 'Next Step:'\n",
            "- LLM output could not be parsed: 'Next Step:'\n",
            "- LLM output could not be parsed: 'Next Step:'\n",
            "- LLM output could not be parsed: 'Next Step:'\n",
            "- LLM output could not be parsed: 'Next Step:'\n",
            "- LLM output could not be parsed: 'Next Step:'\n",
            "- LLM output could not be parsed: 'Next Step:'\n",
            "- LLM output could not be parsed: 'Next Step:'\n",
            "- LLM output could not be parsed: 'Next Step:'\n",
            "- LLM output could not be parsed: 'Next Step:'\n",
            "- LLM output could not be parsed: 'Next Step:'\n",
            "\n",
            "Final Computed C (by LLM algorithm):\n",
            "[[nan nan]\n",
            " [nan nan]]\n",
            "Result: INCOMPLETE (C was not fully computed)\n",
            "\n",
            "--- Test Case 10/10 ---\n",
            "Matrix A:\n",
            "[[ 5  3]\n",
            " [ 1 -2]]\n",
            "Matrix B:\n",
            "[[1 4]\n",
            " [1 4]]\n",
            "Expected C:\n",
            "[[ 8 32]\n",
            " [-1 -4]]\n",
            "  LLM Output (Step 1): 'M0 = A[0,0]*B[0,0]\n",
            "Next DSL Step:' -> Parse Error.\n",
            "  LLM Output (Step 2): 'M0 = A[0,0]*B[0,0]\n",
            "Next DSL Step:' -> Parse Error.\n",
            "  LLM Output (Step 3): 'M0 = A[0,0]*B[0,0]\n",
            "Next DSL Step:' -> Parse Error.\n",
            "  LLM Output (Step 4): 'M0 = A[0,0]*B[0,0]\n",
            "Next DSL Step:' -> Parse Error.\n",
            "  LLM Output (Step 5): 'M0 = A[0,0]*B[0,0]\n",
            "Next DSL Step:' -> Parse Error.\n",
            "  LLM Output (Step 6): 'M0 = A[0,0]*B[0,0]\n",
            "Next DSL Step:' -> Parse Error.\n",
            "  LLM Output (Step 7): 'M0 = A[0,0]*B[0,0]\n",
            "Next DSL Step:' -> Parse Error.\n",
            "  LLM Output (Step 8): 'M0 = A[0,0]*B[0,0]\n",
            "Next DSL Step:' -> Parse Error.\n",
            "  LLM Output (Step 9): 'M0 = A[0,0]*B[0,0]\n",
            "Next DSL Step:' -> Parse Error.\n",
            "  LLM Output (Step 10): 'M0 = A[0,0]*B[0,0]\n",
            "Next DSL Step:' -> Parse Error.\n",
            "  LLM Output (Step 11): 'M0 = A[0,0]*B[0,0]\n",
            "Next DSL Step:' -> Parse Error.\n",
            "  LLM Output (Step 12): 'M0 = A[0,0]*B[0,0]\n",
            "Next DSL Step:' -> Parse Error.\n",
            "  LLM Output (Step 13): 'M0 = A[0,0]*B[0,0]\n",
            "Next DSL Step:' -> Parse Error.\n",
            "  LLM Output (Step 14): 'M0 = A[0,0]*B[0,0]\n",
            "Next DSL Step:' -> Parse Error.\n",
            "  LLM Output (Step 15): 'M0 = A[0,0]*B[0,0]\n",
            "Next DSL Step:' -> Parse Error.\n",
            "  LLM Output (Step 16): 'M0 = A[0,0]*B[0,0]\n",
            "Next DSL Step:' -> Parse Error.\n",
            "  LLM Output (Step 17): 'M0 = A[0,0]*B[0,0]\n",
            "Next DSL Step:' -> Parse Error.\n",
            "  LLM Output (Step 18): 'M0 = A[0,0]*B[0,0]\n",
            "Next DSL Step:' -> Parse Error.\n",
            "  LLM Output (Step 19): 'M0 = A[0,0]*B[0,0]\n",
            "Next DSL Step:' -> Parse Error.\n",
            "  LLM Output (Step 20): 'M0 = A[0,0]*B[0,0]\n",
            "Next DSL Step:' -> Parse Error.\n",
            "\n",
            "Generated DSL Algorithm:\n",
            "No valid DSL steps were executed.\n",
            "\n",
            "Errors encountered during generation/execution:\n",
            "- LLM output could not be parsed: 'M0 = A[0,0]*B[0,0]\n",
            "Next DSL Step:'\n",
            "- LLM output could not be parsed: 'M0 = A[0,0]*B[0,0]\n",
            "Next DSL Step:'\n",
            "- LLM output could not be parsed: 'M0 = A[0,0]*B[0,0]\n",
            "Next DSL Step:'\n",
            "- LLM output could not be parsed: 'M0 = A[0,0]*B[0,0]\n",
            "Next DSL Step:'\n",
            "- LLM output could not be parsed: 'M0 = A[0,0]*B[0,0]\n",
            "Next DSL Step:'\n",
            "- LLM output could not be parsed: 'M0 = A[0,0]*B[0,0]\n",
            "Next DSL Step:'\n",
            "- LLM output could not be parsed: 'M0 = A[0,0]*B[0,0]\n",
            "Next DSL Step:'\n",
            "- LLM output could not be parsed: 'M0 = A[0,0]*B[0,0]\n",
            "Next DSL Step:'\n",
            "- LLM output could not be parsed: 'M0 = A[0,0]*B[0,0]\n",
            "Next DSL Step:'\n",
            "- LLM output could not be parsed: 'M0 = A[0,0]*B[0,0]\n",
            "Next DSL Step:'\n",
            "- LLM output could not be parsed: 'M0 = A[0,0]*B[0,0]\n",
            "Next DSL Step:'\n",
            "- LLM output could not be parsed: 'M0 = A[0,0]*B[0,0]\n",
            "Next DSL Step:'\n",
            "- LLM output could not be parsed: 'M0 = A[0,0]*B[0,0]\n",
            "Next DSL Step:'\n",
            "- LLM output could not be parsed: 'M0 = A[0,0]*B[0,0]\n",
            "Next DSL Step:'\n",
            "- LLM output could not be parsed: 'M0 = A[0,0]*B[0,0]\n",
            "Next DSL Step:'\n",
            "- LLM output could not be parsed: 'M0 = A[0,0]*B[0,0]\n",
            "Next DSL Step:'\n",
            "- LLM output could not be parsed: 'M0 = A[0,0]*B[0,0]\n",
            "Next DSL Step:'\n",
            "- LLM output could not be parsed: 'M0 = A[0,0]*B[0,0]\n",
            "Next DSL Step:'\n",
            "- LLM output could not be parsed: 'M0 = A[0,0]*B[0,0]\n",
            "Next DSL Step:'\n",
            "- LLM output could not be parsed: 'M0 = A[0,0]*B[0,0]\n",
            "Next DSL Step:'\n",
            "\n",
            "Final Computed C (by LLM algorithm):\n",
            "[[nan nan]\n",
            " [nan nan]]\n",
            "Result: INCOMPLETE (C was not fully computed)\n",
            "\n",
            "--- Test Summary ---\n",
            "Total correct algorithms: 0/10 (0.00%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%capture\n",
        "# !pip install transformers torch accelerate sentencepiece\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import numpy as np\n",
        "import re\n",
        "import random\n",
        "\n",
        "# --- Model Setup ---\n",
        "MODEL_NAME = \"Qwen/Qwen1.5-0.5B-Chat\"\n",
        "\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        torch_dtype=\"auto\",\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "    print(f\"Successfully loaded model and tokenizer: {MODEL_NAME}\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"Model is on GPU: {model.device}\")\n",
        "    else:\n",
        "        print(\"Model is on CPU. Execution might be slow.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")\n",
        "    model = None\n",
        "\n",
        "# --- DSL Definition and Parser ---\n",
        "ASSIGN_INTERMEDIATE_REGEX = re.compile(r\"^\\s*(M\\d+)\\s*=\\s*([A-Z]\\[\\d,\\d\\]|M\\d+)\\s*([+\\-*])\\s*([A-Z]\\[\\d,\\d\\]|M\\d+)\\s*$\")\n",
        "ASSIGN_OUTPUT_REGEX = re.compile(r\"^\\s*(C\\[\\d,\\d\\])\\s*=\\s*([A-Z]\\[\\d,\\d\\]|M\\d+)\\s*$\")\n",
        "ASSIGN_INTERMEDIATE_SINGLE_REGEX = re.compile(r\"^\\s*(M\\d+)\\s*=\\s*([A-Z]\\[\\d,\\d\\]|M\\d+)\\s*$\")\n",
        "\n",
        "def parse_dsl_step(line):\n",
        "    line = line.strip()\n",
        "    match_inter = ASSIGN_INTERMEDIATE_REGEX.match(line)\n",
        "    if match_inter:\n",
        "        return {\"type\": \"intermediate_op\", \"out\": match_inter.group(1), \"op1\": match_inter.group(2),\n",
        "                \"op_type\": match_inter.group(3), \"op2\": match_inter.group(4)}\n",
        "\n",
        "    match_output = ASSIGN_OUTPUT_REGEX.match(line)\n",
        "    if match_output:\n",
        "        return {\"type\": \"output_assign\", \"out_cell\": match_output.group(1), \"source\": match_output.group(2)}\n",
        "\n",
        "    match_inter_single = ASSIGN_INTERMEDIATE_SINGLE_REGEX.match(line)\n",
        "    if match_inter_single:\n",
        "        return {\"type\": \"intermediate_assign_single\", \"out\": match_inter_single.group(1), \"source\": match_inter_single.group(2)}\n",
        "    return None\n",
        "\n",
        "# --- Algorithm Executor ---\n",
        "class MatrixMathState:\n",
        "    def __init__(self, a, b): # Expects numpy arrays\n",
        "        if not (isinstance(a, np.ndarray) and isinstance(b, np.ndarray) and a.shape == (2,2) and b.shape == (2,2)):\n",
        "            raise ValueError(\"Matrices A and B must be 2x2 numpy arrays.\")\n",
        "        self.matrix_a = a\n",
        "        self.matrix_b = b\n",
        "        self.expected_c = np.matmul(a, b)\n",
        "        self.intermediate_vars = {}\n",
        "        self.output_c = np.full((2,2), np.nan)\n",
        "        self.errors = []\n",
        "        self.dsl_steps_executed = []\n",
        "\n",
        "    def get_operand_value(self, operand_str):\n",
        "        operand_str = operand_str.strip()\n",
        "        if operand_str.startswith(\"M\"):\n",
        "            if operand_str in self.intermediate_vars:\n",
        "                return self.intermediate_vars[operand_str]\n",
        "            else:\n",
        "                self.errors.append(f\"Error: Intermediate variable {operand_str} not found.\")\n",
        "                return None\n",
        "        elif operand_str.startswith(\"A[\"):\n",
        "            try:\n",
        "                r, c = map(int, operand_str[2:-1].split(','))\n",
        "                return self.matrix_a[r,c]\n",
        "            except:\n",
        "                self.errors.append(f\"Error: Invalid format or index for matrix A operand: {operand_str}\")\n",
        "                return None\n",
        "        elif operand_str.startswith(\"B[\"):\n",
        "            try:\n",
        "                r, c = map(int, operand_str[2:-1].split(','))\n",
        "                return self.matrix_b[r,c]\n",
        "            except:\n",
        "                self.errors.append(f\"Error: Invalid format or index for matrix B operand: {operand_str}\")\n",
        "                return None\n",
        "        self.errors.append(f\"Error: Unknown operand type: {operand_str}\")\n",
        "        return None\n",
        "\n",
        "    def execute_step(self, parsed_step, original_line=\"\"):\n",
        "        if parsed_step is None: # Should not happen if clean_llm_output returns FORMAT_ERROR for unparsable\n",
        "            self.errors.append(f\"Internal Error: execute_step called with None parsed_step for line: {original_line}\")\n",
        "            return False\n",
        "\n",
        "        op_type = parsed_step[\"type\"]\n",
        "        success = False\n",
        "        try:\n",
        "            if op_type == \"intermediate_op\":\n",
        "                val1 = self.get_operand_value(parsed_step[\"op1\"])\n",
        "                val2 = self.get_operand_value(parsed_step[\"op2\"])\n",
        "                if val1 is not None and val2 is not None:\n",
        "                    op = parsed_step[\"op_type\"]\n",
        "                    if op == '+': result = val1 + val2\n",
        "                    elif op == '-': result = val1 - val2\n",
        "                    elif op == '*': result = val1 * val2\n",
        "                    else:\n",
        "                        self.errors.append(f\"Unknown arithmetic operator: {op}\")\n",
        "                        return False # Keep this return\n",
        "                    self.intermediate_vars[parsed_step[\"out\"]] = result\n",
        "                    success = True\n",
        "            elif op_type == \"intermediate_assign_single\":\n",
        "                val_source = self.get_operand_value(parsed_step[\"source\"])\n",
        "                if val_source is not None:\n",
        "                    self.intermediate_vars[parsed_step[\"out\"]] = val_source\n",
        "                    success = True\n",
        "            elif op_type == \"output_assign\":\n",
        "                val_source = self.get_operand_value(parsed_step[\"source\"])\n",
        "                if val_source is not None:\n",
        "                    cell_str = parsed_step[\"out_cell\"]\n",
        "                    r, c = map(int, cell_str[2:-1].split(','))\n",
        "                    if 0 <= r < 2 and 0 <= c < 2:\n",
        "                        self.output_c[r,c] = val_source\n",
        "                        success = True\n",
        "                    else:\n",
        "                        self.errors.append(f\"Error: Output cell C[{r},{c}] out of bounds.\")\n",
        "            else:\n",
        "                self.errors.append(f\"Unknown parsed step type: {op_type}\")\n",
        "        except Exception as e:\n",
        "            self.errors.append(f\"Execution error for '{original_line}': {e}\")\n",
        "            success = False\n",
        "\n",
        "        if success:\n",
        "            self.dsl_steps_executed.append(original_line)\n",
        "        # If not success, error is already logged by get_operand_value or within this function\n",
        "        return success\n",
        "\n",
        "    def is_c_complete(self):\n",
        "        return not np.isnan(self.output_c).any()\n",
        "\n",
        "    def verify_c(self):\n",
        "        if not self.is_c_complete():\n",
        "            return False\n",
        "        return np.allclose(self.output_c, self.expected_c)\n",
        "\n",
        "# --- LLM Interaction Logic ---\n",
        "def clean_llm_output(raw_output):\n",
        "    cleaned = raw_output.strip()\n",
        "\n",
        "    code_block_match = re.search(r\"```(?:[a-zA-Z]+\\n)?(.*?)```\", cleaned, re.DOTALL)\n",
        "    if code_block_match:\n",
        "        potential_lines = code_block_match.group(1).strip().split('\\n')\n",
        "        if potential_lines:\n",
        "            cleaned = potential_lines[0].strip()\n",
        "\n",
        "    prefixes_to_remove = [\n",
        "        \"Sure! Here's the next DSL step:\", \"Okay, the next DSL step is:\",\n",
        "        \"Here's the next DSL step:\", \"The next DSL step is:\", \"Next DSL step:\",\n",
        "        \"Here is the next step:\", \"Here's the step:\", \"Okay, here's the step:\",\n",
        "        \"Next single DSL step:\", \"Next DSL Step:\", \"The DSL step is:\",\n",
        "        \"The next step is:\", \"Alright, the next step is:\"\n",
        "    ]\n",
        "    temp_cleaned_after_prefix_removal = cleaned\n",
        "    for prefix in prefixes_to_remove:\n",
        "        if temp_cleaned_after_prefix_removal.lower().startswith(prefix.lower()):\n",
        "            temp_cleaned_after_prefix_removal = temp_cleaned_after_prefix_removal[len(prefix):].strip()\n",
        "            break\n",
        "    cleaned = temp_cleaned_after_prefix_removal\n",
        "\n",
        "    possible_dsl_lines = cleaned.split('\\n')\n",
        "    for line_candidate in possible_dsl_lines:\n",
        "        line_candidate = line_candidate.strip()\n",
        "        if parse_dsl_step(line_candidate):\n",
        "            return line_candidate\n",
        "\n",
        "    for line_candidate in possible_dsl_lines:\n",
        "        if line_candidate.strip():\n",
        "            return f\"FORMAT_ERROR: {line_candidate.strip()}\"\n",
        "\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "def get_llm_step_suggestion(current_prompt_messages):\n",
        "    if model is None or tokenizer is None:\n",
        "        # This case should be handled before calling, but as a safeguard:\n",
        "        print(\"Error: Model or tokenizer not loaded in get_llm_step_suggestion.\")\n",
        "        return \"Error: Model not loaded.\"\n",
        "\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        current_prompt_messages,\n",
        "        add_generation_prompt=True, # This is important for chat models\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=25, # Reduced\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        do_sample=True,\n",
        "        temperature=0.4, # Reduced\n",
        "        top_k=30,        # Reduced\n",
        "        top_p=0.9,\n",
        "        no_repeat_ngram_size=2 # More aggressive\n",
        "    )\n",
        "    response_ids = outputs[0][input_ids.shape[-1]:]\n",
        "    llm_raw_output = tokenizer.decode(response_ids, skip_special_tokens=True).strip()\n",
        "    return clean_llm_output(llm_raw_output)\n",
        "\n",
        "\n",
        "def generate_matrix_mult_algorithm(current_mat_a_np, current_mat_b_np, max_llm_steps=20):\n",
        "    state = MatrixMathState(current_mat_a_np, current_mat_b_np) # Pass numpy arrays directly\n",
        "\n",
        "    system_message = f\"\"\"ASSISTANT'S ONLY TASK: Output ONE SINGLE VALID DSL line for 2x2 matrix multiplication (C = A * B).\n",
        "NO PYTHON. NO EXPLANATIONS. ONLY THE DSL LINE.\n",
        "If stuck, output ONLY: Error: Cannot determine next step.\n",
        "\n",
        "DSL FORMAT:\n",
        "M<id> = <op1> + <op2>\n",
        "M<id> = <op1> - <op2>\n",
        "M<id> = <op1> * <op2>\n",
        "M<id> = <operand>\n",
        "C[r,c] = <operand>\n",
        "(Operands: A[r,c], B[r,c], or M<id>)\n",
        "\n",
        "VALID DSL EXAMPLES:\n",
        "M0 = A[0,0] * B[0,0]\n",
        "M1 = M0 + A[0,1]\n",
        "C[0,0] = M1\n",
        "\"\"\"\n",
        "    conversation_history = [{\"role\": \"system\", \"content\": system_message}]\n",
        "\n",
        "    example_matrix_a_repr = \"[[1, 2], [3, 4]]\"\n",
        "    example_matrix_b_repr = \"[[5, 6], [7, 8]]\"\n",
        "    example_output_c_repr = \"[[nan, nan],\\n [nan, nan]]\" # Explicitly two lines for example\n",
        "    example_initial_user_prompt = (\n",
        "        f\"Current algorithm state:\\nMatrices:\\nA = {example_matrix_a_repr}\\nB = {example_matrix_b_repr}\\n\"\n",
        "        \"Executed Steps:\\nNone yet.\\n\"\n",
        "        \"Intermediate Variables (M values): {}\\n\"\n",
        "        f\"Output Matrix C (NaN means not yet computed):\\n{example_output_c_repr}\\n\"\n",
        "        \"Provide the NEXT SINGLE DSL step according to the rules:\"\n",
        "    )\n",
        "    conversation_history.append({\"role\": \"user\", \"content\": example_initial_user_prompt})\n",
        "    conversation_history.append({\"role\": \"assistant\", \"content\": \"M0 = A[0,0] * B[0,0]\"})\n",
        "\n",
        "    for step_num in range(max_llm_steps):\n",
        "        if state.is_c_complete():\n",
        "            print(\"  Algorithm complete: Output C is fully populated.\")\n",
        "            break\n",
        "\n",
        "        prompt_matrix_a_repr = str(state.matrix_a.tolist())\n",
        "        prompt_matrix_b_repr = str(state.matrix_b.tolist())\n",
        "        output_c_repr_lines = str(state.output_c).split('\\n') # Ensure C is also consistently formatted\n",
        "        output_c_repr_for_prompt = \"\\n\".join(output_c_repr_lines)\n",
        "\n",
        "\n",
        "        current_state_summary = (\n",
        "            f\"Current algorithm state:\\nMatrices:\\nA = {prompt_matrix_a_repr}\\nB = {prompt_matrix_b_repr}\\n\"\n",
        "            \"Executed Steps:\\n\" + (\"\\n\".join(state.dsl_steps_executed) if state.dsl_steps_executed else \"None yet.\") + \"\\n\"\n",
        "            f\"Intermediate Variables (M values): {state.intermediate_vars}\\n\"\n",
        "            f\"Output Matrix C (NaN means not yet computed):\\n{output_c_repr_for_prompt}\\n\"\n",
        "            \"Provide the NEXT SINGLE DSL step according to the rules:\"\n",
        "        )\n",
        "\n",
        "        user_message = {\"role\": \"user\", \"content\": current_state_summary}\n",
        "        # Use a slice of history if it gets too long, though Qwen0.5B context is 4096 tokens.\n",
        "        # For now, using full history.\n",
        "        # max_history_turns = 5 # Keep last 5 assistant/user pairs + system + initial example\n",
        "        # if len(conversation_history) > (2 * max_history_turns + 3): # 3 for system + 2 for initial example\n",
        "        #     prompt_messages_for_llm = [conversation_history[0]] + \\\n",
        "        #                               conversation_history[1:3] + \\\n",
        "        #                               conversation_history[-(2*max_history_turns):] + \\\n",
        "        #                               [user_message]\n",
        "        # else:\n",
        "        prompt_messages_for_llm = conversation_history + [user_message]\n",
        "\n",
        "\n",
        "        llm_dsl_line = get_llm_step_suggestion(prompt_messages_for_llm)\n",
        "\n",
        "        if llm_dsl_line == \"Error: Model not loaded.\": # Check specific error string\n",
        "            state.errors.append(llm_dsl_line)\n",
        "            print(f\"  Stopping due to model loading error string from LLM.\")\n",
        "            break\n",
        "        if llm_dsl_line == \"Error: Cannot determine next step.\":\n",
        "            state.errors.append(\"LLM indicated it cannot determine next step.\")\n",
        "            print(f\"  LLM Output (Step {step_num+1}): '{llm_dsl_line}' -> LLM cannot determine step. Stopping this test case.\")\n",
        "            break\n",
        "\n",
        "        # Add current turn to history AFTER getting suggestion\n",
        "        conversation_history.append(user_message)\n",
        "        conversation_history.append({\"role\": \"assistant\", \"content\": llm_dsl_line}) # Store cleaned line\n",
        "\n",
        "        if llm_dsl_line.startswith(\"FORMAT_ERROR:\"):\n",
        "            state.errors.append(f\"LLM output format error (cleaned): '{llm_dsl_line}'\")\n",
        "            print(f\"  LLM Output (Step {step_num+1}) (cleaned to format error): '{llm_dsl_line}') -> Parse Error. Stopping this test case.\")\n",
        "            break # Stop on format error\n",
        "\n",
        "        parsed = parse_dsl_step(llm_dsl_line) # Should parse if not FORMAT_ERROR\n",
        "\n",
        "        if parsed: # If it's not a format error, it should have been parsed by clean_llm_output's check\n",
        "            if not state.execute_step(parsed, llm_dsl_line):\n",
        "                # Error already logged in state.execute_step\n",
        "                print(f\"  LLM Output (Step {step_num+1}): '{llm_dsl_line}' -> Parsed, but execution failed. Error: {state.errors[-1] if state.errors else 'Unknown exec error'}. Stopping this test case.\")\n",
        "                # break # Decide if to stop on execution error\n",
        "            else:\n",
        "                print(f\"  LLM Output (Step {step_num+1}): '{llm_dsl_line}' -> Executed successfully.\")\n",
        "        else:\n",
        "            # This case should ideally be caught by \"FORMAT_ERROR\" from clean_llm_output\n",
        "            state.errors.append(f\"LLM output (after cleaning: '{llm_dsl_line}') still could not be parsed by primary parser.\")\n",
        "            print(f\"  LLM Output (Step {step_num+1}) (cleaned: '{llm_dsl_line}') -> Internal Parse Error. Stopping this test case.\")\n",
        "            break\n",
        "\n",
        "    return state\n",
        "\n",
        "# --- Main Test Loop ---\n",
        "if model is None:\n",
        "    print(\"\\nSkipping main test loop as model failed to load.\")\n",
        "else:\n",
        "    print(\"\\n--- Starting 2x2 Matrix Multiplication Test (Revised v3) ---\")\n",
        "    num_tests = 10 # You can reduce this for faster testing e.g., to 3-5\n",
        "    correct_algorithms = 0\n",
        "\n",
        "    for i in range(num_tests):\n",
        "        print(f\"\\n--- Test Case {i+1}/{num_tests} ---\")\n",
        "        mat_a_np = np.random.randint(-5, 6, size=(2,2))\n",
        "        mat_b_np = np.random.randint(-5, 6, size=(2,2))\n",
        "\n",
        "        print(f\"Matrix A:\\n{mat_a_np}\")\n",
        "        print(f\"Matrix B:\\n{mat_b_np}\")\n",
        "        expected_c = np.matmul(mat_a_np, mat_b_np)\n",
        "        print(f\"Expected C:\\n{expected_c}\")\n",
        "\n",
        "        final_state = generate_matrix_mult_algorithm(mat_a_np, mat_b_np) # Pass numpy arrays\n",
        "\n",
        "        print(\"\\nGenerated DSL Algorithm:\")\n",
        "        if final_state.dsl_steps_executed:\n",
        "            for step_line in final_state.dsl_steps_executed:\n",
        "                print(step_line)\n",
        "        else:\n",
        "            print(\"No valid DSL steps were executed.\")\n",
        "\n",
        "        if final_state.errors:\n",
        "            print(\"\\nErrors encountered during generation/execution:\")\n",
        "            # Print unique errors to avoid too much repetition if LLM gets stuck\n",
        "            unique_errors = []\n",
        "            for err in final_state.errors:\n",
        "                if err not in unique_errors:\n",
        "                    unique_errors.append(err)\n",
        "            for err in unique_errors:\n",
        "                print(f\"- {err}\")\n",
        "\n",
        "\n",
        "        print(f\"\\nFinal Computed C (by LLM algorithm):\\n{final_state.output_c}\")\n",
        "\n",
        "        if final_state.is_c_complete():\n",
        "            if final_state.verify_c():\n",
        "                print(\"Result: CORRECT\")\n",
        "                correct_algorithms += 1\n",
        "            else:\n",
        "                print(\"Result: INCORRECT (C is complete but values are wrong)\")\n",
        "        else:\n",
        "            if final_state.dsl_steps_executed: # It tried but didn't finish\n",
        "                 print(\"Result: INCOMPLETE (C was not fully computed but some steps were made)\")\n",
        "            else: # No steps made, likely immediate errors\n",
        "                 print(\"Result: FAILED TO START (No valid DSL steps executed)\")\n",
        "\n",
        "\n",
        "    print(f\"\\n--- Test Summary ---\")\n",
        "    print(f\"Total correct algorithms: {correct_algorithms}/{num_tests} ({correct_algorithms/num_tests*100:.2f}%)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_N0IOeJ2b_LF",
        "outputId": "922c99e4-8e93-430a-a968-685b890938a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded model and tokenizer: Qwen/Qwen1.5-0.5B-Chat\n",
            "Model is on GPU: cuda:0\n",
            "\n",
            "--- Starting 2x2 Matrix Multiplication Test (Revised v3) ---\n",
            "\n",
            "--- Test Case 1/10 ---\n",
            "Matrix A:\n",
            "[[-4  5]\n",
            " [ 3 -4]]\n",
            "Matrix B:\n",
            "[[ 2  2]\n",
            " [-4  1]]\n",
            "Expected C:\n",
            "[[-28  -3]\n",
            " [ 22   2]]\n",
            "  LLM Output (Step 1) (cleaned to format error): 'FORMAT_ERROR: M2 = B[-4][2] + B [-2][1] - M[2') -> Parse Error. Stopping this test case.\n",
            "\n",
            "Generated DSL Algorithm:\n",
            "No valid DSL steps were executed.\n",
            "\n",
            "Errors encountered during generation/execution:\n",
            "- LLM output format error (cleaned): 'FORMAT_ERROR: M2 = B[-4][2] + B [-2][1] - M[2'\n",
            "\n",
            "Final Computed C (by LLM algorithm):\n",
            "[[nan nan]\n",
            " [nan nan]]\n",
            "Result: FAILED TO START (No valid DSL steps executed)\n",
            "\n",
            "--- Test Case 2/10 ---\n",
            "Matrix A:\n",
            "[[ 1 -2]\n",
            " [-3 -1]]\n",
            "Matrix B:\n",
            "[[-2 -4]\n",
            " [-1 -1]]\n",
            "Expected C:\n",
            "[[ 0 -2]\n",
            " [ 7 13]]\n",
            "  LLM Output (Step 1) (cleaned to format error): 'FORMAT_ERROR: Next一步 is to multiply the matrices by the appropriate scalar value.') -> Parse Error. Stopping this test case.\n",
            "\n",
            "Generated DSL Algorithm:\n",
            "No valid DSL steps were executed.\n",
            "\n",
            "Errors encountered during generation/execution:\n",
            "- LLM output format error (cleaned): 'FORMAT_ERROR: Next一步 is to multiply the matrices by the appropriate scalar value.'\n",
            "\n",
            "Final Computed C (by LLM algorithm):\n",
            "[[nan nan]\n",
            " [nan nan]]\n",
            "Result: FAILED TO START (No valid DSL steps executed)\n",
            "\n",
            "--- Test Case 3/10 ---\n",
            "Matrix A:\n",
            "[[-2 -2]\n",
            " [ 5  4]]\n",
            "Matrix B:\n",
            "[[-3  1]\n",
            " [ 0  1]]\n",
            "Expected C:\n",
            "[[  6  -4]\n",
            " [-15   9]]\n",
            "  LLM Output (Step 1) (cleaned to format error): 'FORMAT_ERROR: Next Step: Perform element-wise multiplication of A and B using the matrix乘法 operator.') -> Parse Error. Stopping this test case.\n",
            "\n",
            "Generated DSL Algorithm:\n",
            "No valid DSL steps were executed.\n",
            "\n",
            "Errors encountered during generation/execution:\n",
            "- LLM output format error (cleaned): 'FORMAT_ERROR: Next Step: Perform element-wise multiplication of A and B using the matrix乘法 operator.'\n",
            "\n",
            "Final Computed C (by LLM algorithm):\n",
            "[[nan nan]\n",
            " [nan nan]]\n",
            "Result: FAILED TO START (No valid DSL steps executed)\n",
            "\n",
            "--- Test Case 4/10 ---\n",
            "Matrix A:\n",
            "[[ 4  5]\n",
            " [ 5 -2]]\n",
            "Matrix B:\n",
            "[[-1  3]\n",
            " [ 5 -2]]\n",
            "Expected C:\n",
            "[[ 21   2]\n",
            " [-15  19]]\n",
            "  LLM Output (Step 1) (cleaned to format error): 'FORMAT_ERROR: Next一步 is to perform matrix addition using the operator +.') -> Parse Error. Stopping this test case.\n",
            "\n",
            "Generated DSL Algorithm:\n",
            "No valid DSL steps were executed.\n",
            "\n",
            "Errors encountered during generation/execution:\n",
            "- LLM output format error (cleaned): 'FORMAT_ERROR: Next一步 is to perform matrix addition using the operator +.'\n",
            "\n",
            "Final Computed C (by LLM algorithm):\n",
            "[[nan nan]\n",
            " [nan nan]]\n",
            "Result: FAILED TO START (No valid DSL steps executed)\n",
            "\n",
            "--- Test Case 5/10 ---\n",
            "Matrix A:\n",
            "[[ 1  1]\n",
            " [-1  1]]\n",
            "Matrix B:\n",
            "[[-1  2]\n",
            " [-3  4]]\n",
            "Expected C:\n",
            "[[-4  6]\n",
            " [-2  2]]\n",
            "  LLM Output (Step 1) (cleaned to format error): 'FORMAT_ERROR: M2 = B[-1][0]*A[-2][1]') -> Parse Error. Stopping this test case.\n",
            "\n",
            "Generated DSL Algorithm:\n",
            "No valid DSL steps were executed.\n",
            "\n",
            "Errors encountered during generation/execution:\n",
            "- LLM output format error (cleaned): 'FORMAT_ERROR: M2 = B[-1][0]*A[-2][1]'\n",
            "\n",
            "Final Computed C (by LLM algorithm):\n",
            "[[nan nan]\n",
            " [nan nan]]\n",
            "Result: FAILED TO START (No valid DSL steps executed)\n",
            "\n",
            "--- Test Case 6/10 ---\n",
            "Matrix A:\n",
            "[[-2  2]\n",
            " [-3  4]]\n",
            "Matrix B:\n",
            "[[-1 -3]\n",
            " [-2 -5]]\n",
            "Expected C:\n",
            "[[ -2  -4]\n",
            " [ -5 -11]]\n",
            "  LLM Output (Step 1) (cleaned to format error): 'FORMAT_ERROR: Next Step: M2 = B[-1][-3] + B[[-5][0]]') -> Parse Error. Stopping this test case.\n",
            "\n",
            "Generated DSL Algorithm:\n",
            "No valid DSL steps were executed.\n",
            "\n",
            "Errors encountered during generation/execution:\n",
            "- LLM output format error (cleaned): 'FORMAT_ERROR: Next Step: M2 = B[-1][-3] + B[[-5][0]]'\n",
            "\n",
            "Final Computed C (by LLM algorithm):\n",
            "[[nan nan]\n",
            " [nan nan]]\n",
            "Result: FAILED TO START (No valid DSL steps executed)\n",
            "\n",
            "--- Test Case 7/10 ---\n",
            "Matrix A:\n",
            "[[ 1  0]\n",
            " [ 4 -1]]\n",
            "Matrix B:\n",
            "[[-3  5]\n",
            " [ 0  2]]\n",
            "Expected C:\n",
            "[[ -3   5]\n",
            " [-12  18]]\n",
            "  LLM Output (Step 1) (cleaned to format error): 'FORMAT_ERROR: Next Step: M2 = B[-3][0]') -> Parse Error. Stopping this test case.\n",
            "\n",
            "Generated DSL Algorithm:\n",
            "No valid DSL steps were executed.\n",
            "\n",
            "Errors encountered during generation/execution:\n",
            "- LLM output format error (cleaned): 'FORMAT_ERROR: Next Step: M2 = B[-3][0]'\n",
            "\n",
            "Final Computed C (by LLM algorithm):\n",
            "[[nan nan]\n",
            " [nan nan]]\n",
            "Result: FAILED TO START (No valid DSL steps executed)\n",
            "\n",
            "--- Test Case 8/10 ---\n",
            "Matrix A:\n",
            "[[1 0]\n",
            " [0 5]]\n",
            "Matrix B:\n",
            "[[-3 -1]\n",
            " [-3 -3]]\n",
            "Expected C:\n",
            "[[ -3  -1]\n",
            " [-15 -15]]\n",
            "  LLM Output (Step 1) (cleaned to format error): 'FORMAT_ERROR: Next Step: M3 = B[-3][-1] + B[M3]') -> Parse Error. Stopping this test case.\n",
            "\n",
            "Generated DSL Algorithm:\n",
            "No valid DSL steps were executed.\n",
            "\n",
            "Errors encountered during generation/execution:\n",
            "- LLM output format error (cleaned): 'FORMAT_ERROR: Next Step: M3 = B[-3][-1] + B[M3]'\n",
            "\n",
            "Final Computed C (by LLM algorithm):\n",
            "[[nan nan]\n",
            " [nan nan]]\n",
            "Result: FAILED TO START (No valid DSL steps executed)\n",
            "\n",
            "--- Test Case 9/10 ---\n",
            "Matrix A:\n",
            "[[ 2  3]\n",
            " [ 4 -5]]\n",
            "Matrix B:\n",
            "[[ 0 -5]\n",
            " [ 4  5]]\n",
            "Expected C:\n",
            "[[ 12   5]\n",
            " [-20 -45]]\n",
            "  LLM Output (Step 1) (cleaned to format error): 'FORMAT_ERROR: Next Step: Perform matrix addition using the result of the previous step.') -> Parse Error. Stopping this test case.\n",
            "\n",
            "Generated DSL Algorithm:\n",
            "No valid DSL steps were executed.\n",
            "\n",
            "Errors encountered during generation/execution:\n",
            "- LLM output format error (cleaned): 'FORMAT_ERROR: Next Step: Perform matrix addition using the result of the previous step.'\n",
            "\n",
            "Final Computed C (by LLM algorithm):\n",
            "[[nan nan]\n",
            " [nan nan]]\n",
            "Result: FAILED TO START (No valid DSL steps executed)\n",
            "\n",
            "--- Test Case 10/10 ---\n",
            "Matrix A:\n",
            "[[-5 -3]\n",
            " [ 4 -2]]\n",
            "Matrix B:\n",
            "[[-4  4]\n",
            " [ 2 -1]]\n",
            "Expected C:\n",
            "[[ 14 -17]\n",
            " [-20  18]]\n",
            "  LLM Output (Step 1) (cleaned to format error): 'FORMAT_ERROR: M2 = (A[1][2] - A[-1]) / (B[2') -> Parse Error. Stopping this test case.\n",
            "\n",
            "Generated DSL Algorithm:\n",
            "No valid DSL steps were executed.\n",
            "\n",
            "Errors encountered during generation/execution:\n",
            "- LLM output format error (cleaned): 'FORMAT_ERROR: M2 = (A[1][2] - A[-1]) / (B[2'\n",
            "\n",
            "Final Computed C (by LLM algorithm):\n",
            "[[nan nan]\n",
            " [nan nan]]\n",
            "Result: FAILED TO START (No valid DSL steps executed)\n",
            "\n",
            "--- Test Summary ---\n",
            "Total correct algorithms: 0/10 (0.00%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate STF"
      ],
      "metadata": {
        "id": "HnsRMMqSjKoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import json\n",
        "\n",
        "# --- System Message and DSL Definition (for our reference and inclusion in prompts) ---\n",
        "SYSTEM_MESSAGE = \"\"\"ASSISTANT'S ONLY TASK: Output ONE SINGLE VALID DSL line for 2x2 matrix multiplication (C = A * B).\n",
        "NO PYTHON. NO EXPLANATIONS. ONLY THE DSL LINE.\n",
        "If stuck, output ONLY: Error: Cannot determine next step.\"\"\"\n",
        "\n",
        "DSL_FORMAT_RULES = \"\"\"DSL FORMAT:\n",
        "M<id> = <op1> + <op2>\n",
        "M<id> = <op1> - <op2>\n",
        "M<id> = <op1> * <op2>\n",
        "M<id> = <operand>\n",
        "C[r,c] = <operand>\n",
        "(Operands: A[r,c], B[r,c], or M<id>)\"\"\"\n",
        "\n",
        "# --- Dataset Generation ---\n",
        "\n",
        "def format_matrix_for_prompt(matrix_name, matrix_values):\n",
        "    return f\"{matrix_name} = {matrix_values}\"\n",
        "\n",
        "def format_c_matrix_for_prompt(c_matrix_computed_values):\n",
        "    c_display = [[\"nan\" for _ in range(2)] for _ in range(2)]\n",
        "    for key, val_expr_tuple in c_matrix_computed_values.items(): # val_expr_tuple is (value, expression_str)\n",
        "        # key is like \"C[0,0]\"\n",
        "        r, c = int(key[2]), int(key[4])\n",
        "        # For display, we just show if it's computed, not the value itself in the prompt C matrix\n",
        "        if val_expr_tuple is not None: # if it has an expression, it's \"computed\"\n",
        "             c_display[r][c] = \"computed\" # Or we could show the M-var, e.g., C[0,0] = M2\n",
        "    return \"Output Matrix C (nan means not yet computed, computed means a DSL line has assigned to it):\\n\" + \\\n",
        "           f\"[[{c_display[0][0]}, {c_display[0][1]}],\\n\" + \\\n",
        "           f\" [{c_display[1][0]}, {c_display[1][1]}]]\"\n",
        "\n",
        "def generate_prompt_text(system_msg, matrices, executed_steps, m_vars, c_matrix_computed_values):\n",
        "    prompt = f\"{system_msg}\\n\\n\"\n",
        "    prompt += \"Current algorithm state:\\n\"\n",
        "    prompt += \"Matrices:\\n\"\n",
        "    prompt += format_matrix_for_prompt(\"A\", matrices['A']) + \"\\n\"\n",
        "    prompt += format_matrix_for_prompt(\"B\", matrices['B']) + \"\\n\"\n",
        "    prompt += \"Executed Steps:\\n\"\n",
        "    if not executed_steps:\n",
        "        prompt += \"None yet.\\n\"\n",
        "    else:\n",
        "        for step in executed_steps:\n",
        "            prompt += step + \"\\n\"\n",
        "    prompt += \"Intermediate Variables (M values):\\n\"\n",
        "    if not m_vars:\n",
        "        prompt += \"{}\\n\"\n",
        "    else:\n",
        "        m_vars_str = \", \".join([f\"{k}: {v_tuple[1]}\" for k, v_tuple in m_vars.items()]) # v_tuple is (value, expression_str)\n",
        "        prompt += \"{\" + m_vars_str + \"}\\n\"\n",
        "    prompt += format_c_matrix_for_prompt(c_matrix_computed_values) + \"\\n\"\n",
        "    prompt += \"Provide the NEXT SINGLE DSL step according to the rules:\"\n",
        "    return prompt\n",
        "\n",
        "def generate_standard_matmul_steps_for_sft(num_example_sets=3, start_m_index=0):\n",
        "    sft_examples = []\n",
        "    current_m_id = start_m_index\n",
        "\n",
        "    # Define a few A and B matrices to ensure variety if we run multiple sets\n",
        "    matrix_variations = [\n",
        "        ({'A': [[2, 3], [1, 4]], 'B': [[5, 1], [2, 6]]}, \"Set1\"),\n",
        "        ({'A': [[-1, 0], [2, 1]], 'B': [[4, 2], [-3, 5]]}, \"Set2\"),\n",
        "        ({'A': [[1, 1], [1, 1]], 'B': [[2, 2], [2, 2]]}, \"Set3\"),\n",
        "        ({'A': [[7, -2], [0, 9]], 'B': [[1, 0], [0, 1]]}, \"Set4\") # Identity B\n",
        "    ]\n",
        "\n",
        "    for i in range(num_example_sets):\n",
        "        matrices = matrix_variations[i % len(matrix_variations)][0]\n",
        "        # These are symbolic, actual values don't matter for format training, but help in state tracking\n",
        "        A_vals = matrices['A']\n",
        "        B_vals = matrices['B']\n",
        "\n",
        "        executed_steps_for_set = []\n",
        "        m_vars_for_set = {} # Stores { \"M0\": (value, \"A[0,0]*B[0,0]\"), ... }\n",
        "        # Stores { \"C[0,0]\": (value, \"M2\"), ... }\n",
        "        c_matrix_computed_values_for_set = {f\"C[{r},{c}]\": None for r in range(2) for c in range(2)}\n",
        "\n",
        "        # C[0,0] = A[0,0]*B[0,0] + A[0,1]*B[1,0]\n",
        "        steps_for_c00 = [\n",
        "            f\"M{current_m_id} = A[0,0] * B[0,0]\",\n",
        "            f\"M{current_m_id+1} = A[0,1] * B[1,0]\",\n",
        "            f\"M{current_m_id+2} = M{current_m_id} + M{current_m_id+1}\",\n",
        "            f\"C[0,0] = M{current_m_id+2}\"\n",
        "        ]\n",
        "        m_expr_c00 = [\n",
        "            (\"A[0,0] * B[0,0]\", (A_vals[0][0] * B_vals[0][0])),\n",
        "            (\"A[0,1] * B[1,0]\", (A_vals[0][1] * B_vals[1][0])),\n",
        "            (f\"M{current_m_id} + M{current_m_id+1}\", m_vars_for_set.get(f\"M{current_m_id}\",(0,\"\"))[0] + m_vars_for_set.get(f\"M{current_m_id+1}\",(0,\"\"))[0]), # Needs update after m_vars populated\n",
        "            (f\"M{current_m_id+2}\", m_vars_for_set.get(f\"M{current_m_id+2}\",(0,\"\"))[0]) # Needs update\n",
        "        ]\n",
        "\n",
        "        # C[0,1] = A[0,0]*B[0,1] + A[0,1]*B[1,1]\n",
        "        steps_for_c01 = [\n",
        "            f\"M{current_m_id+3} = A[0,0] * B[0,1]\",\n",
        "            f\"M{current_m_id+4} = A[0,1] * B[1,1]\",\n",
        "            f\"M{current_m_id+5} = M{current_m_id+3} + M{current_m_id+4}\",\n",
        "            f\"C[0,1] = M{current_m_id+5}\"\n",
        "        ]\n",
        "        m_expr_c01 = [\n",
        "            (\"A[0,0] * B[0,1]\", (A_vals[0][0] * B_vals[0][1])),\n",
        "            (\"A[0,1] * B[1,1]\", (A_vals[0][1] * B_vals[1][1])),\n",
        "            (f\"M{current_m_id+3} + M{current_m_id+4}\",0), # placeholder\n",
        "            (f\"M{current_m_id+5}\",0) # placeholder\n",
        "        ]\n",
        "\n",
        "        # C[1,0] = A[1,0]*B[0,0] + A[1,1]*B[1,0]\n",
        "        steps_for_c10 = [\n",
        "            f\"M{current_m_id+6} = A[1,0] * B[0,0]\",\n",
        "            f\"M{current_m_id+7} = A[1,1] * B[1,0]\",\n",
        "            f\"M{current_m_id+8} = M{current_m_id+6} + M{current_m_id+7}\",\n",
        "            f\"C[1,0] = M{current_m_id+8}\"\n",
        "        ]\n",
        "        m_expr_c10 = [\n",
        "            (\"A[1,0] * B[0,0]\", (A_vals[1][0] * B_vals[0][0])),\n",
        "            (\"A[1,1] * B[1,0]\", (A_vals[1][1] * B_vals[1][0])),\n",
        "            (f\"M{current_m_id+6} + M{current_m_id+7}\",0),\n",
        "            (f\"M{current_m_id+8}\",0)\n",
        "        ]\n",
        "\n",
        "        # C[1,1] = A[1,0]*B[0,1] + A[1,1]*B[1,1]\n",
        "        steps_for_c11 = [\n",
        "            f\"M{current_m_id+9} = A[1,0] * B[0,1]\",\n",
        "            f\"M{current_m_id+10} = A[1,1] * B[1,1]\",\n",
        "            f\"M{current_m_id+11} = M{current_m_id+9} + M{current_m_id+10}\",\n",
        "            f\"C[1,1] = M{current_m_id+11}\"\n",
        "        ]\n",
        "        m_expr_c11 = [\n",
        "            (\"A[1,0] * B[0,1]\", (A_vals[1][0] * B_vals[0][1])),\n",
        "            (\"A[1,1] * B[1,1]\", (A_vals[1][1] * B_vals[1][1])),\n",
        "            (f\"M{current_m_id+9} + M{current_m_id+10}\",0),\n",
        "            (f\"M{current_m_id+11}\",0)\n",
        "        ]\n",
        "\n",
        "        all_steps_for_set = steps_for_c00 + steps_for_c01 + steps_for_c10 + steps_for_c11\n",
        "        # This logic for m_expr needs to be more dynamic during step processing\n",
        "        # For simplicity in generation, we'll pre-calculate symbolic expressions for M vars\n",
        "\n",
        "        # Correctly create (prompt, completion) pairs\n",
        "        temp_m_id_counter = current_m_id # Used to track M variable names for expressions\n",
        "        for step_index, target_dsl_step in enumerate(all_steps_for_set):\n",
        "            # 1. Generate PROMPT based on current state (executed_steps_for_set, m_vars_for_set, c_matrix_computed_values_for_set)\n",
        "            prompt = generate_prompt_text(SYSTEM_MESSAGE, matrices, executed_steps_for_set, m_vars_for_set, c_matrix_computed_values_for_set)\n",
        "\n",
        "            # 2. The COMPLETION is the target_dsl_step\n",
        "            completion = target_dsl_step\n",
        "\n",
        "            # Add to SFT examples\n",
        "            # For Qwen, the format is typically ChatML. SFTTrainer can often handle a 'text' field\n",
        "            # containing the full conversation turn.\n",
        "            # We will prepare it like this:\n",
        "            # <|im_start|>system\n",
        "            # {SYSTEM_MESSAGE}<|im_end|>\n",
        "            # <|im_start|>user\n",
        "            # {USER_PROMPT_CONTENT_MINUS_SYSTEM_MESSAGE}<|im_end|>\n",
        "            # <|im_start|>assistant\n",
        "            # {DSL_LINE}<|im_end|>\n",
        "            # The user_prompt_content is what generate_prompt_text creates, MINUS the system message part.\n",
        "            user_prompt_content = generate_prompt_text(\"\", matrices, executed_steps_for_set, m_vars_for_set, c_matrix_computed_values_for_set) # Pass empty system msg for this part\n",
        "\n",
        "            # Construct the full text in ChatML format\n",
        "            chatml_text = f\"<|im_start|>system\\n{SYSTEM_MESSAGE}<|im_end|>\\n\"\n",
        "            chatml_text += f\"<|im_start|>user\\n{user_prompt_content}<|im_end|>\\n\" # user_prompt_content already has \"Current algo state...\"\n",
        "            chatml_text += f\"<|im_start|>assistant\\n{completion}<|im_end|>\"\n",
        "            sft_examples.append({\"text\": chatml_text})\n",
        "\n",
        "            # 3. UPDATE state for the NEXT iteration's prompt\n",
        "            executed_steps_for_set.append(target_dsl_step)\n",
        "            parts = target_dsl_step.split(\" = \")\n",
        "            assign_to = parts[0].strip()\n",
        "            expr_str = parts[1].strip()\n",
        "\n",
        "            if assign_to.startswith(\"M\"):\n",
        "                # Simplified: just store the expression string. Actual value computation not needed for format training.\n",
        "                m_vars_for_set[assign_to] = (None, expr_str) # (value_placeholder, expression_string)\n",
        "            elif assign_to.startswith(\"C[\"):\n",
        "                c_matrix_computed_values_for_set[assign_to] = (None, expr_str)\n",
        "\n",
        "        current_m_id += 12 # Increment for the next set of M variables\n",
        "\n",
        "    # Add a few more examples for other DSL rules if not well covered\n",
        "    # Example for M<id> = <operand>\n",
        "    if num_example_sets > 0: # Ensure some state exists\n",
        "        matrices = matrix_variations[0][0] # Use first matrix set for consistency\n",
        "        # Assume C[0,0] and M0, M1, M2 are done.\n",
        "        # Executed: M0=A[0,0]*B[0,0], M1=A[0,1]*B[1,0], M2=M0+M1, C[0,0]=M2\n",
        "        _executed_steps = [f\"M{start_m_index}=A[0,0]*B[0,0]\", f\"M{start_m_index+1}=A[0,1]*B[1,0]\", f\"M{start_m_index+2}=M{start_m_index}+M{start_m_index+1}\", f\"C[0,0]=M{start_m_index+2}\"]\n",
        "        _m_vars = {\n",
        "            f\"M{start_m_index}\": (None, \"A[0,0]*B[0,0]\"),\n",
        "            f\"M{start_m_index+1}\": (None, \"A[0,1]*B[1,0]\"),\n",
        "            f\"M{start_m_index+2}\": (None, f\"M{start_m_index}+M{start_m_index+1}\")\n",
        "        }\n",
        "        _c_vals = {f\"C[0,0]\": (None, f\"M{start_m_index+2}\"), \"C[0,1]\": None, \"C[1,0]\": None, \"C[1,1]\": None}\n",
        "\n",
        "        # 1. M = A[r,c]\n",
        "        target_dsl = f\"M{current_m_id} = A[1,1]\"\n",
        "        user_prompt_content = generate_prompt_text(\"\", matrices, _executed_steps, _m_vars, _c_vals)\n",
        "        chatml_text = f\"<|im_start|>system\\n{SYSTEM_MESSAGE}<|im_end|>\\n<|im_start|>user\\n{user_prompt_content}<|im_end|>\\n<|im_start|>assistant\\n{target_dsl}<|im_end|>\"\n",
        "        sft_examples.append({\"text\": chatml_text})\n",
        "        _executed_steps.append(target_dsl)\n",
        "        _m_vars[f\"M{current_m_id}\"] = (None, \"A[1,1]\")\n",
        "        current_m_id += 1\n",
        "\n",
        "        # 2. M = M_prev (less common but valid)\n",
        "        target_dsl = f\"M{current_m_id} = M{start_m_index+1}\"\n",
        "        user_prompt_content = generate_prompt_text(\"\", matrices, _executed_steps, _m_vars, _c_vals)\n",
        "        chatml_text = f\"<|im_start|>system\\n{SYSTEM_MESSAGE}<|im_end|>\\n<|im_start|>user\\n{user_prompt_content}<|im_end|>\\n<|im_start|>assistant\\n{target_dsl}<|im_end|>\"\n",
        "        sft_examples.append({\"text\": chatml_text})\n",
        "        _executed_steps.append(target_dsl)\n",
        "        _m_vars[f\"M{current_m_id}\"] = (None, f\"M{start_m_index+1}\")\n",
        "        current_m_id += 1\n",
        "\n",
        "        # 3. M = op1 - op2\n",
        "        target_dsl = f\"M{current_m_id} = A[0,0] - B[0,0]\"\n",
        "        user_prompt_content = generate_prompt_text(\"\", matrices, _executed_steps, _m_vars, _c_vals)\n",
        "        chatml_text = f\"<|im_start|>system\\n{SYSTEM_MESSAGE}<|im_end|>\\n<|im_start|>user\\n{user_prompt_content}<|im_end|>\\n<|im_start|>assistant\\n{target_dsl}<|im_end|>\"\n",
        "        sft_examples.append({\"text\": chatml_text})\n",
        "        # No need to update state further for this example generator\n",
        "\n",
        "    print(f\"Generated {len(sft_examples)} SFT examples.\")\n",
        "    return sft_examples\n",
        "\n",
        "# Generate the dataset (aim for ~50, 16 per set, so 3 sets = 48 + a few more)\n",
        "# 3 sets * 16 steps/set = 48 examples. Plus the 3 manual ones = 51.\n",
        "fine_tuning_data = generate_standard_matmul_steps_for_sft(num_example_sets=3)\n",
        "\n",
        "# Save to a JSONL file for inspection or use by Hugging Face datasets\n",
        "output_jsonl_path = \"dsl_finetune_data.jsonl\"\n",
        "with open(output_jsonl_path, 'w') as f:\n",
        "    for item in fine_tuning_data:\n",
        "        f.write(json.dumps(item) + \"\\n\")\n",
        "\n",
        "print(f\"Fine-tuning data saved to {output_jsonl_path}\")\n",
        "print(\"First example:\")\n",
        "print(json.dumps(fine_tuning_data[0], indent=2))\n",
        "print(\"\\nLast example:\")\n",
        "print(json.dumps(fine_tuning_data[-20], indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LEmlxHXd294",
        "outputId": "8e3813a4-fef1-483c-cf27-aa16782e87ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated 51 SFT examples.\n",
            "Fine-tuning data saved to dsl_finetune_data.jsonl\n",
            "First example:\n",
            "{\n",
            "  \"text\": \"<|im_start|>system\\nASSISTANT'S ONLY TASK: Output ONE SINGLE VALID DSL line for 2x2 matrix multiplication (C = A * B).\\nNO PYTHON. NO EXPLANATIONS. ONLY THE DSL LINE.\\nIf stuck, output ONLY: Error: Cannot determine next step.<|im_end|>\\n<|im_start|>user\\n\\n\\nCurrent algorithm state:\\nMatrices:\\nA = [[2, 3], [1, 4]]\\nB = [[5, 1], [2, 6]]\\nExecuted Steps:\\nNone yet.\\nIntermediate Variables (M values):\\n{}\\nOutput Matrix C (nan means not yet computed, computed means a DSL line has assigned to it):\\n[[nan, nan],\\n [nan, nan]]\\nProvide the NEXT SINGLE DSL step according to the rules:<|im_end|>\\n<|im_start|>assistant\\nM0 = A[0,0] * B[0,0]<|im_end|>\"\n",
            "}\n",
            "\n",
            "Last example:\n",
            "{\n",
            "  \"text\": \"<|im_start|>system\\nASSISTANT'S ONLY TASK: Output ONE SINGLE VALID DSL line for 2x2 matrix multiplication (C = A * B).\\nNO PYTHON. NO EXPLANATIONS. ONLY THE DSL LINE.\\nIf stuck, output ONLY: Error: Cannot determine next step.<|im_end|>\\n<|im_start|>user\\n\\n\\nCurrent algorithm state:\\nMatrices:\\nA = [[-1, 0], [2, 1]]\\nB = [[4, 2], [-3, 5]]\\nExecuted Steps:\\nM12 = A[0,0] * B[0,0]\\nM13 = A[0,1] * B[1,0]\\nM14 = M12 + M13\\nC[0,0] = M14\\nM15 = A[0,0] * B[0,1]\\nM16 = A[0,1] * B[1,1]\\nM17 = M15 + M16\\nC[0,1] = M17\\nM18 = A[1,0] * B[0,0]\\nM19 = A[1,1] * B[1,0]\\nM20 = M18 + M19\\nC[1,0] = M20\\nM21 = A[1,0] * B[0,1]\\nM22 = A[1,1] * B[1,1]\\nM23 = M21 + M22\\nIntermediate Variables (M values):\\n{M12: A[0,0] * B[0,0], M13: A[0,1] * B[1,0], M14: M12 + M13, M15: A[0,0] * B[0,1], M16: A[0,1] * B[1,1], M17: M15 + M16, M18: A[1,0] * B[0,0], M19: A[1,1] * B[1,0], M20: M18 + M19, M21: A[1,0] * B[0,1], M22: A[1,1] * B[1,1], M23: M21 + M22}\\nOutput Matrix C (nan means not yet computed, computed means a DSL line has assigned to it):\\n[[computed, computed],\\n [computed, nan]]\\nProvide the NEXT SINGLE DSL step according to the rules:<|im_end|>\\n<|im_start|>assistant\\nC[1,1] = M23<|im_end|>\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install trl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yM-K011fap5H",
        "outputId": "d18eb1af-50d1-478a-9528-575d678e4a14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting trl\n",
            "  Using cached trl-0.17.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: accelerate>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from trl) (1.6.0)\n",
            "Collecting datasets>=3.0.0 (from trl)\n",
            "  Using cached datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl) (13.9.4)\n",
            "Requirement already satisfied: transformers>=4.46.0 in /usr/local/lib/python3.11/dist-packages (from trl) (4.51.3)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (2.6.0+cu124)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (0.31.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (0.70.15)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl)\n",
            "  Using cached fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.46.0->trl) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.46.0->trl) (0.21.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl) (2.19.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate>=0.34.0->trl) (4.13.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2025.4.26)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->accelerate>=0.34.0->trl)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->accelerate>=0.34.0->trl)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->accelerate>=0.34.0->trl)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate>=0.34.0->trl)\n",
            "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate>=0.34.0->trl)\n",
            "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate>=0.34.0->trl)\n",
            "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate>=0.34.0->trl)\n",
            "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate>=0.34.0->trl)\n",
            "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate>=0.34.0->trl)\n",
            "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate>=0.34.0->trl)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate>=0.34.0->trl) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate>=0.34.0->trl) (3.0.2)\n",
            "Downloading trl-0.17.0-py3-none-any.whl (348 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m348.0/348.0 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "Using cached fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, datasets, trl\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 2.14.4\n",
            "    Uninstalling datasets-2.14.4:\n",
            "      Successfully uninstalled datasets-2.14.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.6.0 fsspec-2025.3.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 trl-0.17.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "datasets",
                  "nvidia"
                ]
              },
              "id": "10ea836139c94cae8655b95d27d1c974"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install peft==0.13.2 transformers==4.41.1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzDDcq6vclvz",
        "outputId": "716d764f-1f15-4903-ce25-1436549fa96a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: peft==0.13.2 in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: transformers==4.41.1 in /usr/local/lib/python3.11/dist-packages (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft==0.13.2) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.13.2) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft==0.13.2) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft==0.13.2) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.13.2) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft==0.13.2) (4.67.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.13.2) (1.6.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft==0.13.2) (0.5.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.13.2) (0.31.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.1) (3.18.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.1) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.1) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.1) (0.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.17.0->peft==0.13.2) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.17.0->peft==0.13.2) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.13.2) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.13.2) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.13.2) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.13.2) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.13.2) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.13.2) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.13.2) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.13.2) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.13.2) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.13.2) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.13.2) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.13.2) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.13.2) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.13.2) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.13.2) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.13.2) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.13.2) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft==0.13.2) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.1) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.1) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.1) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft==0.13.2) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n"
      ],
      "metadata": {
        "id": "mckt40J-cw-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== 1. Installs =====\n",
        "# !pip install -q transformers==4.38.2 datasets peft accelerate bitsandbytes trl\n",
        "\n",
        "# ===== 2. Imports & Drive Mount =====\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "from google.colab import drive\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    DataCollatorForLanguageModeling,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "# Mount Google Drive for saving\n",
        "drive.mount('/content/drive')\n",
        "DRIVE_BASE_PATH = \"/content/drive/MyDrive/Qwen_DSL_FineTune\"\n",
        "os.makedirs(DRIVE_BASE_PATH, exist_ok=True)\n",
        "\n",
        "# ===== 3. Prepare or Upload Your JSONL Dataset =====\n",
        "if not os.path.exists(\"dsl_finetune_data.jsonl\"):\n",
        "    def _gen_dummy():\n",
        "        sys_msg = (\n",
        "            \"ASSISTANT'S ONLY TASK: Output ONE SINGLE VALID DSL line for 2x2 matrix \"\n",
        "            \"multiplication. NO PYTHON. NO EXPLANATIONS. ONLY THE DSL LINE.\"\n",
        "        )\n",
        "        examples = []\n",
        "        # Example 1\n",
        "        user1 = (\n",
        "            \"Current algorithm state:\\n\"\n",
        "            \"Matrices:\\nA = [[2,3],[1,4]] B = [[5,1],[2,6]]\\n\"\n",
        "            \"Executed Steps: None yet.\\n\"\n",
        "            \"Output C: [[nan,nan],[nan,nan]]\\n\"\n",
        "            \"Provide the NEXT SINGLE DSL step:\"\n",
        "        )\n",
        "        asst1 = \"M0 = A[0,0] * B[0,0]\"\n",
        "        text1 = (\n",
        "            f\"<|im_start|>system\\n{sys_msg}<|im_end|>\\n\"\n",
        "            f\"<|im_start|>user\\n{user1}<|im_end|>\\n\"\n",
        "            f\"<|im_start|>assistant\\n{asst1}<|im_end|>\"\n",
        "        )\n",
        "        examples.append({\"text\": text1})\n",
        "        # Example 2\n",
        "        user2 = (\n",
        "            \"Current algorithm state:\\n\"\n",
        "            \"Matrices:\\nA = [[-1,0],[2,1]] B = [[4,2],[-3,5]]\\n\"\n",
        "            \"Executed Steps:\\n\"\n",
        "            \"M0 = A[0,0]*B[0,1]\\nM1 = A[0,0]*B[0,0]\\n\"\n",
        "            \"M2 = A[0,1]*B[1,0]\\nM3 = A[0,1]*B[1,1]\\n\"\n",
        "            \"M4 = M1+M2\\nM5 = M0+M3\\n\"\n",
        "            \"Output C: [[nan,nan],[nan,nan]]\\n\"\n",
        "            \"Provide the NEXT SINGLE DSL step:\"\n",
        "        )\n",
        "        asst2 = \"C[0,1] = M5\"\n",
        "        text2 = (\n",
        "            f\"<|im_start|>system\\n{sys_msg}<|im_end|>\\n\"\n",
        "            f\"<|im_start|>user\\n{user2}<|im_end|>\\n\"\n",
        "            f\"<|im_start|>assistant\\n{asst2}<|im_end|>\"\n",
        "        )\n",
        "        examples.append({\"text\": text2})\n",
        "        with open(\"dsl_finetune_data.jsonl\", \"w\") as f:\n",
        "            for ex in examples:\n",
        "                f.write(json.dumps(ex) + \"\\n\")\n",
        "        print(\"Dummy dsl_finetune_data.jsonl created.\")\n",
        "    _gen_dummy()\n",
        "\n",
        "# ===== 4. Load Dataset =====\n",
        "dataset = load_dataset(\"json\", data_files=\"dsl_finetune_data.jsonl\", split=\"train\")\n",
        "print(f\"Loaded {len(dataset)} examples, e.g.:\")\n",
        "print(dataset[0][\"text\"])\n",
        "\n",
        "# ===== 5. Model & Tokenizer Setup =====\n",
        "MODEL_NAME = \"Qwen/Qwen1.5-0.5B-Chat\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "model = prepare_model_for_kbit_training(base_model)\n",
        "\n",
        "# ===== 6. PEFT LoRA Config =====\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
        "    ],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# ===== 7. SFTConfig for Trainer =====\n",
        "sft_config = SFTConfig(\n",
        "    output_dir=os.path.join(DRIVE_BASE_PATH, \"qwen_dsl_adapter\"),\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=2,\n",
        "    learning_rate=2e-4,\n",
        "    num_train_epochs=3,\n",
        "    bf16=True,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    report_to=\"none\",\n",
        "    max_seq_length=512,\n",
        "    dataset_text_field=\"text\",\n",
        "    packing=False,\n",
        ")\n",
        "\n",
        "# ===== 8. Causal-LM Data Collator =====\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,\n",
        "    return_special_tokens_mask=False,\n",
        ")\n",
        "\n",
        "# ===== 9. Initialize SFTTrainer =====\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    peft_config=lora_config,\n",
        "    args=sft_config,\n",
        "    processing_class=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# ===== 10. Fine-tune =====\n",
        "print(\"🚀 Starting fine-tuning...\")\n",
        "trainer.train()\n",
        "print(\"🎉 Fine-tuning complete!\")\n",
        "\n",
        "# ===== 11. Save LoRA Adapter & Tokenizer =====\n",
        "final_adapter_path = os.path.join(DRIVE_BASE_PATH, \"qwen_dsl_final_adapter\")\n",
        "trainer.save_model(final_adapter_path)\n",
        "tokenizer.save_pretrained(final_adapter_path)\n",
        "print(f\"LoRA adapter & tokenizer saved to: {final_adapter_path}\")\n",
        "\n",
        "# ===== 12. (Optional) Quick Test =====\n",
        "from peft import PeftModel\n",
        "\n",
        "base = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "ft_model = PeftModel.from_pretrained(base, final_adapter_path)\n",
        "ft_model.eval()\n",
        "\n",
        "system_msg = (\n",
        "    \"ASSISTANT'S ONLY TASK: Output ONE SINGLE VALID DSL line for 2x2 matrix multiplication.\"\n",
        ")\n",
        "user_prompt = (\n",
        "    \"Current algorithm state:\\n\"\n",
        "    \"A=[[2,3],[1,4]] B=[[5,1],[2,6]]\\n\"\n",
        "    \"Executed Steps: None.\\n\"\n",
        "    \"Output C: [[nan,nan],[nan,nan]]\\n\"\n",
        "    \"Provide the NEXT SINGLE DSL step:\"\n",
        ")\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system_msg},\n",
        "    {\"role\": \"user\",   \"content\": user_prompt},\n",
        "]\n",
        "input_text = tokenizer.apply_chat_template(\n",
        "    messages, tokenize=False, add_generation_prompt=True\n",
        ")\n",
        "inputs = tokenizer([input_text], return_tensors=\"pt\").to(ft_model.device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    out = ft_model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=25,\n",
        "        do_sample=False,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "generated = tokenizer.decode(\n",
        "    out[0][inputs[\"input_ids\"].shape[-1]:],\n",
        "    skip_special_tokens=True\n",
        ").strip()\n",
        "print(\"Generated DSL step:\", generated)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "nLEHh2DHjM4B",
        "outputId": "3701c5d1-41ed-4349-dd81-2d6007fcf222"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Failed to import trl.trainer.sft_trainer because of the following error (look up to see its traceback):\ncannot import name 'BaseImageProcessor' from 'transformers' (/usr/local/lib/python3.11/dist-packages/transformers/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/trl/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhuggingface_hub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_deprecation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_deprecate_arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m from transformers import (\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'BaseImageProcessor' from 'transformers' (/usr/local/lib/python3.11/dist-packages/transformers/__init__.py)",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-ae2ca7de64f7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpeft\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLoraConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_peft_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprepare_model_for_kbit_training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtrl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSFTTrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSFTConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Mount Google Drive for saving\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/trl/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"module {self.__name__} has no attribute {name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/trl/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/trl/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m    103\u001b[0m                 \u001b[0;34mf\"Failed to import {self.__name__}.{module_name} because of the following error (look up to see its\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0;34mf\" traceback):\\n{e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Failed to import trl.trainer.sft_trainer because of the following error (look up to see its traceback):\ncannot import name 'BaseImageProcessor' from 'transformers' (/usr/local/lib/python3.11/dist-packages/transformers/__init__.py)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade \"transformers>=4.42.0\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "lpqOMutvgBs7",
        "outputId": "551ba6dd-afd8-4aa2-d3ea-4d29c2defaad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers>=4.42.0 in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Collecting transformers>=4.42.0\n",
            "  Downloading transformers-4.52.2-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.2/40.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers>=4.42.0) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.42.0) (0.31.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.42.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.42.0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.42.0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.42.0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.42.0) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.42.0) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.42.0) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.42.0) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=4.42.0) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=4.42.0) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.42.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.42.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.42.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.42.0) (2025.4.26)\n",
            "Downloading transformers-4.52.2-py3-none-any.whl (10.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m133.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.51.3\n",
            "    Uninstalling transformers-4.51.3:\n",
            "      Successfully uninstalled transformers-4.51.3\n",
            "Successfully installed transformers-4.52.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "transformers"
                ]
              },
              "id": "996f675af5c145ce8717bc1254bc72dc"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================\n",
        "# QWEN DSL Fine-Tuning Full Script\n",
        "# ==============================================\n",
        "# Dependencies (install in your Colab / environment):\n",
        "# !pip install -q transformers==4.38.2 datasets peft accelerate bitsandbytes trl\n",
        "\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM, AutoTokenizer,\n",
        "    BitsAndBytesConfig, TrainingArguments\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 1. (Optional) Mount Google Drive\n",
        "# ------------------------------------------------\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# DRIVE_BASE_PATH = \"/content/drive/MyDrive/Qwen_DSL_FineTune\"\n",
        "# os.makedirs(DRIVE_BASE_PATH, exist_ok=True)\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 2. System Message and Dummy Dataset Generation\n",
        "# ------------------------------------------------\n",
        "SYSTEM_MESSAGE = (\n",
        "    \"ASSISTANT'S ONLY TASK: Output ONE SINGLE VALID DSL line for 2x2 matrix multiplication (C = A * B).\"\n",
        "    \"\\nNO PYTHON. NO EXPLANATIONS. ONLY THE DSL LINE.\"\n",
        "    \"\\nIf stuck, output ONLY: Error: Cannot determine next step.\"\n",
        ")\n",
        "\n",
        "# Create minimal dummy data if JSONL not present\n",
        "dataset_path = \"dsl_finetune_data.jsonl\"\n",
        "if not os.path.exists(dataset_path):\n",
        "    def _generate_dummy():\n",
        "        examples = []\n",
        "        def make_item(user, assistant):\n",
        "            chatml = (\n",
        "                f\"<|im_start|>system\\n{SYSTEM_MESSAGE}<|im_end|>\"\n",
        "                f\"\\n<|im_start|>user\\n{user}<|im_end|>\"\n",
        "                f\"\\n<|im_start|>assistant\\n{assistant}<|im_end|>\"\n",
        "            )\n",
        "            return {\"text\": chatml}\n",
        "\n",
        "        # Example 1\n",
        "        u1 = (\n",
        "            \"Current algorithm state:\\n\"\n",
        "            \"Matrices:\\n\"\n",
        "            \"A = [[2, 3], [1, 4]]\\n\"\n",
        "            \"B = [[5, 1], [2, 6]]\\n\"\n",
        "            \"Executed Steps:\\nNone yet.\\n\"\n",
        "            \"Intermediate Variables (M values): {}\\n\"\n",
        "            \"Output Matrix C (NaN means not yet computed):\\n\"\n",
        "            \"[[nan, nan],\\n [nan, nan]]\\n\"\n",
        "            \"Provide the NEXT SINGLE DSL step according to the rules:\"\n",
        "        )\n",
        "        a1 = \"M0 = A[0,0] * B[0,0]\"\n",
        "        examples.append(make_item(u1, a1))\n",
        "\n",
        "        # Example 2\n",
        "        u2 = (\n",
        "            \"Current algorithm state:\\n\"\n",
        "            \"Matrices:\\n\"\n",
        "            \"A = [[-1, 0], [2, 1]]\\n\"\n",
        "            \"B = [[4, 2], [-3, 5]]\\n\"\n",
        "            \"Executed Steps:\\n\"\n",
        "            \"M0 = A[0,0] * B[0,1]\\nM1 = A[0,0] * B[0,0]\\n\"\n",
        "            \"M2 = A[0,1] * B[1,0]\\nM3 = A[0,1] * B[1,1]\\n\"\n",
        "            \"M4 = M1 + M2\\nM5 = M0 + M3\\n\"\n",
        "            \"Intermediate Variables (M values): {M0: A[0,0]*B[0,1], M1: A[0,0]*B[0,0], M2: A[0,1]*B[1,0], M3: A[0,1]*B[1,1], M4: M1+M2, M5: M0+M3}\\n\"\n",
        "            \"Output Matrix C (NaN means not yet computed):\\n\"\n",
        "            \"[[nan, nan],\\n [nan, nan]]\\n\"\n",
        "            \"Provide the NEXT SINGLE DSL step according to the rules:\"\n",
        "        )\n",
        "        a2 = \"C[0,1] = M5\"\n",
        "        examples.append(make_item(u2, a2))\n",
        "\n",
        "        with open(dataset_path, 'w') as f:\n",
        "            for ex in examples:\n",
        "                f.write(json.dumps(ex) + \"\\n\")\n",
        "        print(f\"Dummy dataset created at {dataset_path}\")\n",
        "\n",
        "    _generate_dummy()\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 3. Load Dataset\n",
        "# ------------------------------------------------\n",
        "dataset = load_dataset(\"json\", data_files=dataset_path, split=\"train\")\n",
        "print(f\"Loaded {len(dataset)} examples from {dataset_path}\")\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 4. Model & Tokenizer Setup\n",
        "# ------------------------------------------------\n",
        "model_name = \"Qwen/Qwen1.5-0.5B-Chat\"\n",
        "\n",
        "# Quantization config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# Model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 5. LoRA Configuration & PEFT Wrapping\n",
        "# ------------------------------------------------\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 6. Sanity Check: Ensure target_modules exist\n",
        "# ------------------------------------------------\n",
        "print(\"\\nValidating target module names:\")\n",
        "targets = lora_config.target_modules\n",
        "leafs = {name.split('.')[-1] for name, _ in model.named_modules()}\n",
        "for t in targets:\n",
        "    print(f\"{t:10s} → {('FOUND' if t in leafs else 'MISSING')}\" )\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 7. Training Arguments & SFTTrainer\n",
        "# ------------------------------------------------\n",
        "output_dir = \"./qwen_dsl_finetuned_adapter\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=2,\n",
        "    learning_rate=2e-4,\n",
        "    num_train_epochs=3,\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    fp16=False,\n",
        "    bf16=True,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    report_to=\"none\",\n",
        "\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    peft_config=lora_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    # max_seq_length=training_args.max_seq_length,\n",
        "    max_seq_length=512,\n",
        ")\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 8. Start Fine-Tuning\n",
        "# ------------------------------------------------\n",
        "print(\"Starting fine-tuning...\")\n",
        "trainer.train()\n",
        "print(\"Fine-tuning completed.\")\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 9. Save LoRA Adapter & Tokenizer\n",
        "# ------------------------------------------------\n",
        "adapter_path = os.path.join(output_dir, \"final_adapter\")\n",
        "trainer.save_model(adapter_path)\n",
        "tokenizer.save_pretrained(adapter_path)\n",
        "print(f\"Adapter and tokenizer saved at {adapter_path}\")\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 10. (Optional) Test the Fine-Tuned Model\n",
        "# ------------------------------------------------\n",
        "def test_model(prompt: str):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_MESSAGE},\n",
        "        {\"role\": \"user\",   \"content\": prompt},\n",
        "    ]\n",
        "    text_in = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tokenizer([text_in], return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=25,\n",
        "            do_sample=False,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    gen = out[0][inputs[\"input_ids\"].shape[-1]:]\n",
        "    return tokenizer.decode(gen, skip_special_tokens=True).strip()\n",
        "\n",
        "# Example tests\n",
        "prompt1 = (\n",
        "    \"Current algorithm state:\\nMatrices:\\nA = [[2, 3], [1, 4]]\\n\"\n",
        "    \"B = [[5, 1], [2, 6]]\\nExecuted Steps:\\nNone yet.\\n\"\n",
        "    \"Intermediate Variables: {}\\nOutput Matrix C ([[nan,nan],[nan,nan]])\\n\"\n",
        "    \"Provide the NEXT SINGLE DSL step.\"\n",
        ")\n",
        "print(\"Test 1 response:\", test_model(prompt1))\n",
        "\n",
        "prompt2 = (\n",
        "    \"Current algorithm state:\\nMatrices:\\nA = [[-1, 0], [2, 1]]\\n\"\n",
        "    \"B = [[4, 2], [-3, 5]]\\nExecuted Steps:\\n\"\n",
        "    \"M0 = A[0,0]*B[0,1]\\nM1 = A[0,0]*B[0,0]\\n\"\n",
        "    \"M2 = A[0,1]*B[1,0]\\nM3 = A[0,1]*B[1,1]\\n\"\n",
        "    \"M4 = M1+M2\\nM5 = M0+M3\\n\"\n",
        "    \"Intermediate Variables: {...}\\nOutput C ([[nan,nan],[nan,nan]])\\n\"\n",
        "    \"Provide the NEXT SINGLE DSL step.\"\n",
        ")\n",
        "print(\"Test 2 response:\", test_model(prompt2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451,
          "referenced_widgets": [
            "3c9a75ec91784893a2266da372a7fdae",
            "62dfc978f3d84404b8fc1ca5c50c5546",
            "6b47ae5b03434bd9856cbb564adfc308",
            "d1037115b07c4bdb977f29569ef8bd1c",
            "a7f81cd0beae49f78a6d7429382ac765",
            "223a0cc24c8345bdb4162a6a9aefc00f",
            "9ba404b7a9fc4dae92e0270210761c78",
            "1ec2baa0334b48c9bdf455885d0b8a79",
            "31d6dfa482cd4f32a9473953520eb649",
            "d6fdfbdd3fdd40f89d6db79edc8b6eb4",
            "ab8b9e860e03493fad87b5249bbaa6a8"
          ]
        },
        "id": "F2VPGEukvncZ",
        "outputId": "88445949-d06a-4d5d-b600-eca70070b54e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dummy dataset created at dsl_finetune_data.jsonl\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3c9a75ec91784893a2266da372a7fdae"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 2 examples from dsl_finetune_data.jsonl\n",
            "trainable params: 7,569,408 || all params: 471,557,120 || trainable%: 1.6052\n",
            "\n",
            "Validating target module names:\n",
            "up_proj    → FOUND\n",
            "v_proj     → FOUND\n",
            "k_proj     → FOUND\n",
            "gate_proj  → FOUND\n",
            "o_proj     → FOUND\n",
            "down_proj  → FOUND\n",
            "q_proj     → FOUND\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "SFTTrainer.__init__() got an unexpected keyword argument 'tokenizer'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-976ac7e73ed4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    163\u001b[0m )\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m trainer = SFTTrainer(\n\u001b[0m\u001b[1;32m    166\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: SFTTrainer.__init__() got an unexpected keyword argument 'tokenizer'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt1 = (\n",
        "    \"Current algorithm state:\\nMatrices:\\nA = [[2, 3], [1, 4]]\\n\"\n",
        "    \"B = [[5, 1], [2, 6]]\\nExecuted Steps:\\nNone yet.\\n\"\n",
        "    \"Intermediate Variables: {}\\nOutput Matrix C ([[nan,nan],[nan,nan]])\\n\"\n",
        "    \"Provide the NEXT SINGLE DSL step.\"\n",
        ")\n",
        "print(\"Test 1 response:\", test_model(prompt1))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDe6eFOvhXAB",
        "outputId": "b5fb7689-e82f-4fab-8b1d-617c278cb1de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test 1 response: Step 1: Perform matrix multiplication C = A * B.\n",
            "Step 2: Calculate the result of the product C.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import json\n",
        "\n",
        "# --- System Message and DSL Definition (for our reference and inclusion in prompts) ---\n",
        "SYSTEM_MESSAGE = \"\"\"ASSISTANT'S ONLY TASK: Output ONE SINGLE VALID DSL line for 2x2 matrix multiplication (C = A * B).\n",
        "NO PYTHON. NO EXPLANATIONS. ONLY THE DSL LINE.\n",
        "If stuck, output ONLY: Error: Cannot determine next step.\"\"\"\n",
        "\n",
        "DSL_FORMAT_RULES = \"\"\"DSL FORMAT:\n",
        "M<id> = <op1> + <op2>\n",
        "M<id> = <op1> - <op2>\n",
        "M<id> = <op1> * <op2>\n",
        "M<id> = <operand>\n",
        "C[r,c] = <operand>\n",
        "(Operands: A[r,c], B[r,c], or M<id>)\"\"\"\n",
        "\n",
        "# --- Dataset Generation ---\n",
        "\n",
        "def format_matrix_for_prompt(matrix_name, matrix_values):\n",
        "    return f\"{matrix_name} = {matrix_values}\"\n",
        "\n",
        "def format_c_matrix_for_prompt(c_matrix_computed_values):\n",
        "    c_display = [[\"nan\" for _ in range(2)] for _ in range(2)]\n",
        "    for key, val_expr_tuple in c_matrix_computed_values.items(): # val_expr_tuple is (value, expression_str)\n",
        "        # key is like \"C[0,0]\"\n",
        "        r, c = int(key[2]), int(key[4])\n",
        "        # For display, we just show if it's computed, not the value itself in the prompt C matrix\n",
        "        if val_expr_tuple is not None: # if it has an expression, it's \"computed\"\n",
        "             c_display[r][c] = \"computed\" # Or we could show the M-var, e.g., C[0,0] = M2\n",
        "    return \"Output Matrix C (nan means not yet computed, computed means a DSL line has assigned to it):\\n\" + \\\n",
        "           f\"[[{c_display[0][0]}, {c_display[0][1]}],\\n\" + \\\n",
        "           f\" [{c_display[1][0]}, {c_display[1][1]}]]\"\n",
        "\n",
        "def generate_prompt_text(system_msg, matrices, executed_steps, m_vars, c_matrix_computed_values):\n",
        "    prompt = f\"{system_msg}\\n\\n\"\n",
        "    prompt += \"Current algorithm state:\\n\"\n",
        "    prompt += \"Matrices:\\n\"\n",
        "    prompt += format_matrix_for_prompt(\"A\", matrices['A']) + \"\\n\"\n",
        "    prompt += format_matrix_for_prompt(\"B\", matrices['B']) + \"\\n\"\n",
        "    prompt += \"Executed Steps:\\n\"\n",
        "    if not executed_steps:\n",
        "        prompt += \"None yet.\\n\"\n",
        "    else:\n",
        "        for step in executed_steps:\n",
        "            prompt += step + \"\\n\"\n",
        "    prompt += \"Intermediate Variables (M values):\\n\"\n",
        "    if not m_vars:\n",
        "        prompt += \"{}\\n\"\n",
        "    else:\n",
        "        m_vars_str = \", \".join([f\"{k}: {v_tuple[1]}\" for k, v_tuple in m_vars.items()]) # v_tuple is (value, expression_str)\n",
        "        prompt += \"{\" + m_vars_str + \"}\\n\"\n",
        "    prompt += format_c_matrix_for_prompt(c_matrix_computed_values) + \"\\n\"\n",
        "    prompt += \"Provide the NEXT SINGLE DSL step according to the rules:\"\n",
        "    return prompt\n",
        "\n",
        "def generate_standard_matmul_steps_for_sft(num_example_sets=3, start_m_index=0):\n",
        "    sft_examples = []\n",
        "    current_m_id = start_m_index\n",
        "\n",
        "    # Define a few A and B matrices to ensure variety if we run multiple sets\n",
        "    matrix_variations = [\n",
        "        ({'A': [[2, 3], [1, 4]], 'B': [[5, 1], [2, 6]]}, \"Set1\"),\n",
        "        ({'A': [[-1, 0], [2, 1]], 'B': [[4, 2], [-3, 5]]}, \"Set2\"),\n",
        "        ({'A': [[1, 1], [1, 1]], 'B': [[2, 2], [2, 2]]}, \"Set3\"),\n",
        "        ({'A': [[7, -2], [0, 9]], 'B': [[1, 0], [0, 1]]}, \"Set4\") # Identity B\n",
        "    ]\n",
        "\n",
        "    for i in range(num_example_sets):\n",
        "        matrices = matrix_variations[i % len(matrix_variations)][0]\n",
        "        # These are symbolic, actual values don't matter for format training, but help in state tracking\n",
        "        A_vals = matrices['A']\n",
        "        B_vals = matrices['B']\n",
        "\n",
        "        executed_steps_for_set = []\n",
        "        m_vars_for_set = {} # Stores { \"M0\": (value, \"A[0,0]*B[0,0]\"), ... }\n",
        "        # Stores { \"C[0,0]\": (value, \"M2\"), ... }\n",
        "        c_matrix_computed_values_for_set = {f\"C[{r},{c}]\": None for r in range(2) for c in range(2)}\n",
        "\n",
        "        # C[0,0] = A[0,0]*B[0,0] + A[0,1]*B[1,0]\n",
        "        steps_for_c00 = [\n",
        "            f\"M{current_m_id} = A[0,0] * B[0,0]\",\n",
        "            f\"M{current_m_id+1} = A[0,1] * B[1,0]\",\n",
        "            f\"M{current_m_id+2} = M{current_m_id} + M{current_m_id+1}\",\n",
        "            f\"C[0,0] = M{current_m_id+2}\"\n",
        "        ]\n",
        "        m_expr_c00 = [\n",
        "            (\"A[0,0] * B[0,0]\", (A_vals[0][0] * B_vals[0][0])),\n",
        "            (\"A[0,1] * B[1,0]\", (A_vals[0][1] * B_vals[1][0])),\n",
        "            (f\"M{current_m_id} + M{current_m_id+1}\", m_vars_for_set.get(f\"M{current_m_id}\",(0,\"\"))[0] + m_vars_for_set.get(f\"M{current_m_id+1}\",(0,\"\"))[0]), # Needs update after m_vars populated\n",
        "            (f\"M{current_m_id+2}\", m_vars_for_set.get(f\"M{current_m_id+2}\",(0,\"\"))[0]) # Needs update\n",
        "        ]\n",
        "\n",
        "        # C[0,1] = A[0,0]*B[0,1] + A[0,1]*B[1,1]\n",
        "        steps_for_c01 = [\n",
        "            f\"M{current_m_id+3} = A[0,0] * B[0,1]\",\n",
        "            f\"M{current_m_id+4} = A[0,1] * B[1,1]\",\n",
        "            f\"M{current_m_id+5} = M{current_m_id+3} + M{current_m_id+4}\",\n",
        "            f\"C[0,1] = M{current_m_id+5}\"\n",
        "        ]\n",
        "        m_expr_c01 = [\n",
        "            (\"A[0,0] * B[0,1]\", (A_vals[0][0] * B_vals[0][1])),\n",
        "            (\"A[0,1] * B[1,1]\", (A_vals[0][1] * B_vals[1][1])),\n",
        "            (f\"M{current_m_id+3} + M{current_m_id+4}\",0), # placeholder\n",
        "            (f\"M{current_m_id+5}\",0) # placeholder\n",
        "        ]\n",
        "\n",
        "        # C[1,0] = A[1,0]*B[0,0] + A[1,1]*B[1,0]\n",
        "        steps_for_c10 = [\n",
        "            f\"M{current_m_id+6} = A[1,0] * B[0,0]\",\n",
        "            f\"M{current_m_id+7} = A[1,1] * B[1,0]\",\n",
        "            f\"M{current_m_id+8} = M{current_m_id+6} + M{current_m_id+7}\",\n",
        "            f\"C[1,0] = M{current_m_id+8}\"\n",
        "        ]\n",
        "        m_expr_c10 = [\n",
        "            (\"A[1,0] * B[0,0]\", (A_vals[1][0] * B_vals[0][0])),\n",
        "            (\"A[1,1] * B[1,0]\", (A_vals[1][1] * B_vals[1][0])),\n",
        "            (f\"M{current_m_id+6} + M{current_m_id+7}\",0),\n",
        "            (f\"M{current_m_id+8}\",0)\n",
        "        ]\n",
        "\n",
        "        # C[1,1] = A[1,0]*B[0,1] + A[1,1]*B[1,1]\n",
        "        steps_for_c11 = [\n",
        "            f\"M{current_m_id+9} = A[1,0] * B[0,1]\",\n",
        "            f\"M{current_m_id+10} = A[1,1] * B[1,1]\",\n",
        "            f\"M{current_m_id+11} = M{current_m_id+9} + M{current_m_id+10}\",\n",
        "            f\"C[1,1] = M{current_m_id+11}\"\n",
        "        ]\n",
        "        m_expr_c11 = [\n",
        "            (\"A[1,0] * B[0,1]\", (A_vals[1][0] * B_vals[0][1])),\n",
        "            (\"A[1,1] * B[1,1]\", (A_vals[1][1] * B_vals[1][1])),\n",
        "            (f\"M{current_m_id+9} + M{current_m_id+10}\",0),\n",
        "            (f\"M{current_m_id+11}\",0)\n",
        "        ]\n",
        "\n",
        "        all_steps_for_set = steps_for_c00 + steps_for_c01 + steps_for_c10 + steps_for_c11\n",
        "        # This logic for m_expr needs to be more dynamic during step processing\n",
        "        # For simplicity in generation, we'll pre-calculate symbolic expressions for M vars\n",
        "\n",
        "        # Correctly create (prompt, completion) pairs\n",
        "        temp_m_id_counter = current_m_id # Used to track M variable names for expressions\n",
        "        for step_index, target_dsl_step in enumerate(all_steps_for_set):\n",
        "            # 1. Generate PROMPT based on current state (executed_steps_for_set, m_vars_for_set, c_matrix_computed_values_for_set)\n",
        "            prompt = generate_prompt_text(SYSTEM_MESSAGE, matrices, executed_steps_for_set, m_vars_for_set, c_matrix_computed_values_for_set)\n",
        "\n",
        "            # 2. The COMPLETION is the target_dsl_step\n",
        "            completion = target_dsl_step\n",
        "\n",
        "            # Add to SFT examples\n",
        "            # For Qwen, the format is typically ChatML. SFTTrainer can often handle a 'text' field\n",
        "            # containing the full conversation turn.\n",
        "            # We will prepare it like this:\n",
        "            # <|im_start|>system\n",
        "            # {SYSTEM_MESSAGE}<|im_end|>\n",
        "            # <|im_start|>user\n",
        "            # {USER_PROMPT_CONTENT_MINUS_SYSTEM_MESSAGE}<|im_end|>\n",
        "            # <|im_start|>assistant\n",
        "            # {DSL_LINE}<|im_end|>\n",
        "            # The user_prompt_content is what generate_prompt_text creates, MINUS the system message part.\n",
        "            user_prompt_content = generate_prompt_text(\"\", matrices, executed_steps_for_set, m_vars_for_set, c_matrix_computed_values_for_set) # Pass empty system msg for this part\n",
        "\n",
        "            # Construct the full text in ChatML format\n",
        "            chatml_text = f\"<|im_start|>system\\n{SYSTEM_MESSAGE}<|im_end|>\\n\"\n",
        "            chatml_text += f\"<|im_start|>user\\n{user_prompt_content}<|im_end|>\\n\" # user_prompt_content already has \"Current algo state...\"\n",
        "            chatml_text += f\"<|im_start|>assistant\\n{completion}<|im_end|>\"\n",
        "            sft_examples.append({\"text\": chatml_text})\n",
        "\n",
        "            # 3. UPDATE state for the NEXT iteration's prompt\n",
        "            executed_steps_for_set.append(target_dsl_step)\n",
        "            parts = target_dsl_step.split(\" = \")\n",
        "            assign_to = parts[0].strip()\n",
        "            expr_str = parts[1].strip()\n",
        "\n",
        "            if assign_to.startswith(\"M\"):\n",
        "                # Simplified: just store the expression string. Actual value computation not needed for format training.\n",
        "                m_vars_for_set[assign_to] = (None, expr_str) # (value_placeholder, expression_string)\n",
        "            elif assign_to.startswith(\"C[\"):\n",
        "                c_matrix_computed_values_for_set[assign_to] = (None, expr_str)\n",
        "\n",
        "        current_m_id += 12 # Increment for the next set of M variables\n",
        "\n",
        "    # Add a few more examples for other DSL rules if not well covered\n",
        "    # Example for M<id> = <operand>\n",
        "    if num_example_sets > 0: # Ensure some state exists\n",
        "        matrices = matrix_variations[0][0] # Use first matrix set for consistency\n",
        "        # Assume C[0,0] and M0, M1, M2 are done.\n",
        "        # Executed: M0=A[0,0]*B[0,0], M1=A[0,1]*B[1,0], M2=M0+M1, C[0,0]=M2\n",
        "        _executed_steps = [f\"M{start_m_index}=A[0,0]*B[0,0]\", f\"M{start_m_index+1}=A[0,1]*B[1,0]\", f\"M{start_m_index+2}=M{start_m_index}+M{start_m_index+1}\", f\"C[0,0]=M{start_m_index+2}\"]\n",
        "        _m_vars = {\n",
        "            f\"M{start_m_index}\": (None, \"A[0,0]*B[0,0]\"),\n",
        "            f\"M{start_m_index+1}\": (None, \"A[0,1]*B[1,0]\"),\n",
        "            f\"M{start_m_index+2}\": (None, f\"M{start_m_index}+M{start_m_index+1}\")\n",
        "        }\n",
        "        _c_vals = {f\"C[0,0]\": (None, f\"M{start_m_index+2}\"), \"C[0,1]\": None, \"C[1,0]\": None, \"C[1,1]\": None}\n",
        "\n",
        "        # 1. M = A[r,c]\n",
        "        target_dsl = f\"M{current_m_id} = A[1,1]\"\n",
        "        user_prompt_content = generate_prompt_text(\"\", matrices, _executed_steps, _m_vars, _c_vals)\n",
        "        chatml_text = f\"<|im_start|>system\\n{SYSTEM_MESSAGE}<|im_end|>\\n<|im_start|>user\\n{user_prompt_content}<|im_end|>\\n<|im_start|>assistant\\n{target_dsl}<|im_end|>\"\n",
        "        sft_examples.append({\"text\": chatml_text})\n",
        "        _executed_steps.append(target_dsl)\n",
        "        _m_vars[f\"M{current_m_id}\"] = (None, \"A[1,1]\")\n",
        "        current_m_id += 1\n",
        "\n",
        "        # 2. M = M_prev (less common but valid)\n",
        "        target_dsl = f\"M{current_m_id} = M{start_m_index+1}\"\n",
        "        user_prompt_content = generate_prompt_text(\"\", matrices, _executed_steps, _m_vars, _c_vals)\n",
        "        chatml_text = f\"<|im_start|>system\\n{SYSTEM_MESSAGE}<|im_end|>\\n<|im_start|>user\\n{user_prompt_content}<|im_end|>\\n<|im_start|>assistant\\n{target_dsl}<|im_end|>\"\n",
        "        sft_examples.append({\"text\": chatml_text})\n",
        "        _executed_steps.append(target_dsl)\n",
        "        _m_vars[f\"M{current_m_id}\"] = (None, f\"M{start_m_index+1}\")\n",
        "        current_m_id += 1\n",
        "\n",
        "        # 3. M = op1 - op2\n",
        "        target_dsl = f\"M{current_m_id} = A[0,0] - B[0,0]\"\n",
        "        user_prompt_content = generate_prompt_text(\"\", matrices, _executed_steps, _m_vars, _c_vals)\n",
        "        chatml_text = f\"<|im_start|>system\\n{SYSTEM_MESSAGE}<|im_end|>\\n<|im_start|>user\\n{user_prompt_content}<|im_end|>\\n<|im_start|>assistant\\n{target_dsl}<|im_end|>\"\n",
        "        sft_examples.append({\"text\": chatml_text})\n",
        "        # No need to update state further for this example generator\n",
        "\n",
        "    print(f\"Generated {len(sft_examples)} SFT examples.\")\n",
        "    return sft_examples\n",
        "\n",
        "# Generate the dataset (aim for ~50, 16 per set, so 3 sets = 48 + a few more)\n",
        "# 3 sets * 16 steps/set = 48 examples. Plus the 3 manual ones = 51.\n",
        "fine_tuning_data = generate_standard_matmul_steps_for_sft(num_example_sets=30)\n",
        "\n",
        "# Save to a JSONL file for inspection or use by Hugging Face datasets\n",
        "output_jsonl_path = \"dsl_finetune_data.jsonl\"\n",
        "with open(output_jsonl_path, 'w') as f:\n",
        "    for item in fine_tuning_data:\n",
        "        f.write(json.dumps(item) + \"\\n\")\n",
        "\n",
        "print(f\"Fine-tuning data saved to {output_jsonl_path}\")\n",
        "print(\"First example:\")\n",
        "print(json.dumps(fine_tuning_data[0], indent=2))\n",
        "print(\"\\nLast example:\")\n",
        "print(json.dumps(fine_tuning_data[-1], indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pb2uKz2UgKKg",
        "outputId": "bb6d3ddd-a327-4d26-e15b-cde7a37f396d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated 483 SFT examples.\n",
            "Fine-tuning data saved to dsl_finetune_data.jsonl\n",
            "First example:\n",
            "{\n",
            "  \"text\": \"<|im_start|>system\\nASSISTANT'S ONLY TASK: Output ONE SINGLE VALID DSL line for 2x2 matrix multiplication (C = A * B).\\nNO PYTHON. NO EXPLANATIONS. ONLY THE DSL LINE.\\nIf stuck, output ONLY: Error: Cannot determine next step.<|im_end|>\\n<|im_start|>user\\n\\n\\nCurrent algorithm state:\\nMatrices:\\nA = [[2, 3], [1, 4]]\\nB = [[5, 1], [2, 6]]\\nExecuted Steps:\\nNone yet.\\nIntermediate Variables (M values):\\n{}\\nOutput Matrix C (nan means not yet computed, computed means a DSL line has assigned to it):\\n[[nan, nan],\\n [nan, nan]]\\nProvide the NEXT SINGLE DSL step according to the rules:<|im_end|>\\n<|im_start|>assistant\\nM0 = A[0,0] * B[0,0]<|im_end|>\"\n",
            "}\n",
            "\n",
            "Last example:\n",
            "{\n",
            "  \"text\": \"<|im_start|>system\\nASSISTANT'S ONLY TASK: Output ONE SINGLE VALID DSL line for 2x2 matrix multiplication (C = A * B).\\nNO PYTHON. NO EXPLANATIONS. ONLY THE DSL LINE.\\nIf stuck, output ONLY: Error: Cannot determine next step.<|im_end|>\\n<|im_start|>user\\n\\n\\nCurrent algorithm state:\\nMatrices:\\nA = [[2, 3], [1, 4]]\\nB = [[5, 1], [2, 6]]\\nExecuted Steps:\\nM0=A[0,0]*B[0,0]\\nM1=A[0,1]*B[1,0]\\nM2=M0+M1\\nC[0,0]=M2\\nM360 = A[1,1]\\nM361 = M1\\nIntermediate Variables (M values):\\n{M0: A[0,0]*B[0,0], M1: A[0,1]*B[1,0], M2: M0+M1, M360: A[1,1], M361: M1}\\nOutput Matrix C (nan means not yet computed, computed means a DSL line has assigned to it):\\n[[computed, nan],\\n [nan, nan]]\\nProvide the NEXT SINGLE DSL step according to the rules:<|im_end|>\\n<|im_start|>assistant\\nM362 = A[0,0] - B[0,0]<|im_end|>\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test with both correct traning code and data"
      ],
      "metadata": {
        "id": "pjWoGWdciXPj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================\n",
        "# QWEN DSL Fine-Tuning Full Script\n",
        "# ==============================================\n",
        "# Dependencies (install in your Colab / environment):\n",
        "# !pip install -q transformers datasets peft accelerate bitsandbytes trl\n",
        "\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM, AutoTokenizer,\n",
        "    BitsAndBytesConfig, TrainingArguments\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 1. (Optional) Mount Google Drive\n",
        "# ------------------------------------------------\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# DRIVE_BASE_PATH = \"/content/drive/MyDrive/Qwen_DSL_FineTune\"\n",
        "# os.makedirs(DRIVE_BASE_PATH, exist_ok=True)\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 2. System Message and Dummy Dataset Generation\n",
        "# ------------------------------------------------\n",
        "SYSTEM_MESSAGE = (\n",
        "    \"ASSISTANT'S ONLY TASK: Output ONE SINGLE VALID DSL line for 2x2 matrix multiplication (C = A * B).\"\n",
        "    \"\\nNO PYTHON. NO EXPLANATIONS. ONLY THE DSL LINE.\"\n",
        "    \"\\nIf stuck, output ONLY: Error: Cannot determine next step.\"\n",
        ")\n",
        "\n",
        "# Create minimal dummy data if JSONL not present\n",
        "dataset_path = \"dsl_finetune_data.jsonl\"\n",
        "if not os.path.exists(dataset_path):\n",
        "    def _generate_dummy():\n",
        "        examples = []\n",
        "        def make_item(user, assistant):\n",
        "            chatml = (\n",
        "                f\"<|im_start|>system\\n{SYSTEM_MESSAGE}<|im_end|>\"\n",
        "                f\"\\n<|im_start|>user\\n{user}<|im_end|>\"\n",
        "                f\"\\n<|im_start|>assistant\\n{assistant}<|im_end|>\"\n",
        "            )\n",
        "            return {\"text\": chatml}\n",
        "\n",
        "        # Example 1\n",
        "        u1 = (\n",
        "            \"Current algorithm state:\\n\"\n",
        "            \"Matrices:\\n\"\n",
        "            \"A = [[2, 3], [1, 4]]\\n\"\n",
        "            \"B = [[5, 1], [2, 6]]\\n\"\n",
        "            \"Executed Steps:\\nNone yet.\\n\"\n",
        "            \"Intermediate Variables (M values): {}\\n\"\n",
        "            \"Output Matrix C (NaN means not yet computed):\\n\"\n",
        "            \"[[nan, nan],\\n [nan, nan]]\\n\"\n",
        "            \"Provide the NEXT SINGLE DSL step according to the rules:\"\n",
        "        )\n",
        "        a1 = \"M0 = A[0,0] * B[0,0]\"\n",
        "        examples.append(make_item(u1, a1))\n",
        "\n",
        "        # Example 2\n",
        "        u2 = (\n",
        "            \"Current algorithm state:\\n\"\n",
        "            \"Matrices:\\n\"\n",
        "            \"A = [[-1, 0], [2, 1]]\\n\"\n",
        "            \"B = [[4, 2], [-3, 5]]\\n\"\n",
        "            \"Executed Steps:\\n\"\n",
        "            \"M0 = A[0,0] * B[0,1]\\nM1 = A[0,0] * B[0,0]\\n\"\n",
        "            \"M2 = A[0,1] * B[1,0]\\nM3 = A[0,1] * B[1,1]\\n\"\n",
        "            \"M4 = M1 + M2\\nM5 = M0 + M3\\n\"\n",
        "            \"Intermediate Variables (M values): {M0: A[0,0]*B[0,1], M1: A[0,0]*B[0,0], M2: A[0,1]*B[1,0], M3: A[0,1]*B[1,1], M4: M1+M2, M5: M0+M3}\\n\"\n",
        "            \"Output Matrix C (NaN means not yet computed):\\n\"\n",
        "            \"[[nan, nan],\\n [nan, nan]]\\n\"\n",
        "            \"Provide the NEXT SINGLE DSL step according to the rules:\"\n",
        "        )\n",
        "        a2 = \"C[0,1] = M5\"\n",
        "        examples.append(make_item(u2, a2))\n",
        "\n",
        "        with open(dataset_path, 'w') as f:\n",
        "            for ex in examples:\n",
        "                f.write(json.dumps(ex) + \"\\n\")\n",
        "        print(f\"Dummy dataset created at {dataset_path}\")\n",
        "\n",
        "    _generate_dummy()\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 3. Load Dataset\n",
        "# ------------------------------------------------\n",
        "dataset = load_dataset(\"json\", data_files=dataset_path, split=\"train\")\n",
        "print(f\"Loaded {len(dataset)} examples from {dataset_path}\")\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 4. Model & Tokenizer Setup\n",
        "# ------------------------------------------------\n",
        "model_name = \"Qwen/Qwen1.5-0.5B-Chat\"\n",
        "\n",
        "# Quantization config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# Model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 5. LoRA Configuration & PEFT Wrapping\n",
        "# ------------------------------------------------\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 6. Sanity Check: Ensure target_modules exist\n",
        "# ------------------------------------------------\n",
        "print(\"\\nValidating target module names:\")\n",
        "targets = lora_config.target_modules\n",
        "leafs = {name.split('.')[-1] for name, _ in model.named_modules()}\n",
        "for t in targets:\n",
        "    print(f\"{t:10s} → {('FOUND' if t in leafs else 'MISSING')}\")\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 7. Training Arguments & SFTTrainer\n",
        "# ------------------------------------------------\n",
        "output_dir = \"./qwen_dsl_finetuned_adapter\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=2,\n",
        "    learning_rate=2e-4,\n",
        "    num_train_epochs=3,\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    fp16=False,\n",
        "    bf16=True,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    peft_config=lora_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=512,\n",
        ")\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 8. Start Fine-Tuning\n",
        "# ------------------------------------------------\n",
        "print(\"Starting fine-tuning...\")\n",
        "trainer.train()\n",
        "print(\"Fine-tuning completed.\")\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 9. Save LoRA Adapter & Tokenizer\n",
        "# ------------------------------------------------\n",
        "adapter_path = os.path.join(output_dir, \"final_adapter\")\n",
        "trainer.save_model(adapter_path)\n",
        "tokenizer.save_pretrained(adapter_path)\n",
        "print(f\"Adapter and tokenizer saved at {adapter_path}\")\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 10. (Optional) Test the Fine-Tuned Model\n",
        "# ------------------------------------------------\n",
        "def test_model(prompt: str):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_MESSAGE},\n",
        "        {\"role\": \"user\",   \"content\": prompt},\n",
        "    ]\n",
        "    text_in = tokenizer.apply_chat_template(\n",
        "        messages, tokenize=False, add_generation_prompt=True\n",
        "    )\n",
        "    inputs = tokenizer([text_in], return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=25,\n",
        "            do_sample=False,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    gen = out[0][inputs[\"input_ids\"].shape[-1]:]\n",
        "    return tokenizer.decode(gen, skip_special_tokens=True).strip()\n",
        "\n",
        "# Example tests\n",
        "prompt1 = (\n",
        "    \"Current algorithm state:\\nMatrices:\\nA = [[2, 3], [1, 4]]\\n\"\n",
        "    \"B = [[5, 1], [2, 6]]\\nExecuted Steps:\\nNone yet.\\n\"\n",
        "    \"Intermediate Variables: {}\\nOutput Matrix C ([[nan,nan],[nan,nan]])\\n\"\n",
        "    \"Provide the NEXT SINGLE DSL step.\"\n",
        ")\n",
        "print(\"Test 1 response:\", test_model(prompt1))\n",
        "\n",
        "prompt2 = (\n",
        "    \"Current algorithm state:\\nMatrices:\\nA = [[-1, 0], [2, 1]]\\n\"\n",
        "    \"B = [[4, 2], [-3, 5]]\\nExecuted Steps:\\n\"\n",
        "    \"M0 = A[0,0]*B[0,1]\\nM1 = A[0,0]*B[0,0]\\n\"\n",
        "    \"M2 = A[0,1]*B[1,0]\\nM3 = A[0,1]*B[1,1]\\n\"\n",
        "    \"M4 = M1+M2\\nM5 = M0+M3\\n\"\n",
        "    \"Intermediate Variables: {...}\\nOutput C ([[nan,nan],[nan,nan]])\\n\"\n",
        "    \"Provide the NEXT SINGLE DSL step.\"\n",
        ")\n",
        "print(\"Test 2 response:\", test_model(prompt2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "4f8260cacf0e40889ce8d44d85aa32d3",
            "2521f657376844c8ba8219b27aa4125d",
            "28a5a88553ad4af1a12540155a861325",
            "30ff21d915604c30bbaee2ad479a5ebc",
            "b9ef314febd84ce7a27631f6a6747275",
            "227b059ddd8e4d38a084e27cdce4461f",
            "eaf9e67f49574fbb92c89061bc350225",
            "6de8cd21e383418eaebd14404f8011d6",
            "826bcca60c8a45fd9ab54f5dcdde801a",
            "cde77c2202e442809ccb6c52976e186a",
            "9dc4d9d549ae499d98916b14e459d863",
            "5cadb84487d84557ab9aaf9fa4f0fc29",
            "947ef962eaac4859ab902975b322b735",
            "fae2691374ee436e9c8838fff9b71ad8",
            "ec45e480698c48229f418ffd93f8d02d",
            "099f6d29f2b846e39f27f0f7ea333037",
            "e076a64c972f4b6990ba89d224bc41aa",
            "859d729d2add4bab98f56e47297c53c1",
            "c9db879c83b343259475849a9ea99531",
            "965a60b985fe4eb1b143ba21ce6b3530",
            "65af1e0afd6c4cf5bc3a87829c10fd81",
            "ce94124d4f6e4208ac489391ff4e5a32"
          ]
        },
        "id": "BRjRW2Q1hHJj",
        "outputId": "fef2f9cb-32dd-44e2-a72b-0daeb7f3a08d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4f8260cacf0e40889ce8d44d85aa32d3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 483 examples from dsl_finetune_data.jsonl\n",
            "trainable params: 7,569,408 || all params: 471,557,120 || trainable%: 1.6052\n",
            "\n",
            "Validating target module names:\n",
            "o_proj     → FOUND\n",
            "k_proj     → FOUND\n",
            "down_proj  → FOUND\n",
            "q_proj     → FOUND\n",
            "up_proj    → FOUND\n",
            "gate_proj  → FOUND\n",
            "v_proj     → FOUND\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '0.13.0'.\n",
            "\n",
            "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py:328: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/483 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5cadb84487d84557ab9aaf9fa4f0fc29"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting fine-tuning...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='363' max='363' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [363/363 18:13, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.756500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.282700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.065400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.040800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.040000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.038200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.035400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.030800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.031600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.028700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.028600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.025700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.023200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.023700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.030700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.035800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.025900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.024200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.023000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.023200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.024800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.022800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>0.022500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.022200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.026500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.023800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>0.022200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.021000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>0.021200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.023200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>0.023800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>0.021100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>0.021500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>0.020300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.020600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>0.020100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tuning completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adapter and tokenizer saved at ./qwen_dsl_finetuned_adapter/final_adapter\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test 1 response: M96 = A[0,0] * B[0,0]\n",
            "Test 2 response: C[0,0] = M5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "def get_dsl_test_suite():\n",
        "    \"\"\"Generates 10 test cases for DSL-based 2x2 matrix multiplication.\"\"\"\n",
        "\n",
        "    ops_definition = [\n",
        "        {'dsl': \"M0 = A[0,0]*B[0,0]\", 'var': 'M0', 'calc': lambda A,B,V: A[0][0]*B[0][0]},\n",
        "        {'dsl': \"M1 = A[0,1]*B[1,0]\", 'var': 'M1', 'calc': lambda A,B,V: A[0][1]*B[1][0]},\n",
        "        {'dsl': \"S0 = M0+M1\",         'var': 'S0', 'calc': lambda A,B,V: V['M0']+V['M1']},\n",
        "        {'dsl': \"C[0,0] = S0\",        'var': None, 'calc': lambda A,B,V: V['S0'], 'c_idx': (0,0)},\n",
        "        {'dsl': \"M2 = A[0,0]*B[0,1]\", 'var': 'M2', 'calc': lambda A,B,V: A[0][0]*B[0][1]},\n",
        "        {'dsl': \"M3 = A[0,1]*B[1,1]\", 'var': 'M3', 'calc': lambda A,B,V: A[0][1]*B[1][1]},\n",
        "        {'dsl': \"S1 = M2+M3\",         'var': 'S1', 'calc': lambda A,B,V: V['M2']+V['M3']},\n",
        "        {'dsl': \"C[0,1] = S1\",        'var': None, 'calc': lambda A,B,V: V['S1'], 'c_idx': (0,1)},\n",
        "        {'dsl': \"M4 = A[1,0]*B[0,0]\", 'var': 'M4', 'calc': lambda A,B,V: A[1][0]*B[0][0]},\n",
        "        {'dsl': \"M5 = A[1,1]*B[1,0]\", 'var': 'M5', 'calc': lambda A,B,V: A[1][1]*B[1][0]},\n",
        "        {'dsl': \"S2 = M4+M5\",         'var': 'S2', 'calc': lambda A,B,V: V['M4']+V['M5']},\n",
        "        {'dsl': \"C[1,0] = S2\",        'var': None, 'calc': lambda A,B,V: V['S2'], 'c_idx': (1,0)},\n",
        "        {'dsl': \"M6 = A[1,0]*B[0,1]\", 'var': 'M6', 'calc': lambda A,B,V: A[1][0]*B[0][1]},\n",
        "        {'dsl': \"M7 = A[1,1]*B[1,1]\", 'var': 'M7', 'calc': lambda A,B,V: A[1][1]*B[1][1]},\n",
        "        {'dsl': \"S3 = M6+M7\",         'var': 'S3', 'calc': lambda A,B,V: V['M6']+V['M7']},\n",
        "        {'dsl': \"C[1,1] = S3\",        'var': None, 'calc': lambda A,B,V: V['S3'], 'c_idx': (1,1)},\n",
        "    ]\n",
        "\n",
        "    matrix_sets = [\n",
        "        {\"A\": [[2,3],[1,4]], \"B\": [[5,1],[2,6]]},\n",
        "        {\"A\": [[-1,0],[2,1]], \"B\": [[4,2],[-3,5]]},\n",
        "        {\"A\": [[1,0],[0,1]], \"B\": [[10,20],[30,40]]},\n",
        "        {\"A\": [[1,1],[1,1]], \"B\": [[1,2],[3,1]]},\n",
        "        {\"A\": [[0,1],[1,0]], \"B\": [[5,8],[2,7]]}\n",
        "    ]\n",
        "\n",
        "    test_points_config = [\n",
        "        (0, 0), (0, 1), (0, 2), (1, 3), (1, 4),\n",
        "        (2, 6), (2, 8), (3, 11), (3, 12), (4, 14)\n",
        "    ]\n",
        "\n",
        "    test_suite = []\n",
        "\n",
        "    for i, (matrix_idx, num_executed) in enumerate(test_points_config):\n",
        "        current_A_val = matrix_sets[matrix_idx][\"A\"]\n",
        "        current_B_val = matrix_sets[matrix_idx][\"B\"]\n",
        "\n",
        "        matrix_a_str = str(current_A_val).replace(\" \", \"\")\n",
        "        matrix_b_str = str(current_B_val).replace(\" \", \"\")\n",
        "\n",
        "        executed_dsl_list_for_prompt = []\n",
        "        intermediate_vars_dict_for_prompt = {}\n",
        "        current_C_matrix_val = [['nan' for _ in range(2)] for _ in range(2)]\n",
        "\n",
        "        for step_idx in range(num_executed):\n",
        "            op = ops_definition[step_idx]\n",
        "            executed_dsl_list_for_prompt.append(op['dsl'])\n",
        "            val = op['calc'](current_A_val, current_B_val, intermediate_vars_dict_for_prompt)\n",
        "            if op['var']:\n",
        "                intermediate_vars_dict_for_prompt[op['var']] = val\n",
        "            elif 'c_idx' in op:\n",
        "                r, c = op['c_idx']\n",
        "                current_C_matrix_val[r][c] = str(val)\n",
        "\n",
        "        executed_steps_str_for_prompt = \"\\n\".join(executed_dsl_list_for_prompt) if executed_dsl_list_for_prompt else \"None yet.\"\n",
        "        intermediate_vars_str_for_prompt = str(intermediate_vars_dict_for_prompt)\n",
        "        c_matrix_str_for_prompt = f\"([[{current_C_matrix_val[0][0]},{current_C_matrix_val[0][1]}],[{current_C_matrix_val[1][0]},{current_C_matrix_val[1][1]}]])\"\n",
        "\n",
        "        prompt_str = (\n",
        "            f\"Current algorithm state:\\nMatrices:\\nA = {matrix_a_str}\\n\"\n",
        "            f\"B = {matrix_b_str}\\nExecuted Steps:\\n{executed_steps_str_for_prompt}\\n\"\n",
        "            f\"Intermediate Variables: {intermediate_vars_str_for_prompt}\\nOutput Matrix C {c_matrix_str_for_prompt}\\n\"\n",
        "            \"Provide the NEXT SINGLE DSL step.\"\n",
        "        )\n",
        "\n",
        "        expected_dsl_step_str = ops_definition[num_executed]['dsl']\n",
        "\n",
        "        test_suite.append({\n",
        "            \"id\": i + 1,\n",
        "            \"prompt\": prompt_str,\n",
        "            \"expected_dsl_step\": expected_dsl_step_str,\n",
        "        })\n",
        "\n",
        "    return test_suite\n",
        "\n",
        "# Ensure your 'test_model' function is defined elsewhere in your script\n",
        "# or imported before this __main__ block is executed.\n",
        "# For example:\n",
        "# from your_module import test_model\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    dsl_tests = get_dsl_test_suite()\n",
        "\n",
        "    print(\"\\nRunning DSL Generation Tests (calling your existing 'test_model' function):\")\n",
        "    correct_predictions = 0\n",
        "    total_tests = len(dsl_tests)\n",
        "\n",
        "    # Make sure your 'test_model' function is accessible here.\n",
        "    # If it's not defined globally or imported, this will cause a NameError.\n",
        "    if 'test_model' not in globals() and 'test_model' not in locals():\n",
        "        print(\"\\nERROR: 'test_model' function not found.\")\n",
        "        print(\"Please ensure your 'test_model(prompt_string)' function is defined or imported.\")\n",
        "        print(\"Skipping test execution.\\n\")\n",
        "    else:\n",
        "        for test_case in dsl_tests:\n",
        "            print(f\"\\n----- Test Case {test_case['id']} -----\")\n",
        "            # print(f\"Full Prompt:\\n{test_case['prompt']}\\n\") # Uncomment to see the full prompt if needed\n",
        "\n",
        "            # Call your existing model function with the prompt\n",
        "            actual_dsl_step = test_model(test_case['prompt']) # This now calls YOUR function\n",
        "\n",
        "            print(f\"Test {test_case['id']} response: {actual_dsl_step}\")\n",
        "            print(f\"Expected DSL Step   : {test_case['expected_dsl_step']}\")\n",
        "\n",
        "            if actual_dsl_step == test_case['expected_dsl_step']:\n",
        "                print(\"Result: CORRECT ✅\")\n",
        "                correct_predictions += 1\n",
        "            else:\n",
        "                print(\"Result: INCORRECT ❌\")\n",
        "                # You might want to print the full prompt here if it's incorrect for debugging\n",
        "                # print(f\"  Full Prompt for Incorrect Test {test_case['id']}:\\n{test_case['prompt']}\")\n",
        "\n",
        "        print(\"\\n----- Test Summary -----\")\n",
        "        print(f\"Total tests run: {total_tests}\")\n",
        "        print(f\"Correct predictions: {correct_predictions}\")\n",
        "        print(f\"Accuracy: {correct_predictions/total_tests:.2%}\" if total_tests > 0 else \"N/A\")\n",
        "        print(\"-------------------------\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSCFUK4Gibed",
        "outputId": "fc8b00b5-5af2-49d7-83e3-45b9b8ed118e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running DSL Generation Tests (calling your existing 'test_model' function):\n",
            "\n",
            "----- Test Case 1 -----\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test 1 response: M96 = A[0,0] * B[0,0]\n",
            "Expected DSL Step   : M0 = A[0,0]*B[0,0]\n",
            "Result: INCORRECT ❌\n",
            "\n",
            "----- Test Case 2 -----\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test 2 response: M180 = A[0,1] - M0\n",
            "Expected DSL Step   : M1 = A[0,1]*B[1,0]\n",
            "Result: INCORRECT ❌\n",
            "\n",
            "----- Test Case 3 -----\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test 3 response: M2 = M0 + M1\n",
            "Expected DSL Step   : S0 = M0+M1\n",
            "Result: INCORRECT ❌\n",
            "\n",
            "----- Test Case 4 -----\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test 4 response: C[0,0] = S0\n",
            "Expected DSL Step   : C[0,0] = S0\n",
            "Result: CORRECT ✅\n",
            "\n",
            "----- Test Case 5 -----\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test 5 response: M2 = A[0,0]*B[0,1]\n",
            "M3 = A[0,1]*B\n",
            "Expected DSL Step   : M2 = A[0,0]*B[0,1]\n",
            "Result: INCORRECT ❌\n",
            "\n",
            "----- Test Case 6 -----\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test 6 response: M4 = A[0,1]*B[1,0]\n",
            "Expected DSL Step   : S1 = M2+M3\n",
            "Result: INCORRECT ❌\n",
            "\n",
            "----- Test Case 7 -----\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test 7 response: M4 = A[1,0]*B[0,0]\n",
            "M5 = A[1,1]*B\n",
            "Expected DSL Step   : M4 = A[1,0]*B[0,0]\n",
            "Result: INCORRECT ❌\n",
            "\n",
            "----- Test Case 8 -----\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test 8 response: M6 = A[1,0]*B[0,1]\n",
            "Expected DSL Step   : C[1,0] = S2\n",
            "Result: INCORRECT ❌\n",
            "\n",
            "----- Test Case 9 -----\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test 9 response: M6 = A[1,0]*B[0,1]\n",
            "Expected DSL Step   : M6 = A[1,0]*B[0,1]\n",
            "Result: CORRECT ✅\n",
            "\n",
            "----- Test Case 10 -----\n",
            "Test 10 response: M7 = A[1,1]*B[1,1]\n",
            "Expected DSL Step   : S3 = M6+M7\n",
            "Result: INCORRECT ❌\n",
            "\n",
            "----- Test Summary -----\n",
            "Total tests run: 10\n",
            "Correct predictions: 2\n",
            "Accuracy: 20.00%\n",
            "-------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the model for normal matrix mul using Finetune"
      ],
      "metadata": {
        "id": "mX4rR0ey5rKQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade \"transformers==4.42.0\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 948
        },
        "id": "b9lE7F-fhbd6",
        "outputId": "87c9b4dd-a872-4e84-f519-f7412834a454"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.42.0\n",
            "  Downloading transformers-4.42.0-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.42.0) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.11/dist-packages (from transformers==4.42.0) (0.31.2)\n",
            "Collecting numpy<2.0,>=1.17 (from transformers==4.42.0)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.42.0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.42.0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.42.0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.42.0) (2.32.3)\n",
            "Collecting tokenizers<0.20,>=0.19 (from transformers==4.42.0)\n",
            "  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.42.0) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.42.0) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.42.0) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.42.0) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.42.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.42.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.42.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.42.0) (2025.4.26)\n",
            "Downloading transformers-4.42.0-py3-none-any.whl (9.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, tokenizers, transformers\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.1\n",
            "    Uninstalling tokenizers-0.21.1:\n",
            "      Successfully uninstalled tokenizers-0.21.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.52.2\n",
            "    Uninstalling transformers-4.52.2:\n",
            "      Successfully uninstalled transformers-4.52.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "trl 0.17.0 requires transformers>=4.46.0, but you have transformers 4.42.0 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 3.20.0 which is incompatible.\n",
            "tensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 3.20.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4 tokenizers-0.19.1 transformers-4.42.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "transformers"
                ]
              },
              "id": "3416542fa0bb482684cd3dbd20134cac"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datasets accelerate bitsandbytes trl\n",
        "!pip install peft==0.13.2\n",
        "!pip install --upgrade \"transformers>=4.42.0\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-qmB_bUFmKB",
        "outputId": "b48be936-2952-4e9d-bb71-fd192fb3877a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m348.0/348.0 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting peft==0.13.2\n",
            "  Downloading peft-0.13.2-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft==0.13.2) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.13.2) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft==0.13.2) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft==0.13.2) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.13.2) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft==0.13.2) (4.51.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft==0.13.2) (4.67.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.13.2) (1.6.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft==0.13.2) (0.5.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.13.2) (0.31.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.17.0->peft==0.13.2) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.17.0->peft==0.13.2) (2025.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.17.0->peft==0.13.2) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.17.0->peft==0.13.2) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.13.2) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.13.2) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.13.2) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.13.2) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.13.2) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.13.2) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.13.2) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.13.2) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.13.2) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.13.2) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.13.2) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.13.2) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.13.2) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.13.2) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.13.2) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.13.2) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.13.2) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft==0.13.2) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft==0.13.2) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->peft==0.13.2) (0.21.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft==0.13.2) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.13.2) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.13.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.13.2) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.13.2) (2025.4.26)\n",
            "Downloading peft-0.13.2-py3-none-any.whl (320 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: peft\n",
            "  Attempting uninstall: peft\n",
            "    Found existing installation: peft 0.15.2\n",
            "    Uninstalling peft-0.15.2:\n",
            "      Successfully uninstalled peft-0.15.2\n",
            "Successfully installed peft-0.13.2\n",
            "Requirement already satisfied: transformers>=4.42.0 in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Collecting transformers>=4.42.0\n",
            "  Downloading transformers-4.52.3-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.2/40.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers>=4.42.0) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.42.0) (0.31.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.42.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.42.0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.42.0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.42.0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.42.0) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.42.0) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.42.0) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.42.0) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=4.42.0) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=4.42.0) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.42.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.42.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.42.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.42.0) (2025.4.26)\n",
            "Downloading transformers-4.52.3-py3-none-any.whl (10.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m133.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.51.3\n",
            "    Uninstalling transformers-4.51.3:\n",
            "      Successfully uninstalled transformers-4.51.3\n",
            "Successfully installed transformers-4.52.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import json\n",
        "\n",
        "# This COMPREHENSIVE_SYSTEM_MESSAGE_FOR_FULL_SEQUENCE defines all potential DSL rules.\n",
        "# The fine-tuning data (assistant responses) will teach the specific algorithm and M/S naming conventions.\n",
        "COMPREHENSIVE_SYSTEM_MESSAGE_FOR_FULL_SEQUENCE = \"\"\"You are an AI assistant. Your ONLY task is to output the COMPLETE sequence of VALID DSL lines to perform a 2x2 matrix multiplication (C = A * B), with each DSL line on a new line.\n",
        "STRICT RULES:\n",
        "1.  Adhere strictly to the DSL formats defined below for each line in the sequence.\n",
        "2.  NO PYTHON code (e.g., 'np.dot', '@', 'np.array', 'loops').\n",
        "3.  NO EXPLANATIONS, apologies, or conversational text. Output only the DSL lines, each on a new line.\n",
        "4.  The generated sequence must correctly implement the standard 2x2 matrix multiplication algorithm as a sequence of elementary operations.\n",
        "    (e.g., C[0,0] = A[0,0]*B[0,0] + A[0,1]*B[1,0] should be broken down into steps like M0=A[0,0]*B[0,0], M1=A[0,1]*B[1,0], S0=M0+M1, C[0,0]=S0, following this pattern for all C elements).\n",
        "    The fine-tuning examples (assistant responses) will demonstrate the specific M-variable and S-variable naming convention (M for products, S for sums).\n",
        "5.  If you cannot determine a valid full DSL sequence based on the rules and input, output ONLY the exact text: Error: Cannot determine full sequence.\n",
        "\n",
        "DSL FORMAT DEFINITIONS:\n",
        "\n",
        "1.  **Intermediate Variable Assignment (M-variables):**\n",
        "    * Purpose: To store intermediate results from arithmetic operations or direct assignments.\n",
        "    * Naming: `<id>` is an integer (e.g., M0, M1, M2, ...). Use the next available or contextually appropriate ID. (The specific M/S convention for matmul will be learned from examples).\n",
        "    * Formats:\n",
        "        * `M<id> = <operand1> + <operand2>`  (Addition)\n",
        "        * `M<id> = <operand1> - <operand2>`  (Subtraction)\n",
        "        * `M<id> = <operand1> * <operand2>`  (Multiplication of operands)\n",
        "        * `M<id> = <operand>`             (Direct assignment)\n",
        "    * Examples:\n",
        "        * `M0 = A[0,0] + B[0,0]`\n",
        "        * `M1 = A[0,1] - M0`\n",
        "        * `M2 = A[0,0] * B[0,0]`\n",
        "        * `M3 = M1`\n",
        "        * `M4 = B[1,0]`\n",
        "\n",
        "2.  **Output Matrix Cell Assignment (C-matrix):**\n",
        "    * Purpose: To assign a computed value to an element of the 2x2 output matrix C.\n",
        "    * Format: `C[<row>,<col>] = <operand>`\n",
        "    * Constraints: `<row>` and `<col>` must be 0 or 1.\n",
        "    * Example: `C[0,1] = M5` (Note: for standard matmul, the operand will typically be an S-variable as learned from examples like S0, S1, etc.)\n",
        "\n",
        "3.  **Operands (`<operand>`, `<operand1>`, `<operand2>`):**\n",
        "    * Can be an element from input matrix A: `A[<row>,<col>]` (e.g., A[0,0], A[1,0])\n",
        "    * Can be an element from input matrix B: `B[<row>,<col>]` (e.g., B[0,1], B[1,1])\n",
        "    * Can be a previously defined intermediate variable: `M<id>` (e.g., M0, M3). (The examples will show S-variables like S0, S1, etc., being used for sums in the matmul context).\n",
        "    * Constraints: All `<row>` and `<col>` indices for A and B must be 0 or 1. Any `M<id>` (or S-variable, which is a type of intermediate variable) used as an operand must have been defined in a previous DSL step in the sequence.\n",
        "\n",
        "You will be given the input matrices A (2x2) and B (2x2).\n",
        "Provide the complete, multi-line DSL sequence to calculate C = A * B.\n",
        "\"\"\"\n",
        "\n",
        "def get_standard_ops_definition():\n",
        "    \"\"\"\n",
        "    Defines the 16 DSL steps for a standard 2x2 matrix multiplication.\n",
        "    The 'dsl' field provides the string for each step.\n",
        "    This sequence implicitly defines the M-for-product and S-for-sum convention.\n",
        "    \"\"\"\n",
        "    ops = [\n",
        "        # C[0,0] = A[0,0]*B[0,0] + A[0,1]*B[1,0]\n",
        "        {'dsl': \"M0 = A[0,0]*B[0,0]\", 'var_out': 'M0', 'symbolic_rhs': \"A[0,0]*B[0,0]\"},\n",
        "        {'dsl': \"M1 = A[0,1]*B[1,0]\", 'var_out': 'M1', 'symbolic_rhs': \"A[0,1]*B[1,0]\"},\n",
        "        {'dsl': \"S0 = M0+M1\",         'var_out': 'S0', 'symbolic_rhs': \"M0+M1\"},\n",
        "        {'dsl': \"C[0,0] = S0\",        'var_out': None, 'c_idx': (0,0), 'symbolic_rhs': \"S0\"},\n",
        "\n",
        "        # C[0,1] = A[0,0]*B[0,1] + A[0,1]*B[1,1]\n",
        "        {'dsl': \"M2 = A[0,0]*B[0,1]\", 'var_out': 'M2', 'symbolic_rhs': \"A[0,0]*B[0,1]\"},\n",
        "        {'dsl': \"M3 = A[0,1]*B[1,1]\", 'var_out': 'M3', 'symbolic_rhs': \"A[0,1]*B[1,1]\"},\n",
        "        {'dsl': \"S1 = M2+M3\",         'var_out': 'S1', 'symbolic_rhs': \"M2+M3\"},\n",
        "        {'dsl': \"C[0,1] = S1\",        'var_out': None, 'c_idx': (0,1), 'symbolic_rhs': \"S1\"},\n",
        "\n",
        "        # C[1,0] = A[1,0]*B[0,0] + A[1,1]*B[1,0]\n",
        "        {'dsl': \"M4 = A[1,0]*B[0,0]\", 'var_out': 'M4', 'symbolic_rhs': \"A[1,0]*B[0,0]\"},\n",
        "        {'dsl': \"M5 = A[1,1]*B[1,0]\", 'var_out': 'M5', 'symbolic_rhs': \"A[1,1]*B[1,0]\"},\n",
        "        {'dsl': \"S2 = M4+M5\",         'var_out': 'S2', 'symbolic_rhs': \"M4+M5\"},\n",
        "        {'dsl': \"C[1,0] = S2\",        'var_out': None, 'c_idx': (1,0), 'symbolic_rhs': \"S2\"},\n",
        "\n",
        "        # C[1,1] = A[1,0]*B[0,1] + A[1,1]*B[1,1]\n",
        "        {'dsl': \"M6 = A[1,0]*B[0,1]\", 'var_out': 'M6', 'symbolic_rhs': \"A[1,0]*B[0,1]\"},\n",
        "        {'dsl': \"M7 = A[1,1]*B[1,1]\", 'var_out': 'M7', 'symbolic_rhs': \"A[1,1]*B[1,1]\"},\n",
        "        {'dsl': \"S3 = M6+M7\",         'var_out': 'S3', 'symbolic_rhs': \"M6+M7\"},\n",
        "        {'dsl': \"C[1,1] = S3\",        'var_out': None, 'c_idx': (1,1), 'symbolic_rhs': \"S3\"},\n",
        "    ]\n",
        "    # Other keys like 'calc_val' are kept for potential other uses but not strictly needed\n",
        "    # for this specific dataset generation task where only 'dsl' is used for the completion.\n",
        "    return ops\n",
        "\n",
        "def generate_random_matrix():\n",
        "    \"\"\"Generates a 2x2 matrix with random integer elements between -9 and 9.\"\"\"\n",
        "    return [[random.randint(-9, 9) for _ in range(2)] for _ in range(2)]\n",
        "\n",
        "def generate_sft_dataset_full_sequence(num_full_multiplications=100, system_message=COMPREHENSIVE_SYSTEM_MESSAGE_FOR_FULL_SEQUENCE):\n",
        "    \"\"\"\n",
        "    Generates a fine-tuning dataset for DSL-based 2x2 matrix multiplication.\n",
        "    Each example is a full multiplication problem (A, B inputs) and the complete DSL sequence as completion.\n",
        "    Formatted in ChatML.\n",
        "    \"\"\"\n",
        "    ops_definition = get_standard_ops_definition()\n",
        "    # The assistant's completion is the fixed sequence of DSL steps for standard 2x2 matmul\n",
        "    full_dsl_completion = \"\\n\".join([op['dsl'] for op in ops_definition])\n",
        "\n",
        "    sft_data_chatml = [] # Store ChatML formatted strings\n",
        "\n",
        "    for i in range(num_full_multiplications):\n",
        "        # Generate new A and B matrices for each full multiplication problem\n",
        "        if i == 0: current_A_val, current_B_val = [[1,0],[0,1]], generate_random_matrix()\n",
        "        elif i == 1: current_A_val, current_B_val = generate_random_matrix(), [[1,0],[0,1]]\n",
        "        elif i == 2: current_A_val, current_B_val = [[0,0],[0,0]], generate_random_matrix()\n",
        "        elif i == 3: current_A_val, current_B_val = generate_random_matrix(), [[0,0],[0,0]]\n",
        "        elif i == 4: current_A_val, current_B_val = [[-2, 3],[1, -4]], [[5, -1],[-2, 6]]\n",
        "        else: current_A_val, current_B_val = generate_random_matrix(), generate_random_matrix()\n",
        "\n",
        "        matrix_a_str_for_prompt = str(current_A_val).replace(\" \", \"\")\n",
        "        matrix_b_str_for_prompt = str(current_B_val).replace(\" \", \"\")\n",
        "\n",
        "        # Construct the \"user\" part of the prompt\n",
        "        user_prompt_content = (\n",
        "            f\"Input matrices:\\n\"\n",
        "            f\"A = {matrix_a_str_for_prompt}\\n\"\n",
        "            f\"B = {matrix_b_str_for_prompt}\\n\"\n",
        "            f\"Provide the COMPLETE sequence of DSL steps to calculate C = A * B.\"\n",
        "        )\n",
        "\n",
        "        # Format into ChatML and add to dataset\n",
        "        chatml_text = f\"<|im_start|>system\\n{system_message.strip()}<|im_end|>\\n\"\n",
        "        chatml_text += f\"<|im_start|>user\\n{user_prompt_content.strip()}<|im_end|>\\n\"\n",
        "        chatml_text += f\"<|im_start|>assistant\\n{full_dsl_completion}<|im_end|>\"\n",
        "        sft_data_chatml.append({\"text\": chatml_text})\n",
        "\n",
        "    return sft_data_chatml\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    num_multiplication_examples = 100 # Number of full 2x2 matrix multiplication problems for the dataset\n",
        "\n",
        "    sft_dataset_chatml = generate_sft_dataset_full_sequence(\n",
        "        num_multiplication_examples,\n",
        "        system_message=COMPREHENSIVE_SYSTEM_MESSAGE_FOR_FULL_SEQUENCE\n",
        "    )\n",
        "\n",
        "    print(f\"Generated {len(sft_dataset_chatml)} ChatML examples for SFT.\")\n",
        "    print(f\"Each example provides input matrices A and B, and expects the full {len(get_standard_ops_definition())}-step DSL sequence as the completion.\")\n",
        "\n",
        "    print(\"\\n----- Example SFT Data (First ChatML item) -----\")\n",
        "    if sft_dataset_chatml:\n",
        "        print(sft_dataset_chatml[0]['text'])\n",
        "        print(\"--------------------------------------------------\")\n",
        "\n",
        "    if len(sft_dataset_chatml) > 1:\n",
        "        print(\"\\n----- Example SFT Data (Second ChatML item, if generated) -----\")\n",
        "        print(sft_dataset_chatml[1]['text'])\n",
        "        print(\"--------------------------------------------------\")\n",
        "\n",
        "    output_jsonl_path = \"dsl_finetune_data_standard_matmul_full_sequence.jsonl\"\n",
        "    with open(output_jsonl_path, 'w') as f:\n",
        "        for item in sft_dataset_chatml:\n",
        "            f.write(json.dumps(item) + \"\\n\")\n",
        "    print(f\"\\nFine-tuning data saved to {output_jsonl_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HBOjX5Mj0Wi",
        "outputId": "9228b8e8-ba4d-4b3d-b989-d203d3013e10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated 100 ChatML examples for SFT.\n",
            "Each example provides input matrices A and B, and expects the full 16-step DSL sequence as the completion.\n",
            "\n",
            "----- Example SFT Data (First ChatML item) -----\n",
            "<|im_start|>system\n",
            "You are an AI assistant. Your ONLY task is to output the COMPLETE sequence of VALID DSL lines to perform a 2x2 matrix multiplication (C = A * B), with each DSL line on a new line.\n",
            "STRICT RULES:\n",
            "1.  Adhere strictly to the DSL formats defined below for each line in the sequence.\n",
            "2.  NO PYTHON code (e.g., 'np.dot', '@', 'np.array', 'loops').\n",
            "3.  NO EXPLANATIONS, apologies, or conversational text. Output only the DSL lines, each on a new line.\n",
            "4.  The generated sequence must correctly implement the standard 2x2 matrix multiplication algorithm as a sequence of elementary operations.\n",
            "    (e.g., C[0,0] = A[0,0]*B[0,0] + A[0,1]*B[1,0] should be broken down into steps like M0=A[0,0]*B[0,0], M1=A[0,1]*B[1,0], S0=M0+M1, C[0,0]=S0, following this pattern for all C elements).\n",
            "    The fine-tuning examples (assistant responses) will demonstrate the specific M-variable and S-variable naming convention (M for products, S for sums).\n",
            "5.  If you cannot determine a valid full DSL sequence based on the rules and input, output ONLY the exact text: Error: Cannot determine full sequence.\n",
            "\n",
            "DSL FORMAT DEFINITIONS:\n",
            "\n",
            "1.  **Intermediate Variable Assignment (M-variables):**\n",
            "    * Purpose: To store intermediate results from arithmetic operations or direct assignments.\n",
            "    * Naming: `<id>` is an integer (e.g., M0, M1, M2, ...). Use the next available or contextually appropriate ID. (The specific M/S convention for matmul will be learned from examples).\n",
            "    * Formats:\n",
            "        * `M<id> = <operand1> + <operand2>`  (Addition)\n",
            "        * `M<id> = <operand1> - <operand2>`  (Subtraction)\n",
            "        * `M<id> = <operand1> * <operand2>`  (Multiplication of operands)\n",
            "        * `M<id> = <operand>`             (Direct assignment)\n",
            "    * Examples:\n",
            "        * `M0 = A[0,0] + B[0,0]`\n",
            "        * `M1 = A[0,1] - M0`\n",
            "        * `M2 = A[0,0] * B[0,0]`\n",
            "        * `M3 = M1`\n",
            "        * `M4 = B[1,0]`\n",
            "\n",
            "2.  **Output Matrix Cell Assignment (C-matrix):**\n",
            "    * Purpose: To assign a computed value to an element of the 2x2 output matrix C.\n",
            "    * Format: `C[<row>,<col>] = <operand>`\n",
            "    * Constraints: `<row>` and `<col>` must be 0 or 1.\n",
            "    * Example: `C[0,1] = M5` (Note: for standard matmul, the operand will typically be an S-variable as learned from examples like S0, S1, etc.)\n",
            "\n",
            "3.  **Operands (`<operand>`, `<operand1>`, `<operand2>`):**\n",
            "    * Can be an element from input matrix A: `A[<row>,<col>]` (e.g., A[0,0], A[1,0])\n",
            "    * Can be an element from input matrix B: `B[<row>,<col>]` (e.g., B[0,1], B[1,1])\n",
            "    * Can be a previously defined intermediate variable: `M<id>` (e.g., M0, M3). (The examples will show S-variables like S0, S1, etc., being used for sums in the matmul context).\n",
            "    * Constraints: All `<row>` and `<col>` indices for A and B must be 0 or 1. Any `M<id>` (or S-variable, which is a type of intermediate variable) used as an operand must have been defined in a previous DSL step in the sequence.\n",
            "\n",
            "You will be given the input matrices A (2x2) and B (2x2).\n",
            "Provide the complete, multi-line DSL sequence to calculate C = A * B.<|im_end|>\n",
            "<|im_start|>user\n",
            "Input matrices:\n",
            "A = [[1,0],[0,1]]\n",
            "B = [[4,1],[-1,-5]]\n",
            "Provide the COMPLETE sequence of DSL steps to calculate C = A * B.<|im_end|>\n",
            "<|im_start|>assistant\n",
            "M0 = A[0,0]*B[0,0]\n",
            "M1 = A[0,1]*B[1,0]\n",
            "S0 = M0+M1\n",
            "C[0,0] = S0\n",
            "M2 = A[0,0]*B[0,1]\n",
            "M3 = A[0,1]*B[1,1]\n",
            "S1 = M2+M3\n",
            "C[0,1] = S1\n",
            "M4 = A[1,0]*B[0,0]\n",
            "M5 = A[1,1]*B[1,0]\n",
            "S2 = M4+M5\n",
            "C[1,0] = S2\n",
            "M6 = A[1,0]*B[0,1]\n",
            "M7 = A[1,1]*B[1,1]\n",
            "S3 = M6+M7\n",
            "C[1,1] = S3<|im_end|>\n",
            "--------------------------------------------------\n",
            "\n",
            "----- Example SFT Data (Second ChatML item, if generated) -----\n",
            "<|im_start|>system\n",
            "You are an AI assistant. Your ONLY task is to output the COMPLETE sequence of VALID DSL lines to perform a 2x2 matrix multiplication (C = A * B), with each DSL line on a new line.\n",
            "STRICT RULES:\n",
            "1.  Adhere strictly to the DSL formats defined below for each line in the sequence.\n",
            "2.  NO PYTHON code (e.g., 'np.dot', '@', 'np.array', 'loops').\n",
            "3.  NO EXPLANATIONS, apologies, or conversational text. Output only the DSL lines, each on a new line.\n",
            "4.  The generated sequence must correctly implement the standard 2x2 matrix multiplication algorithm as a sequence of elementary operations.\n",
            "    (e.g., C[0,0] = A[0,0]*B[0,0] + A[0,1]*B[1,0] should be broken down into steps like M0=A[0,0]*B[0,0], M1=A[0,1]*B[1,0], S0=M0+M1, C[0,0]=S0, following this pattern for all C elements).\n",
            "    The fine-tuning examples (assistant responses) will demonstrate the specific M-variable and S-variable naming convention (M for products, S for sums).\n",
            "5.  If you cannot determine a valid full DSL sequence based on the rules and input, output ONLY the exact text: Error: Cannot determine full sequence.\n",
            "\n",
            "DSL FORMAT DEFINITIONS:\n",
            "\n",
            "1.  **Intermediate Variable Assignment (M-variables):**\n",
            "    * Purpose: To store intermediate results from arithmetic operations or direct assignments.\n",
            "    * Naming: `<id>` is an integer (e.g., M0, M1, M2, ...). Use the next available or contextually appropriate ID. (The specific M/S convention for matmul will be learned from examples).\n",
            "    * Formats:\n",
            "        * `M<id> = <operand1> + <operand2>`  (Addition)\n",
            "        * `M<id> = <operand1> - <operand2>`  (Subtraction)\n",
            "        * `M<id> = <operand1> * <operand2>`  (Multiplication of operands)\n",
            "        * `M<id> = <operand>`             (Direct assignment)\n",
            "    * Examples:\n",
            "        * `M0 = A[0,0] + B[0,0]`\n",
            "        * `M1 = A[0,1] - M0`\n",
            "        * `M2 = A[0,0] * B[0,0]`\n",
            "        * `M3 = M1`\n",
            "        * `M4 = B[1,0]`\n",
            "\n",
            "2.  **Output Matrix Cell Assignment (C-matrix):**\n",
            "    * Purpose: To assign a computed value to an element of the 2x2 output matrix C.\n",
            "    * Format: `C[<row>,<col>] = <operand>`\n",
            "    * Constraints: `<row>` and `<col>` must be 0 or 1.\n",
            "    * Example: `C[0,1] = M5` (Note: for standard matmul, the operand will typically be an S-variable as learned from examples like S0, S1, etc.)\n",
            "\n",
            "3.  **Operands (`<operand>`, `<operand1>`, `<operand2>`):**\n",
            "    * Can be an element from input matrix A: `A[<row>,<col>]` (e.g., A[0,0], A[1,0])\n",
            "    * Can be an element from input matrix B: `B[<row>,<col>]` (e.g., B[0,1], B[1,1])\n",
            "    * Can be a previously defined intermediate variable: `M<id>` (e.g., M0, M3). (The examples will show S-variables like S0, S1, etc., being used for sums in the matmul context).\n",
            "    * Constraints: All `<row>` and `<col>` indices for A and B must be 0 or 1. Any `M<id>` (or S-variable, which is a type of intermediate variable) used as an operand must have been defined in a previous DSL step in the sequence.\n",
            "\n",
            "You will be given the input matrices A (2x2) and B (2x2).\n",
            "Provide the complete, multi-line DSL sequence to calculate C = A * B.<|im_end|>\n",
            "<|im_start|>user\n",
            "Input matrices:\n",
            "A = [[-3,1],[-6,-7]]\n",
            "B = [[1,0],[0,1]]\n",
            "Provide the COMPLETE sequence of DSL steps to calculate C = A * B.<|im_end|>\n",
            "<|im_start|>assistant\n",
            "M0 = A[0,0]*B[0,0]\n",
            "M1 = A[0,1]*B[1,0]\n",
            "S0 = M0+M1\n",
            "C[0,0] = S0\n",
            "M2 = A[0,0]*B[0,1]\n",
            "M3 = A[0,1]*B[1,1]\n",
            "S1 = M2+M3\n",
            "C[0,1] = S1\n",
            "M4 = A[1,0]*B[0,0]\n",
            "M5 = A[1,1]*B[1,0]\n",
            "S2 = M4+M5\n",
            "C[1,0] = S2\n",
            "M6 = A[1,0]*B[0,1]\n",
            "M7 = A[1,1]*B[1,1]\n",
            "S3 = M6+M7\n",
            "C[1,1] = S3<|im_end|>\n",
            "--------------------------------------------------\n",
            "\n",
            "Fine-tuning data saved to dsl_finetune_data_standard_matmul_full_sequence.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================\n",
        "# QWEN DSL Fine-Tuning Full Script (Corrected)\n",
        "# ==============================================\n",
        "# Dependencies (install in your Colab / environment):\n",
        "# !pip install -q datasets accelerate bitsandbytes trl\n",
        "\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM, AutoTokenizer,\n",
        "    BitsAndBytesConfig # TrainingArguments not strictly needed if all args in SFTConfig\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training # PeftModel not used in training script\n",
        "from trl import SFTConfig, SFTTrainer\n",
        "\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 1. (Optional) Mount Google Drive\n",
        "# ------------------------------------------------\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# DRIVE_BASE_PATH = \"/content/drive/MyDrive/Qwen_DSL_FineTune\"\n",
        "# os.makedirs(DRIVE_BASE_PATH, exist_ok=True)\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 2. System Message and Dataset Path\n",
        "# ------------------------------------------------\n",
        "SYSTEM_MESSAGE = (\n",
        "   \"\"\"You are an AI assistant. Your ONLY task is to output the COMPLETE sequence of VALID DSL lines to perform a 2x2 matrix multiplication (C = A * B), with each DSL line on a new line.\n",
        "STRICT RULES:\n",
        "1.  Adhere strictly to the DSL formats defined below for each line in the sequence.\n",
        "2.  NO PYTHON code (e.g., 'np.dot', '@', 'np.array', 'loops').\n",
        "3.  NO EXPLANATIONS, apologies, or conversational text. Output only the DSL lines, each on a new line.\n",
        "4.  The generated sequence must correctly implement the standard 2x2 matrix multiplication algorithm as a sequence of elementary operations.\n",
        "    (e.g., C[0,0] = A[0,0]*B[0,0] + A[0,1]*B[1,0] should be broken down into steps like M0=A[0,0]*B[0,0], M1=A[0,1]*B[1,0], S0=M0+M1, C[0,0]=S0, following this pattern for all C elements).\n",
        "    The fine-tuning examples (assistant responses) will demonstrate the specific M-variable and S-variable naming convention (M for products, S for sums).\n",
        "5.  If you cannot determine a valid full DSL sequence based on the rules and input, output ONLY the exact text: Error: Cannot determine full sequence.\n",
        "\n",
        "DSL FORMAT DEFINITIONS:\n",
        "\n",
        "1.  **Intermediate Variable Assignment (M-variables):**\n",
        "    * Purpose: To store intermediate results from arithmetic operations or direct assignments.\n",
        "    * Naming: `<id>` is an integer (e.g., M0, M1, M2, ...). Use the next available or contextually appropriate ID. (The specific M/S convention for matmul will be learned from examples).\n",
        "    * Formats:\n",
        "        * `M<id> = <operand1> + <operand2>`  (Addition)\n",
        "        * `M<id> = <operand1> - <operand2>`  (Subtraction)\n",
        "        * `M<id> = <operand1> * <operand2>`  (Multiplication of operands)\n",
        "        * `M<id> = <operand>`             (Direct assignment)\n",
        "    * Examples:\n",
        "        * `M0 = A[0,0] + B[0,0]`\n",
        "        * `M1 = A[0,1] - M0`\n",
        "        * `M2 = A[0,0] * B[0,0]`\n",
        "        * `M3 = M1`\n",
        "        * `M4 = B[1,0]`\n",
        "\n",
        "2.  **Output Matrix Cell Assignment (C-matrix):**\n",
        "    * Purpose: To assign a computed value to an element of the 2x2 output matrix C.\n",
        "    * Format: `C[<row>,<col>] = <operand>`\n",
        "    * Constraints: `<row>` and `<col>` must be 0 or 1.\n",
        "    * Example: `C[0,1] = M5` (Note: for standard matmul, the operand will typically be an S-variable as learned from examples like S0, S1, etc.)\n",
        "\n",
        "3.  **Operands (`<operand>`, `<operand1>`, `<operand2>`):**\n",
        "    * Can be an element from input matrix A: `A[<row>,<col>]` (e.g., A[0,0], A[1,0])\n",
        "    * Can be an element from input matrix B: `B[<row>,<col>]` (e.g., B[0,1], B[1,1])\n",
        "    * Can be a previously defined intermediate variable: `M<id>` (e.g., M0, M3). (The examples will show S-variables like S0, S1, etc., being used for sums in the matmul context).\n",
        "    * Constraints: All `<row>` and `<col>` indices for A and B must be 0 or 1. Any `M<id>` (or S-variable, which is a type of intermediate variable) used as an operand must have been defined in a previous DSL step in the sequence.\n",
        "\n",
        "You will be given the input matrices A (2x2) and B (2x2).\n",
        "Provide the complete, multi-line DSL sequence to calculate C = A * B.\"\"\"\n",
        ")\n",
        "\n",
        "# Dataset path (ensure this file exists and is correctly formatted)\n",
        "dataset_path = \"/content/dsl_finetune_data_standard_matmul_full_sequence.jsonl\"\n",
        "if not os.path.exists(dataset_path):\n",
        "  print(f\"Dataset file not found: {dataset_path}. Please ensure it exists.\")\n",
        "  # Consider exiting or handling this error more robustly if the file is essential\n",
        "  # exit()\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 3. Load Dataset\n",
        "# ------------------------------------------------\n",
        "dataset = load_dataset(\"json\", data_files=dataset_path, split=\"train\")\n",
        "print(f\"Loaded {len(dataset)} examples from {dataset_path}\")\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 4. Model & Tokenizer Setup\n",
        "# ------------------------------------------------\n",
        "model_name = \"Qwen/Qwen1.5-0.5B-Chat\"\n",
        "\n",
        "# Quantization config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# Model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 5. LoRA Configuration & PEFT Wrapping\n",
        "# ------------------------------------------------\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 6. Sanity Check: Ensure target_modules exist\n",
        "# ------------------------------------------------\n",
        "print(\"\\nValidating target module names:\")\n",
        "targets = lora_config.target_modules\n",
        "leafs = {name.split('.')[-1] for name, _ in model.named_modules()}\n",
        "for t in targets:\n",
        "    print(f\"{t:10s} → {('FOUND' if t in leafs else 'MISSING')}\")\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 7. Training Configuration and Trainer Initialization\n",
        "# ------------------------------------------------\n",
        "output_dir = \"./qwen_dsl_finetuned_adapter\" # Base directory for all outputs\n",
        "\n",
        "# SFTConfig inherits from TrainingArguments, so all parameters are set here.\n",
        "sft_args = SFTConfig(\n",
        "    # SFT specific parameters\n",
        "    dataset_text_field=\"text\",         # Field in your dataset that contains the ChatML string\n",
        "    max_seq_length=1500,                # Max sequence length (corrected from max_length)\n",
        "    packing=False,                     # Whether to pack multiple short examples if True\n",
        "\n",
        "    # Standard TrainingArguments parameters integrated here\n",
        "    output_dir=output_dir,             # Trainer will save checkpoints, logs etc. here\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=2,     # Effective batch size = 2 * 2 = 4\n",
        "    learning_rate=2e-4,\n",
        "    num_train_epochs=5,                # Increased epochs for small dataset (e.g., 25 examples). Adjust as needed.\n",
        "    logging_steps=10,                  # Log training progress every 10 steps\n",
        "    save_strategy=\"epoch\",             # Save a checkpoint at the end of each epoch\n",
        "    bf16=True,                         # Use bfloat16 precision (ensure GPU supports it)\n",
        "    # fp16=False,                      # If bf16 not supported, set fp16=True\n",
        "    optim=\"paged_adamw_8bit\",          # Optimizer\n",
        "    # report_to=\"none\",                # Already handled by os.environ[\"WANDB_DISABLED\"]\n",
        "    # Add other relevant TrainingArguments as needed:\n",
        "    # weight_decay=0.01,\n",
        "    # lr_scheduler_type=\"cosine\",\n",
        "    # warmup_steps=50,\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    processing_class=tokenizer,                # Pass the tokenizer instance directly\n",
        "    args=sft_args,                      # Pass the comprehensive SFTConfig object\n",
        "    train_dataset=dataset,\n",
        "    peft_config=lora_config,\n",
        "    # dataset_text_field, max_seq_length, packing are sourced from sft_args now\n",
        ")\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 8. Start Fine-Tuning\n",
        "# ------------------------------------------------\n",
        "print(\"Starting fine-tuning with effective arguments...\")\n",
        "trainer.train() # Single, effective train call\n",
        "print(\"Fine-tuning completed.\")\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 9. Save LoRA Adapter & Tokenizer\n",
        "# ------------------------------------------------\n",
        "# The adapter will be saved in a subdirectory of output_dir (e.g. output_dir/checkpoint-xxx or output_dir if save_strategy=\"no\")\n",
        "# To save the final adapter explicitly to a desired path after training:\n",
        "adapter_path = os.path.join(output_dir, \"final_adapter\")\n",
        "os.makedirs(adapter_path, exist_ok=True) # Ensure the directory exists\n",
        "trainer.save_model(adapter_path)\n",
        "tokenizer.save_pretrained(adapter_path)\n",
        "print(f\"Final adapter and tokenizer saved at {adapter_path}\")\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 10. (Optional) Test the Fine-Tuned Model (after training and saving)\n",
        "# ------------------------------------------------\n",
        "def test_model(prompt: str, trained_model, trained_tokenizer): # Pass the trained model and tokenizer\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_MESSAGE},\n",
        "        {\"role\": \"user\",   \"content\": prompt},\n",
        "    ]\n",
        "    text_in = trained_tokenizer.apply_chat_template(\n",
        "        messages, tokenize=False, add_generation_prompt=True\n",
        "    )\n",
        "    inputs = trained_tokenizer([text_in], return_tensors=\"pt\").to(trained_model.device)\n",
        "    with torch.no_grad():\n",
        "        out = trained_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=1500, # Increased from 25 to accommodate full DSL sequence\n",
        "            do_sample=False,\n",
        "            pad_token_id=trained_tokenizer.eos_token_id,\n",
        "            eos_token_id=trained_tokenizer.eos_token_id\n",
        "        )\n",
        "    gen_tokens = out[0][inputs[\"input_ids\"].shape[-1]:]\n",
        "    return trained_tokenizer.decode(gen_tokens, skip_special_tokens=True).strip()\n",
        "\n",
        "# To use the test_model function after training, you would typically load the saved adapter:\n",
        "# from peft import PeftModel\n",
        "#\n",
        "# print(\"\\n--- Testing the fine-tuned model ---\")\n",
        "# # Load the base model again (quantized)\n",
        "# base_model_for_test = AutoModelForCausalLM.from_pretrained(\n",
        "# model_name,\n",
        "# quantization_config=bnb_config,\n",
        "# device_map=\"auto\",\n",
        "# trust_remote_code=True\n",
        "# )\n",
        "# # Load the PeftModel with the adapter\n",
        "# loaded_model = PeftModel.from_pretrained(base_model_for_test, adapter_path)\n",
        "# loaded_model.eval() # Set to evaluation mode\n",
        "#\n",
        "# loaded_tokenizer = AutoTokenizer.from_pretrained(adapter_path) # Load tokenizer from adapter path\n",
        "#\n",
        "# test_prompt_example = (\n",
        "#     \"Input matrices:\\n\"\n",
        "#     \"A = [[1,2],[3,4]]\\n\"\n",
        "#     \"B = [[5,6],[7,8]]\\n\"\n",
        "#     \"Provide the COMPLETE sequence of DSL steps to calculate C = A * B.\"\n",
        "# )\n",
        "# response = test_model(test_prompt_example, loaded_model, loaded_tokenizer)\n",
        "# print(f\"Test Prompt:\\n{test_prompt_example}\")\n",
        "# print(f\"\\nModel Response:\\n{response}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "7e735713c3984c69be2c041ba73d39e9",
            "6cb2e50921e7447e89eb98d1e2f3f9c3",
            "eed0f7f2590f45bbae487a0865b6f2c0",
            "b80ad5e11469408b84d318d692aa75a5",
            "dd07033057144a2cb07e19857e65592e",
            "f366de9a2f3043b3b82513bde1ab3e48",
            "bdee79ee09a942eb8b8e312dc72b8e3e",
            "3bf0081155244275be751e34c0ab7b1e",
            "8845ba76fa8a4985b942370a10acacfc",
            "b9e5848616304fd383309cd5e4a8ae30",
            "bd728393791c455180e2ff318364e060",
            "64bc88e0e13e49b19076817704184fe4",
            "8c0b651f64bb497f8c10302aef7572e4",
            "ba27c7a704274be59de6e6177be5b2de",
            "67d0f23567264882841e77ae6bf79dca",
            "1d6d8c2021b54920af72755da7b912c9",
            "9b8ab05ae3b946dd8bb055b961f9f607",
            "b72c4d62bb964543a45090e73fd7b24e",
            "85aff509efff4be5ab1d1b1dd7857721",
            "2f3bb4d07f504356833e1f4370561da9",
            "8c76ca220b36414b8ae8ae0b2ff681a4",
            "ee15ad838d72465a9fa9c3028a163c9c",
            "64e4469a06fb4a91b1274965ddeaee35",
            "782f6a89987740d9942decd42b0200c1",
            "f16058a451d741a8a4ff986aa464842e",
            "cfa2893b1e6948b89bd762bc44e9c7c4",
            "588a4aa7b74b4f91a4823cba21be8866",
            "b275c7b92bf6460cae23817bb0a4941b",
            "c58414fc50ec4cc48ea02f47823cd23c",
            "fdebd8916b64440394f523aac014ded9",
            "d09069c89a164482a7f9cafc7f8932ab",
            "bdfdff265e184d7e8de519b9ab16beec",
            "5c6f7c0da7e547e7a1b767fe684d3445",
            "67833c3d4ef348abbb9a80ad7396dcb5",
            "d9da1eb7ed9e4779b7516b87329298f7",
            "7ecfc9d360514d6f992ca56b61e684a2",
            "e8cfd6e6fff04981aad6e7773c9dc3fb",
            "48d1859165884085bf8ec867aa7a7ca8",
            "cc12a5058f32465da60aaad3ea7c7cc1",
            "55300839c10a413ba6915aa7d08153fb",
            "a12859cb0c3642279df729f623af6207",
            "ff2eb0a63234430c9768e720e0d4ed9f",
            "6ee8aa63e095408d8b22dce9a52cc519",
            "502220cc7d8b466aa4617ee0f13cb1de",
            "7455f38d60d647a0afda0c910b88daad",
            "dd5e2af6eb2e48a3bdcb580aadc49ed9",
            "3d7d6ffb80d747488197e8014f4d7ce4",
            "435d4cabc63d4ce587a1d8590b793359",
            "a81ce26c129d476591af5e2dc34d1730",
            "3904bb68392149029b77d4e2c766620a",
            "126bf658920c4afaa81620216286b4af",
            "29652c3442b0405cb09f23171f1af53e",
            "22e35192e85e42028e0aa9b647c9f5fc",
            "b7b80513044f4ada9f55578ab32697fd",
            "dd63de0f24f948c5bb9ad9e537f96e40"
          ]
        },
        "id": "02ACo-uVepdo",
        "outputId": "c273699e-b1a0-45ff-fc40-3a369f4670a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7e735713c3984c69be2c041ba73d39e9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 100 examples from /content/dsl_finetune_data_standard_matmul_full_sequence.jsonl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 7,569,408 || all params: 471,557,120 || trainable%: 1.6052\n",
            "\n",
            "Validating target module names:\n",
            "v_proj     → FOUND\n",
            "up_proj    → FOUND\n",
            "q_proj     → FOUND\n",
            "gate_proj  → FOUND\n",
            "o_proj     → FOUND\n",
            "down_proj  → FOUND\n",
            "k_proj     → FOUND\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Converting train dataset to ChatML:   0%|          | 0/100 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "64bc88e0e13e49b19076817704184fe4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Adding EOS to train dataset:   0%|          | 0/100 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "64e4469a06fb4a91b1274965ddeaee35"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizing train dataset:   0%|          | 0/100 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "67833c3d4ef348abbb9a80ad7396dcb5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Truncating train dataset:   0%|          | 0/100 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7455f38d60d647a0afda0c910b88daad"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting fine-tuning with effective arguments...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 17:58, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.457800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.339600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.047300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.025300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.023700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.022500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.022200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.021900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.021800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.021700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.021500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.021600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tuning completed.\n",
            "Final adapter and tokenizer saved at ./qwen_dsl_finetuned_adapter/final_adapter\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " import os\n",
        "import json\n",
        "import torch\n",
        "import random\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM, AutoTokenizer,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from peft import PeftModel\n",
        "\n",
        "# Ensure WANDB is disabled if you had it enabled previously for training\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "# ------------------------------------------------\n",
        "# Configuration (Adjust these paths if necessary)\n",
        "# ------------------------------------------------\n",
        "BASE_MODEL_NAME = \"Qwen/Qwen1.5-0.5B-Chat\"\n",
        "# This is the directory where trainer.save_model() saved the adapter\n",
        "ADAPTER_PATH = \"./qwen_dsl_finetuned_adapter/final_adapter\" # Output from step 9\n",
        "\n",
        "# ------------------------------------------------\n",
        "# System Message (MUST be IDENTICAL to the one used for fine-tuning)\n",
        "# ------------------------------------------------\n",
        "SYSTEM_MESSAGE = (\n",
        "   \"\"\"You are an AI assistant. Your ONLY task is to output the COMPLETE sequence of VALID DSL lines to perform a 2x2 matrix multiplication (C = A * B), with each DSL line on a new line.\n",
        "STRICT RULES:\n",
        "1.  Adhere strictly to the DSL formats defined below for each line in the sequence.\n",
        "2.  NO PYTHON code (e.g., 'np.dot', '@', 'np.array', 'loops').\n",
        "3.  NO EXPLANATIONS, apologies, or conversational text. Output only the DSL lines, each on a new line.\n",
        "4.  The generated sequence must correctly implement the standard 2x2 matrix multiplication algorithm as a sequence of elementary operations.\n",
        "    (e.g., C[0,0] = A[0,0]*B[0,0] + A[0,1]*B[1,0] should be broken down into steps like M0=A[0,0]*B[0,0], M1=A[0,1]*B[1,0], S0=M0+M1, C[0,0]=S0, following this pattern for all C elements).\n",
        "    The fine-tuning examples (assistant responses) will demonstrate the specific M-variable and S-variable naming convention (M for products, S for sums).\n",
        "5.  If you cannot determine a valid full DSL sequence based on the rules and input, output ONLY the exact text: Error: Cannot determine full sequence.\n",
        "\n",
        "DSL FORMAT DEFINITIONS:\n",
        "\n",
        "1.  **Intermediate Variable Assignment (M-variables):**\n",
        "    * Purpose: To store intermediate results from arithmetic operations or direct assignments.\n",
        "    * Naming: `<id>` is an integer (e.g., M0, M1, M2, ...). Use the next available or contextually appropriate ID. (The specific M/S convention for matmul will be learned from examples).\n",
        "    * Formats:\n",
        "        * `M<id> = <operand1> + <operand2>`  (Addition)\n",
        "        * `M<id> = <operand1> - <operand2>`  (Subtraction)\n",
        "        * `M<id> = <operand1> * <operand2>`  (Multiplication of operands)\n",
        "        * `M<id> = <operand>`             (Direct assignment)\n",
        "    * Examples:\n",
        "        * `M0 = A[0,0] + B[0,0]`\n",
        "        * `M1 = A[0,1] - M0`\n",
        "        * `M2 = A[0,0] * B[0,0]`\n",
        "        * `M3 = M1`\n",
        "        * `M4 = B[1,0]`\n",
        "\n",
        "2.  **Output Matrix Cell Assignment (C-matrix):**\n",
        "    * Purpose: To assign a computed value to an element of the 2x2 output matrix C.\n",
        "    * Format: `C[<row>,<col>] = <operand>`\n",
        "    * Constraints: `<row>` and `<col>` must be 0 or 1.\n",
        "    * Example: `C[0,1] = M5` (Note: for standard matmul, the operand will typically be an S-variable as learned from examples like S0, S1, etc.)\n",
        "\n",
        "3.  **Operands (`<operand>`, `<operand1>`, `<operand2>`):**\n",
        "    * Can be an element from input matrix A: `A[<row>,<col>]` (e.g., A[0,0], A[1,0])\n",
        "    * Can be an element from input matrix B: `B[<row>,<col>]` (e.g., B[0,1], B[1,1])\n",
        "    * Can be a previously defined intermediate variable: `M<id>` (e.g., M0, M3). (The examples will show S-variables like S0, S1, etc., being used for sums in the matmul context).\n",
        "    * Constraints: All `<row>` and `<col>` indices for A and B must be 0 or 1. Any `M<id>` (or S-variable, which is a type of intermediate variable) used as an operand must have been defined in a previous DSL step in the sequence.\n",
        "\n",
        "You will be given the input matrices A (2x2) and B (2x2).\n",
        "Provide the complete, multi-line DSL sequence to calculate C = A * B.\"\"\"\n",
        ")\n",
        "\n",
        "# ------------------------------------------------\n",
        "# Helper functions from your data generation script\n",
        "# ------------------------------------------------\n",
        "def get_standard_ops_definition():\n",
        "    \"\"\"\n",
        "    Defines the 16 DSL steps for a standard 2x2 matrix multiplication.\n",
        "    \"\"\"\n",
        "    ops = [\n",
        "        {'dsl': \"M0 = A[0,0]*B[0,0]\"}, {'dsl': \"M1 = A[0,1]*B[1,0]\"},\n",
        "        {'dsl': \"S0 = M0+M1\"},         {'dsl': \"C[0,0] = S0\"},\n",
        "        {'dsl': \"M2 = A[0,0]*B[0,1]\"}, {'dsl': \"M3 = A[0,1]*B[1,1]\"},\n",
        "        {'dsl': \"S1 = M2+M3\"},         {'dsl': \"C[0,1] = S1\"},\n",
        "        {'dsl': \"M4 = A[1,0]*B[0,0]\"}, {'dsl': \"M5 = A[1,1]*B[1,0]\"},\n",
        "        {'dsl': \"S2 = M4+M5\"},         {'dsl': \"C[1,0] = S2\"},\n",
        "        {'dsl': \"M6 = A[1,0]*B[0,1]\"}, {'dsl': \"M7 = A[1,1]*B[1,1]\"},\n",
        "        {'dsl': \"S3 = M6+M7\"},         {'dsl': \"C[1,1] = S3\"},\n",
        "    ]\n",
        "    return ops\n",
        "\n",
        "def generate_random_matrix():\n",
        "    \"\"\"Generates a 2x2 matrix with random integer elements between -9 and 9.\"\"\"\n",
        "    return [[random.randint(-9, 9) for _ in range(2)] for _ in range(2)]\n",
        "\n",
        "EXPECTED_DSL_SEQUENCE = \"\\n\".join([op['dsl'] for op in get_standard_ops_definition()])\n",
        "\n",
        "# ------------------------------------------------\n",
        "# Load Fine-Tuned Model for Inference\n",
        "# ------------------------------------------------\n",
        "def load_model_for_inference(base_model_name, adapter_path):\n",
        "    print(f\"Loading base model: {base_model_name}\")\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True, # Or 8bit if you trained with that\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "    )\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model_name,\n",
        "        quantization_config=bnb_config, # Use same quantization as training for base\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "    print(f\"Loading adapter from: {adapter_path}\")\n",
        "    # Load the PEFT model by specifying the adapter path\n",
        "    # The `is_trainable=False` is good practice for inference.\n",
        "    model = PeftModel.from_pretrained(model, adapter_path, is_trainable=False)\n",
        "\n",
        "    print(\"Merging adapter into the base model...\")\n",
        "    # Merge LoRA layers for faster inference.\n",
        "    # If you run out of memory, you can skip this and use the PeftModel directly,\n",
        "    # but inference will be slower.\n",
        "    model = model.merge_and_unload()\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(adapter_path, trust_remote_code=True)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token # Or tokenizer.unk_token\n",
        "    tokenizer.padding_side = \"right\" # Important for generation\n",
        "\n",
        "    print(\"Model and tokenizer loaded successfully.\")\n",
        "    return model, tokenizer\n",
        "\n",
        "# ------------------------------------------------\n",
        "# Generate Model Response\n",
        "# ------------------------------------------------\n",
        "def get_model_response(model, tokenizer, user_prompt_content):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_MESSAGE},\n",
        "        {\"role\": \"user\", \"content\": user_prompt_content},\n",
        "    ]\n",
        "\n",
        "    # Apply chat template\n",
        "    text_input = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True # Crucial for Qwen and other instruction-tuned models\n",
        "    )\n",
        "\n",
        "    inputs = tokenizer([text_input], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # Generation parameters\n",
        "    # max_new_tokens needs to be sufficient for the 16 lines of DSL.\n",
        "    # 16 lines * ~20 chars/line = 320. Add some buffer.\n",
        "    # Using do_sample=False for deterministic output, good for testing exact match.\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=2000, # Increased for full sequence\n",
        "            do_sample=False,    # For deterministic output\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id # Model should stop after generating its response\n",
        "        )\n",
        "\n",
        "    # Decode only the generated part\n",
        "    generated_tokens = outputs[0][inputs[\"input_ids\"].shape[-1]:]\n",
        "    response_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "    return response_text.strip()\n",
        "\n",
        "# ------------------------------------------------\n",
        "# Main Testing Logic\n",
        "# ------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    if not os.path.exists(ADAPTER_PATH):\n",
        "        print(f\"ERROR: Adapter path not found: {ADAPTER_PATH}\")\n",
        "        print(\"Please ensure you have trained the model and the adapter is saved correctly.\")\n",
        "        exit(1)\n",
        "\n",
        "    model, tokenizer = load_model_for_inference(BASE_MODEL_NAME, ADAPTER_PATH)\n",
        "    model.eval() # Set model to evaluation mode\n",
        "\n",
        "    num_test_cases = 10\n",
        "    correct_predictions = 0\n",
        "\n",
        "    print(f\"\\n--- Starting {num_test_cases} Test Cases ---\")\n",
        "    print(f\"Expected DSL Sequence (all test cases):\\n{EXPECTED_DSL_SEQUENCE}\\n\")\n",
        "\n",
        "    for i in range(num_test_cases):\n",
        "        print(f\"--- Test Case {i+1}/{num_test_cases} ---\")\n",
        "\n",
        "        # Generate random matrices for the prompt\n",
        "        matrix_a = generate_random_matrix()\n",
        "        matrix_b = generate_random_matrix()\n",
        "\n",
        "        matrix_a_str = str(matrix_a).replace(\" \", \"\")\n",
        "        matrix_b_str = str(matrix_b).replace(\" \", \"\")\n",
        "\n",
        "        user_prompt = (\n",
        "            f\"Input matrices:\\n\"\n",
        "            f\"A = {matrix_a_str}\\n\"\n",
        "            f\"B = {matrix_b_str}\\n\"\n",
        "            f\"Provide the COMPLETE sequence of DSL steps to calculate C = A * B.\"\n",
        "        )\n",
        "        print(f\"User Prompt (content part):\\n{user_prompt}\")\n",
        "\n",
        "        generated_dsl = get_model_response(model, tokenizer, user_prompt)\n",
        "\n",
        "        print(f\"\\nModel's Generated DSL:\\n{generated_dsl}\")\n",
        "\n",
        "        # Normalize whitespace for comparison (e.g., handle potential trailing newlines)\n",
        "        normalized_generated = \"\\n\".join([line.strip() for line in generated_dsl.strip().split('\\n')])\n",
        "        normalized_expected = \"\\n\".join([line.strip() for line in EXPECTED_DSL_SEQUENCE.strip().split('\\n')])\n",
        "\n",
        "        if normalized_generated == normalized_expected:\n",
        "            print(\"\\nResult: CORRECT\")\n",
        "            correct_predictions += 1\n",
        "        else:\n",
        "            print(\"\\nResult: INCORRECT\")\n",
        "            # For detailed diff, you could use difflib\n",
        "            # import difflib\n",
        "            # diff = difflib.unified_diff(\n",
        "            #     normalized_expected.splitlines(keepends=True),\n",
        "            #     normalized_generated.splitlines(keepends=True),\n",
        "            #     fromfile='expected',\n",
        "            #     tofile='generated',\n",
        "            # )\n",
        "            # print(\"Diff:\")\n",
        "            # print(''.join(diff))\n",
        "            if len(normalized_generated.split('\\n')) != len(normalized_expected.split('\\n')):\n",
        "                print(f\"\"\"Length Mismatch: Expected {len(normalized_expected.split('     '))} lines, Got {len(normalized_generated.split('     '))} lines.\"\"\")\n",
        "\n",
        "\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "    accuracy = (correct_predictions / num_test_cases) * 100\n",
        "    print(f\"\\n--- Test Summary ---\")\n",
        "    print(f\"Total Test Cases: {num_test_cases}\")\n",
        "    print(f\"Correct Predictions: {correct_predictions}\")\n",
        "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "    # Clean up model from GPU memory if necessary\n",
        "    del model\n",
        "    del tokenizer\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F98ecVNCg33S",
        "outputId": "e0c160fc-a20a-4f91-b378-339c2782b022"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading base model: Qwen/Qwen1.5-0.5B-Chat\n",
            "Loading adapter from: ./qwen_dsl_finetuned_adapter/final_adapter\n",
            "Merging adapter into the base model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py:336: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
            "  warnings.warn(\n",
            "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model and tokenizer loaded successfully.\n",
            "\n",
            "--- Starting 10 Test Cases ---\n",
            "Expected DSL Sequence (all test cases):\n",
            "M0 = A[0,0]*B[0,0]\n",
            "M1 = A[0,1]*B[1,0]\n",
            "S0 = M0+M1\n",
            "C[0,0] = S0\n",
            "M2 = A[0,0]*B[0,1]\n",
            "M3 = A[0,1]*B[1,1]\n",
            "S1 = M2+M3\n",
            "C[0,1] = S1\n",
            "M4 = A[1,0]*B[0,0]\n",
            "M5 = A[1,1]*B[1,0]\n",
            "S2 = M4+M5\n",
            "C[1,0] = S2\n",
            "M6 = A[1,0]*B[0,1]\n",
            "M7 = A[1,1]*B[1,1]\n",
            "S3 = M6+M7\n",
            "C[1,1] = S3\n",
            "\n",
            "--- Test Case 1/10 ---\n",
            "User Prompt (content part):\n",
            "Input matrices:\n",
            "A = [[-6,-9],[-1,-2]]\n",
            "B = [[-2,-5],[-6,8]]\n",
            "Provide the COMPLETE sequence of DSL steps to calculate C = A * B.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model's Generated DSL:\n",
            "Here's the complete sequence of DSL steps to calculate C = A * B:\n",
            "```\n",
            "M0 = A[0,0] + B[0,0]\n",
            "M1 = A[0,1] - M0\n",
            "M2 = A[0,0] * B[0,0]\n",
            "M3 = M1\n",
            "S0 = M0 + M1\n",
            "C = M0 * M1\n",
            "```\n",
            "\n",
            "Explanation:\n",
            "\n",
            "  * `M0 = A[0,0] + B[0,0]`: This line calculates the sum of the first two elements of the input matrices A and B.\n",
            "  * `M1 = A[0,1] - M0`: This line calculates the difference between the second and third elements of the input matrices A and B.\n",
            "  * `M2 = A[0,0] * B[0,0]:` This line calculates the product of the first and third elements of the input matrices A and B.\n",
            "  * `M3 = M1`: This line calculates the final result of the calculation, which is the computed value of A * B.\n",
            "\n",
            "Note that the values returned by these steps are not necessarily the same as the actual values calculated by the matmul operation. However, they represent the computed value of A * B.\n",
            "\n",
            "Result: INCORRECT\n",
            "Length Mismatch: Expected 1 lines, Got 1 lines.\n",
            "------------------------------\n",
            "--- Test Case 2/10 ---\n",
            "User Prompt (content part):\n",
            "Input matrices:\n",
            "A = [[-7,9],[4,-8]]\n",
            "B = [[-9,-7],[-3,-2]]\n",
            "Provide the COMPLETE sequence of DSL steps to calculate C = A * B.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model's Generated DSL:\n",
            "Here's the complete sequence of DSL steps to calculate C = A * B:\n",
            "```\n",
            "M0 = A[0,0] + B[0,0]\n",
            "M1 = A[0,1] - M0\n",
            "M2 = A[0,0] * B[0,0]\n",
            "M3 = M1\n",
            "S0 = M0 + M1\n",
            "C = M0 * M1\n",
            "```\n",
            "\n",
            "Explanation:\n",
            "\n",
            "  * `M0 = A[0,0] + B[0,0]`: This line calculates the sum of the first two columns of A and the second column of B.\n",
            "  * `M1 = A[0,1] - M0`: This line calculates the difference between the first two columns of A and the second column of B.\n",
            "  * `M2 = A[0,0] * B[0,0]:` This line calculates the product of the first two columns of A and the second column of B.\n",
            "  * `M3 = M1`: This line assigns the result of the previous step to the current element of the output matrix C.\n",
            "  * `S0 = M0 + M1`: This line adds the values of M0 and M1 together to get the final value of C.\n",
            "  * `C = M0 * M1`: This line combines the values of M0 and M1 to get the final value of C.\n",
            "\n",
            "Note that the values of M0 and M1 are determined by the input matrices A and B, respectively. These values can be adjusted as needed to match your specific use case.\n",
            "\n",
            "Result: INCORRECT\n",
            "Length Mismatch: Expected 1 lines, Got 1 lines.\n",
            "------------------------------\n",
            "--- Test Case 3/10 ---\n",
            "User Prompt (content part):\n",
            "Input matrices:\n",
            "A = [[7,-9],[8,-3]]\n",
            "B = [[8,4],[-2,5]]\n",
            "Provide the COMPLETE sequence of DSL steps to calculate C = A * B.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model's Generated DSL:\n",
            "Here's the complete sequence of DSL steps to calculate C = A * B:\n",
            "```\n",
            "M0 = A[0,0] + B[0,0]\n",
            "M1 = A[0,1] - M0\n",
            "M2 = A[0,0] * B[0,0]\n",
            "M3 = M1\n",
            "S0 = M0 + M1\n",
            "C = M0 + M1\n",
            "```\n",
            "\n",
            "Explanation:\n",
            "\n",
            "  * `M0 = A[0,0] + B[0,0]`: This line calculates the sum of the first two elements of the input matrices.\n",
            "  * `M1 = A[0,1] - M0`: This line calculates the difference between the second and third elements of the input matrices.\n",
            "  * `M2 = A[0,0] * B[0,0]:` This line calculates the product of the first and third elements of the input matrices.\n",
            "  * `M3 = M1`: This line calculates the final result of the calculation.\n",
            "  * `S0 = M0 + M1`: This line adds the values of `M0` and `M1` together to get the final result.\n",
            "  * `C = M0 + M1`: This line combines the values of `M0` and `M1` to get the final result.\n",
            "\n",
            "Note that the values of `M0` and `M1` are not explicitly defined in the input matrices. However, they can be calculated using the same logic as before.\n",
            "\n",
            "Result: INCORRECT\n",
            "Length Mismatch: Expected 1 lines, Got 1 lines.\n",
            "------------------------------\n",
            "--- Test Case 4/10 ---\n",
            "User Prompt (content part):\n",
            "Input matrices:\n",
            "A = [[9,-1],[-9,-4]]\n",
            "B = [[4,1],[-1,-5]]\n",
            "Provide the COMPLETE sequence of DSL steps to calculate C = A * B.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model's Generated DSL:\n",
            "Here's the complete sequence of DSL steps to calculate C = A * B:\n",
            "```\n",
            "M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 =\n",
            "\n",
            "Result: INCORRECT\n",
            "Length Mismatch: Expected 1 lines, Got 1 lines.\n",
            "------------------------------\n",
            "--- Test Case 5/10 ---\n",
            "User Prompt (content part):\n",
            "Input matrices:\n",
            "A = [[-3,1],[-6,-7]]\n",
            "B = [[3,-6],[2,2]]\n",
            "Provide the COMPLETE sequence of DSL steps to calculate C = A * B.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model's Generated DSL:\n",
            "Here's the complete sequence of DSL steps to calculate C = A * B:\n",
            "```\n",
            "M0 = A[0,0] + B[0,0]\n",
            "M1 = A[0,1] - M0\n",
            "M2 = A[0,0] * B[0,0]\n",
            "M3 = M1\n",
            "S0 = M0 + M1\n",
            "C = M0 * M1\n",
            "```\n",
            "\n",
            "Explanation:\n",
            "\n",
            "  * `M0 = A[0,0] + B[0,0]`: This line calculates the sum of the first two columns of A and the second column of B.\n",
            "  * `M1 = A[0,1] - M0`: This line calculates the difference between the first two columns of A and the second column of B.\n",
            "  * `M2 = A[0,0] * B[0,0]:` This line calculates the product of the first two columns of A and the second column of B.\n",
            "  * `M3 = M1`: This line assigns the result of the previous step to the current element of the output matrix C.\n",
            "  * `S0 = M0 + M1`: This line adds the values of M0 and M1 together to get the final value of C.\n",
            "  * `C = M0 * M1`: This line combines the values of M0 and M1 to get the final value of C.\n",
            "\n",
            "Note that the values of M0 and M1 are determined by the input matrices A and B, respectively. These values can be adjusted as needed to match your specific use case.\n",
            "\n",
            "Result: INCORRECT\n",
            "Length Mismatch: Expected 1 lines, Got 1 lines.\n",
            "------------------------------\n",
            "--- Test Case 6/10 ---\n",
            "User Prompt (content part):\n",
            "Input matrices:\n",
            "A = [[-1,-8],[5,8]]\n",
            "B = [[-6,3],[-7,8]]\n",
            "Provide the COMPLETE sequence of DSL steps to calculate C = A * B.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model's Generated DSL:\n",
            "Here's the complete sequence of DSL steps to calculate C = A * B:\n",
            "```\n",
            "M0 = A[0,0] + B[0,0]\n",
            "M1 = A[0,1] - M0\n",
            "M2 = A[0,0] * B[0,0]\n",
            "M3 = M1\n",
            "S0 = M0 + M1\n",
            "C = M0 * M1\n",
            "```\n",
            "\n",
            "Explanation:\n",
            "\n",
            "  * `M0 = A[0,0] + B[0,0]`: This line calculates the sum of the first two columns of A and the second column of B.\n",
            "  * `M1 = A[0,1] - M0`: This line calculates the difference between the first two columns of A and the second column of B.\n",
            "  * `M2 = A[0,0] * B[0,0]:` This line calculates the product of the first two columns of A and the second column of B.\n",
            "  * `M3 = M1`: This line assigns the result of the previous step to the current element of the output matrix C.\n",
            "  * `S0 = M0 + M1`: This line adds the values of M0 and M1 together to get the final value of C.\n",
            "  * `C = M0 * M1`: This line combines the values of M0 and M1 to get the final value of C.\n",
            "\n",
            "Note that the values of M0 and M1 are determined by the input matrices A and B, respectively. These values can be adjusted as needed to match your specific use case.\n",
            "\n",
            "Result: INCORRECT\n",
            "Length Mismatch: Expected 1 lines, Got 1 lines.\n",
            "------------------------------\n",
            "--- Test Case 7/10 ---\n",
            "User Prompt (content part):\n",
            "Input matrices:\n",
            "A = [[0,2],[9,-3]]\n",
            "B = [[-7,-8],[-2,0]]\n",
            "Provide the COMPLETE sequence of DSL steps to calculate C = A * B.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model's Generated DSL:\n",
            "Here's the complete sequence of DSL steps to calculate C = A * B:\n",
            "```\n",
            "M0 = A[0,0] + B[0,0]\n",
            "M1 = A[0,1] - M0\n",
            "M2 = A[0,0] * B[0,0]\n",
            "M3 = M1\n",
            "S0 = M0 + M1\n",
            "C = M0 + M1\n",
            "```\n",
            "\n",
            "Explanation:\n",
            "\n",
            "  * `M0 = A[0,0] + B[0,0]`: This line calculates the sum of the first two elements of the input matrices A and B.\n",
            "  * `M1 = A[0,1] - M0`: This line calculates the difference between the second and third elements of the input matrices A and B.\n",
            "  * `M2 = A[0,0] * B[0,0]:` This line calculates the product of the first and third elements of the input matrices A and B.\n",
            "  * `M3 = M1`: This line calculates the final result of the calculation, which is the computed value of A * B.\n",
            "\n",
            "Note that the values returned by these steps are not necessarily the same as the actual values calculated by the matmul operation. However, they represent the computed value of A * B.\n",
            "\n",
            "Result: INCORRECT\n",
            "Length Mismatch: Expected 1 lines, Got 1 lines.\n",
            "------------------------------\n",
            "--- Test Case 8/10 ---\n",
            "User Prompt (content part):\n",
            "Input matrices:\n",
            "A = [[-7,-2],[-6,3]]\n",
            "B = [[-1,5],[2,-4]]\n",
            "Provide the COMPLETE sequence of DSL steps to calculate C = A * B.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model's Generated DSL:\n",
            "Here's the complete sequence of DSL steps to calculate C = A * B:\n",
            "```\n",
            "M0 = A[0,0] + B[0,0]\n",
            "M1 = A[0,1] - M0\n",
            "M2 = A[0,0] * B[0,0]\n",
            "M3 = M1\n",
            "S0 = M0 + M1\n",
            "C = M0 * M1\n",
            "```\n",
            "\n",
            "Explanation:\n",
            "\n",
            "  * `M0 = A[0,0] + B[0,0]`: This line calculates the sum of the first two elements of the input matrices A and B.\n",
            "  * `M1 = A[0,1] - M0`: This line calculates the difference between the second and third elements of the input matrices A and B.\n",
            "  * `M2 = A[0,0] * B[0,0]:` This line calculates the product of the first and third elements of the input matrices A and B.\n",
            "  * `M3 = M1`: This line calculates the final result of the calculation, which is the computed value of A * B.\n",
            "\n",
            "Note that the values returned by these steps are not necessarily the same as the actual values calculated by the matmul operation. However, they represent the computed value of A * B.\n",
            "\n",
            "Result: INCORRECT\n",
            "Length Mismatch: Expected 1 lines, Got 1 lines.\n",
            "------------------------------\n",
            "--- Test Case 9/10 ---\n",
            "User Prompt (content part):\n",
            "Input matrices:\n",
            "A = [[2,2],[-3,-1]]\n",
            "B = [[-7,-4],[8,-2]]\n",
            "Provide the COMPLETE sequence of DSL steps to calculate C = A * B.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model's Generated DSL:\n",
            "Here's the complete sequence of DSL steps to calculate C = A * B:\n",
            "```\n",
            "M0 = A[0,0] + B[0,0]\n",
            "M1 = A[0,1] - M0\n",
            "M2 = A[0,0] * B[0,0]\n",
            "M3 = M1\n",
            "S0 = M0 + M1\n",
            "C = M0 * M1\n",
            "```\n",
            "\n",
            "Explanation:\n",
            "\n",
            "  * `M0 = A[0,0] + B[0,0]`: This line calculates the sum of the first two elements of A and B.\n",
            "  * `M1 = A[0,1] - M0`: This line calculates the difference between the second and third elements of A and B.\n",
            "  * `M2 = A[0,0] * B[0,0]:` This line calculates the product of the first and third elements of A and B.\n",
            "  * `M3 = M1`: This line calculates the final result of the entire calculation by multiplying the first and third elements of A and B together.\n",
            "\n",
            "Note that the values in the output matrix C are calculated using the same syntax as in the original input matrices. The resulting values are then assigned to the corresponding elements of the output matrix.\n",
            "\n",
            "Result: INCORRECT\n",
            "Length Mismatch: Expected 1 lines, Got 1 lines.\n",
            "------------------------------\n",
            "--- Test Case 10/10 ---\n",
            "User Prompt (content part):\n",
            "Input matrices:\n",
            "A = [[-4,5],[3,-1]]\n",
            "B = [[8,-2],[1,-8]]\n",
            "Provide the COMPLETE sequence of DSL steps to calculate C = A * B.\n",
            "\n",
            "Model's Generated DSL:\n",
            "Here's the complete sequence of DSL steps to calculate C = A * B:\n",
            "```\n",
            "M0 = A[0,0] + B[0,0]\n",
            "M1 = A[0,1] - M0\n",
            "M2 = A[0,0] * B[0,0]\n",
            "M3 = M1\n",
            "S0 = M0 + M1\n",
            "C = M0 * M1\n",
            "```\n",
            "\n",
            "Explanation:\n",
            "\n",
            "  * `M0 = A[0,0] + B[0,0]`: This line calculates the sum of the first two columns of A and the second column of B.\n",
            "  * `M1 = A[0,1] - M0`: This line calculates the difference between the first two columns of A and the second column of B.\n",
            "  * `M2 = A[0,0] * B[0,0]:` This line calculates the product of the first two columns of A and the second column of B.\n",
            "  * `M3 = M1`: This line assigns the result of the previous step to the current element of the output matrix C.\n",
            "  * `S0 = M0 + M1`: This line adds the values of M0 and M1 together to get the final value of C.\n",
            "  * `C = M0 * M1`: This line combines the values of M0 and M1 to get the final value of C.\n",
            "\n",
            "Note that the values of M0 and M1 are determined by the input matrices A and B, respectively. These values can be adjusted as needed to match your specific use case.\n",
            "\n",
            "Result: INCORRECT\n",
            "Length Mismatch: Expected 1 lines, Got 1 lines.\n",
            "------------------------------\n",
            "\n",
            "--- Test Summary ---\n",
            "Total Test Cases: 10\n",
            "Correct Predictions: 0\n",
            "Accuracy: 0.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## More Epoch More Data - 25 epoch and 200 data"
      ],
      "metadata": {
        "id": "eOzX_Xehg6AG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import json\n",
        "\n",
        "# This COMPREHENSIVE_SYSTEM_MESSAGE_FOR_FULL_SEQUENCE defines all potential DSL rules.\n",
        "# The fine-tuning data (assistant responses) will teach the specific algorithm and M/S naming conventions.\n",
        "COMPREHENSIVE_SYSTEM_MESSAGE_FOR_FULL_SEQUENCE = \"\"\"You are an AI assistant. Your ONLY task is to output the COMPLETE sequence of VALID DSL lines to perform a 2x2 matrix multiplication (C = A * B), with each DSL line on a new line.\n",
        "STRICT RULES:\n",
        "1.  Adhere strictly to the DSL formats defined below for each line in the sequence.\n",
        "2.  NO PYTHON code (e.g., 'np.dot', '@', 'np.array', 'loops').\n",
        "3.  NO EXPLANATIONS, apologies, or conversational text. Output only the DSL lines, each on a new line.\n",
        "4.  The generated sequence must correctly implement the standard 2x2 matrix multiplication algorithm as a sequence of elementary operations.\n",
        "    (e.g., C[0,0] = A[0,0]*B[0,0] + A[0,1]*B[1,0] should be broken down into steps like M0=A[0,0]*B[0,0], M1=A[0,1]*B[1,0], S0=M0+M1, C[0,0]=S0, following this pattern for all C elements).\n",
        "    The fine-tuning examples (assistant responses) will demonstrate the specific M-variable and S-variable naming convention (M for products, S for sums).\n",
        "5.  If you cannot determine a valid full DSL sequence based on the rules and input, output ONLY the exact text: Error: Cannot determine full sequence.\n",
        "\n",
        "DSL FORMAT DEFINITIONS:\n",
        "\n",
        "1.  **Intermediate Variable Assignment (M-variables):**\n",
        "    * Purpose: To store intermediate results from arithmetic operations or direct assignments.\n",
        "    * Naming: `<id>` is an integer (e.g., M0, M1, M2, ...). Use the next available or contextually appropriate ID. (The specific M/S convention for matmul will be learned from examples).\n",
        "    * Formats:\n",
        "        * `M<id> = <operand1> + <operand2>`  (Addition)\n",
        "        * `M<id> = <operand1> - <operand2>`  (Subtraction)\n",
        "        * `M<id> = <operand1> * <operand2>`  (Multiplication of operands)\n",
        "        * `M<id> = <operand>`             (Direct assignment)\n",
        "    * Examples:\n",
        "        * `M0 = A[0,0] + B[0,0]`\n",
        "        * `M1 = A[0,1] - M0`\n",
        "        * `M2 = A[0,0] * B[0,0]`\n",
        "        * `M3 = M1`\n",
        "        * `M4 = B[1,0]`\n",
        "\n",
        "2.  **Output Matrix Cell Assignment (C-matrix):**\n",
        "    * Purpose: To assign a computed value to an element of the 2x2 output matrix C.\n",
        "    * Format: `C[<row>,<col>] = <operand>`\n",
        "    * Constraints: `<row>` and `<col>` must be 0 or 1.\n",
        "    * Example: `C[0,1] = M5` (Note: for standard matmul, the operand will typically be an S-variable as learned from examples like S0, S1, etc.)\n",
        "\n",
        "3.  **Operands (`<operand>`, `<operand1>`, `<operand2>`):**\n",
        "    * Can be an element from input matrix A: `A[<row>,<col>]` (e.g., A[0,0], A[1,0])\n",
        "    * Can be an element from input matrix B: `B[<row>,<col>]` (e.g., B[0,1], B[1,1])\n",
        "    * Can be a previously defined intermediate variable: `M<id>` (e.g., M0, M3). (The examples will show S-variables like S0, S1, etc., being used for sums in the matmul context).\n",
        "    * Constraints: All `<row>` and `<col>` indices for A and B must be 0 or 1. Any `M<id>` (or S-variable, which is a type of intermediate variable) used as an operand must have been defined in a previous DSL step in the sequence.\n",
        "\n",
        "You will be given the input matrices A (2x2) and B (2x2).\n",
        "Provide the complete, multi-line DSL sequence to calculate C = A * B.\n",
        "\"\"\"\n",
        "\n",
        "def get_standard_ops_definition():\n",
        "    \"\"\"\n",
        "    Defines the 16 DSL steps for a standard 2x2 matrix multiplication.\n",
        "    The 'dsl' field provides the string for each step.\n",
        "    This sequence implicitly defines the M-for-product and S-for-sum convention.\n",
        "    \"\"\"\n",
        "    ops = [\n",
        "        # C[0,0] = A[0,0]*B[0,0] + A[0,1]*B[1,0]\n",
        "        {'dsl': \"M0 = A[0,0]*B[0,0]\", 'var_out': 'M0', 'symbolic_rhs': \"A[0,0]*B[0,0]\"},\n",
        "        {'dsl': \"M1 = A[0,1]*B[1,0]\", 'var_out': 'M1', 'symbolic_rhs': \"A[0,1]*B[1,0]\"},\n",
        "        {'dsl': \"S0 = M0+M1\",         'var_out': 'S0', 'symbolic_rhs': \"M0+M1\"},\n",
        "        {'dsl': \"C[0,0] = S0\",        'var_out': None, 'c_idx': (0,0), 'symbolic_rhs': \"S0\"},\n",
        "\n",
        "        # C[0,1] = A[0,0]*B[0,1] + A[0,1]*B[1,1]\n",
        "        {'dsl': \"M2 = A[0,0]*B[0,1]\", 'var_out': 'M2', 'symbolic_rhs': \"A[0,0]*B[0,1]\"},\n",
        "        {'dsl': \"M3 = A[0,1]*B[1,1]\", 'var_out': 'M3', 'symbolic_rhs': \"A[0,1]*B[1,1]\"},\n",
        "        {'dsl': \"S1 = M2+M3\",         'var_out': 'S1', 'symbolic_rhs': \"M2+M3\"},\n",
        "        {'dsl': \"C[0,1] = S1\",        'var_out': None, 'c_idx': (0,1), 'symbolic_rhs': \"S1\"},\n",
        "\n",
        "        # C[1,0] = A[1,0]*B[0,0] + A[1,1]*B[1,0]\n",
        "        {'dsl': \"M4 = A[1,0]*B[0,0]\", 'var_out': 'M4', 'symbolic_rhs': \"A[1,0]*B[0,0]\"},\n",
        "        {'dsl': \"M5 = A[1,1]*B[1,0]\", 'var_out': 'M5', 'symbolic_rhs': \"A[1,1]*B[1,0]\"},\n",
        "        {'dsl': \"S2 = M4+M5\",         'var_out': 'S2', 'symbolic_rhs': \"M4+M5\"},\n",
        "        {'dsl': \"C[1,0] = S2\",        'var_out': None, 'c_idx': (1,0), 'symbolic_rhs': \"S2\"},\n",
        "\n",
        "        # C[1,1] = A[1,0]*B[0,1] + A[1,1]*B[1,1]\n",
        "        {'dsl': \"M6 = A[1,0]*B[0,1]\", 'var_out': 'M6', 'symbolic_rhs': \"A[1,0]*B[0,1]\"},\n",
        "        {'dsl': \"M7 = A[1,1]*B[1,1]\", 'var_out': 'M7', 'symbolic_rhs': \"A[1,1]*B[1,1]\"},\n",
        "        {'dsl': \"S3 = M6+M7\",         'var_out': 'S3', 'symbolic_rhs': \"M6+M7\"},\n",
        "        {'dsl': \"C[1,1] = S3\",        'var_out': None, 'c_idx': (1,1), 'symbolic_rhs': \"S3\"},\n",
        "    ]\n",
        "    # Other keys like 'calc_val' are kept for potential other uses but not strictly needed\n",
        "    # for this specific dataset generation task where only 'dsl' is used for the completion.\n",
        "    return ops\n",
        "\n",
        "def generate_random_matrix():\n",
        "    \"\"\"Generates a 2x2 matrix with random integer elements between -9 and 9.\"\"\"\n",
        "    return [[random.randint(-9, 9) for _ in range(2)] for _ in range(2)]\n",
        "\n",
        "def generate_sft_dataset_full_sequence(num_full_multiplications=200, system_message=COMPREHENSIVE_SYSTEM_MESSAGE_FOR_FULL_SEQUENCE):\n",
        "    \"\"\"\n",
        "    Generates a fine-tuning dataset for DSL-based 2x2 matrix multiplication.\n",
        "    Each example is a full multiplication problem (A, B inputs) and the complete DSL sequence as completion.\n",
        "    Formatted in ChatML.\n",
        "    \"\"\"\n",
        "    ops_definition = get_standard_ops_definition()\n",
        "    # The assistant's completion is the fixed sequence of DSL steps for standard 2x2 matmul\n",
        "    full_dsl_completion = \"\\n\".join([op['dsl'] for op in ops_definition])\n",
        "\n",
        "    sft_data_chatml = [] # Store ChatML formatted strings\n",
        "\n",
        "    for i in range(num_full_multiplications):\n",
        "        # Generate new A and B matrices for each full multiplication problem\n",
        "        if i == 0: current_A_val, current_B_val = [[1,0],[0,1]], generate_random_matrix()\n",
        "        elif i == 1: current_A_val, current_B_val = generate_random_matrix(), [[1,0],[0,1]]\n",
        "        elif i == 2: current_A_val, current_B_val = [[0,0],[0,0]], generate_random_matrix()\n",
        "        elif i == 3: current_A_val, current_B_val = generate_random_matrix(), [[0,0],[0,0]]\n",
        "        elif i == 4: current_A_val, current_B_val = [[-2, 3],[1, -4]], [[5, -1],[-2, 6]]\n",
        "        else: current_A_val, current_B_val = generate_random_matrix(), generate_random_matrix()\n",
        "\n",
        "        matrix_a_str_for_prompt = str(current_A_val).replace(\" \", \"\")\n",
        "        matrix_b_str_for_prompt = str(current_B_val).replace(\" \", \"\")\n",
        "\n",
        "        # Construct the \"user\" part of the prompt\n",
        "        user_prompt_content = (\n",
        "            f\"Input matrices:\\n\"\n",
        "            f\"A = {matrix_a_str_for_prompt}\\n\"\n",
        "            f\"B = {matrix_b_str_for_prompt}\\n\"\n",
        "            f\"Provide the COMPLETE sequence of DSL steps to calculate C = A * B.\"\n",
        "        )\n",
        "\n",
        "        # Format into ChatML and add to dataset\n",
        "        chatml_text = f\"<|im_start|>system\\n{system_message.strip()}<|im_end|>\\n\"\n",
        "        chatml_text += f\"<|im_start|>user\\n{user_prompt_content.strip()}<|im_end|>\\n\"\n",
        "        chatml_text += f\"<|im_start|>assistant\\n{full_dsl_completion}<|im_end|>\"\n",
        "        sft_data_chatml.append({\"text\": chatml_text})\n",
        "\n",
        "    return sft_data_chatml\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    num_multiplication_examples = 200 # Number of full 2x2 matrix multiplication problems for the dataset\n",
        "\n",
        "    sft_dataset_chatml = generate_sft_dataset_full_sequence(\n",
        "        num_multiplication_examples,\n",
        "        system_message=COMPREHENSIVE_SYSTEM_MESSAGE_FOR_FULL_SEQUENCE\n",
        "    )\n",
        "\n",
        "    print(f\"Generated {len(sft_dataset_chatml)} ChatML examples for SFT.\")\n",
        "    print(f\"Each example provides input matrices A and B, and expects the full {len(get_standard_ops_definition())}-step DSL sequence as the completion.\")\n",
        "\n",
        "    print(\"\\n----- Example SFT Data (First ChatML item) -----\")\n",
        "    if sft_dataset_chatml:\n",
        "        print(sft_dataset_chatml[0]['text'])\n",
        "        print(\"--------------------------------------------------\")\n",
        "\n",
        "    if len(sft_dataset_chatml) > 1:\n",
        "        print(\"\\n----- Example SFT Data (Second ChatML item, if generated) -----\")\n",
        "        print(sft_dataset_chatml[1]['text'])\n",
        "        print(\"--------------------------------------------------\")\n",
        "\n",
        "    output_jsonl_path = \"dsl_finetune_data_standard_matmul_full_sequence.jsonl\"\n",
        "    with open(output_jsonl_path, 'w') as f:\n",
        "        for item in sft_dataset_chatml:\n",
        "            f.write(json.dumps(item) + \"\\n\")\n",
        "    print(f\"\\nFine-tuning data saved to {output_jsonl_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6wol4MshTmA",
        "outputId": "374f04fb-fb2b-4153-9d57-72dbcb6c24ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated 200 ChatML examples for SFT.\n",
            "Each example provides input matrices A and B, and expects the full 16-step DSL sequence as the completion.\n",
            "\n",
            "----- Example SFT Data (First ChatML item) -----\n",
            "<|im_start|>system\n",
            "You are an AI assistant. Your ONLY task is to output the COMPLETE sequence of VALID DSL lines to perform a 2x2 matrix multiplication (C = A * B), with each DSL line on a new line.\n",
            "STRICT RULES:\n",
            "1.  Adhere strictly to the DSL formats defined below for each line in the sequence.\n",
            "2.  NO PYTHON code (e.g., 'np.dot', '@', 'np.array', 'loops').\n",
            "3.  NO EXPLANATIONS, apologies, or conversational text. Output only the DSL lines, each on a new line.\n",
            "4.  The generated sequence must correctly implement the standard 2x2 matrix multiplication algorithm as a sequence of elementary operations.\n",
            "    (e.g., C[0,0] = A[0,0]*B[0,0] + A[0,1]*B[1,0] should be broken down into steps like M0=A[0,0]*B[0,0], M1=A[0,1]*B[1,0], S0=M0+M1, C[0,0]=S0, following this pattern for all C elements).\n",
            "    The fine-tuning examples (assistant responses) will demonstrate the specific M-variable and S-variable naming convention (M for products, S for sums).\n",
            "5.  If you cannot determine a valid full DSL sequence based on the rules and input, output ONLY the exact text: Error: Cannot determine full sequence.\n",
            "\n",
            "DSL FORMAT DEFINITIONS:\n",
            "\n",
            "1.  **Intermediate Variable Assignment (M-variables):**\n",
            "    * Purpose: To store intermediate results from arithmetic operations or direct assignments.\n",
            "    * Naming: `<id>` is an integer (e.g., M0, M1, M2, ...). Use the next available or contextually appropriate ID. (The specific M/S convention for matmul will be learned from examples).\n",
            "    * Formats:\n",
            "        * `M<id> = <operand1> + <operand2>`  (Addition)\n",
            "        * `M<id> = <operand1> - <operand2>`  (Subtraction)\n",
            "        * `M<id> = <operand1> * <operand2>`  (Multiplication of operands)\n",
            "        * `M<id> = <operand>`             (Direct assignment)\n",
            "    * Examples:\n",
            "        * `M0 = A[0,0] + B[0,0]`\n",
            "        * `M1 = A[0,1] - M0`\n",
            "        * `M2 = A[0,0] * B[0,0]`\n",
            "        * `M3 = M1`\n",
            "        * `M4 = B[1,0]`\n",
            "\n",
            "2.  **Output Matrix Cell Assignment (C-matrix):**\n",
            "    * Purpose: To assign a computed value to an element of the 2x2 output matrix C.\n",
            "    * Format: `C[<row>,<col>] = <operand>`\n",
            "    * Constraints: `<row>` and `<col>` must be 0 or 1.\n",
            "    * Example: `C[0,1] = M5` (Note: for standard matmul, the operand will typically be an S-variable as learned from examples like S0, S1, etc.)\n",
            "\n",
            "3.  **Operands (`<operand>`, `<operand1>`, `<operand2>`):**\n",
            "    * Can be an element from input matrix A: `A[<row>,<col>]` (e.g., A[0,0], A[1,0])\n",
            "    * Can be an element from input matrix B: `B[<row>,<col>]` (e.g., B[0,1], B[1,1])\n",
            "    * Can be a previously defined intermediate variable: `M<id>` (e.g., M0, M3). (The examples will show S-variables like S0, S1, etc., being used for sums in the matmul context).\n",
            "    * Constraints: All `<row>` and `<col>` indices for A and B must be 0 or 1. Any `M<id>` (or S-variable, which is a type of intermediate variable) used as an operand must have been defined in a previous DSL step in the sequence.\n",
            "\n",
            "You will be given the input matrices A (2x2) and B (2x2).\n",
            "Provide the complete, multi-line DSL sequence to calculate C = A * B.<|im_end|>\n",
            "<|im_start|>user\n",
            "Input matrices:\n",
            "A = [[1,0],[0,1]]\n",
            "B = [[-5,6],[5,-9]]\n",
            "Provide the COMPLETE sequence of DSL steps to calculate C = A * B.<|im_end|>\n",
            "<|im_start|>assistant\n",
            "M0 = A[0,0]*B[0,0]\n",
            "M1 = A[0,1]*B[1,0]\n",
            "S0 = M0+M1\n",
            "C[0,0] = S0\n",
            "M2 = A[0,0]*B[0,1]\n",
            "M3 = A[0,1]*B[1,1]\n",
            "S1 = M2+M3\n",
            "C[0,1] = S1\n",
            "M4 = A[1,0]*B[0,0]\n",
            "M5 = A[1,1]*B[1,0]\n",
            "S2 = M4+M5\n",
            "C[1,0] = S2\n",
            "M6 = A[1,0]*B[0,1]\n",
            "M7 = A[1,1]*B[1,1]\n",
            "S3 = M6+M7\n",
            "C[1,1] = S3<|im_end|>\n",
            "--------------------------------------------------\n",
            "\n",
            "----- Example SFT Data (Second ChatML item, if generated) -----\n",
            "<|im_start|>system\n",
            "You are an AI assistant. Your ONLY task is to output the COMPLETE sequence of VALID DSL lines to perform a 2x2 matrix multiplication (C = A * B), with each DSL line on a new line.\n",
            "STRICT RULES:\n",
            "1.  Adhere strictly to the DSL formats defined below for each line in the sequence.\n",
            "2.  NO PYTHON code (e.g., 'np.dot', '@', 'np.array', 'loops').\n",
            "3.  NO EXPLANATIONS, apologies, or conversational text. Output only the DSL lines, each on a new line.\n",
            "4.  The generated sequence must correctly implement the standard 2x2 matrix multiplication algorithm as a sequence of elementary operations.\n",
            "    (e.g., C[0,0] = A[0,0]*B[0,0] + A[0,1]*B[1,0] should be broken down into steps like M0=A[0,0]*B[0,0], M1=A[0,1]*B[1,0], S0=M0+M1, C[0,0]=S0, following this pattern for all C elements).\n",
            "    The fine-tuning examples (assistant responses) will demonstrate the specific M-variable and S-variable naming convention (M for products, S for sums).\n",
            "5.  If you cannot determine a valid full DSL sequence based on the rules and input, output ONLY the exact text: Error: Cannot determine full sequence.\n",
            "\n",
            "DSL FORMAT DEFINITIONS:\n",
            "\n",
            "1.  **Intermediate Variable Assignment (M-variables):**\n",
            "    * Purpose: To store intermediate results from arithmetic operations or direct assignments.\n",
            "    * Naming: `<id>` is an integer (e.g., M0, M1, M2, ...). Use the next available or contextually appropriate ID. (The specific M/S convention for matmul will be learned from examples).\n",
            "    * Formats:\n",
            "        * `M<id> = <operand1> + <operand2>`  (Addition)\n",
            "        * `M<id> = <operand1> - <operand2>`  (Subtraction)\n",
            "        * `M<id> = <operand1> * <operand2>`  (Multiplication of operands)\n",
            "        * `M<id> = <operand>`             (Direct assignment)\n",
            "    * Examples:\n",
            "        * `M0 = A[0,0] + B[0,0]`\n",
            "        * `M1 = A[0,1] - M0`\n",
            "        * `M2 = A[0,0] * B[0,0]`\n",
            "        * `M3 = M1`\n",
            "        * `M4 = B[1,0]`\n",
            "\n",
            "2.  **Output Matrix Cell Assignment (C-matrix):**\n",
            "    * Purpose: To assign a computed value to an element of the 2x2 output matrix C.\n",
            "    * Format: `C[<row>,<col>] = <operand>`\n",
            "    * Constraints: `<row>` and `<col>` must be 0 or 1.\n",
            "    * Example: `C[0,1] = M5` (Note: for standard matmul, the operand will typically be an S-variable as learned from examples like S0, S1, etc.)\n",
            "\n",
            "3.  **Operands (`<operand>`, `<operand1>`, `<operand2>`):**\n",
            "    * Can be an element from input matrix A: `A[<row>,<col>]` (e.g., A[0,0], A[1,0])\n",
            "    * Can be an element from input matrix B: `B[<row>,<col>]` (e.g., B[0,1], B[1,1])\n",
            "    * Can be a previously defined intermediate variable: `M<id>` (e.g., M0, M3). (The examples will show S-variables like S0, S1, etc., being used for sums in the matmul context).\n",
            "    * Constraints: All `<row>` and `<col>` indices for A and B must be 0 or 1. Any `M<id>` (or S-variable, which is a type of intermediate variable) used as an operand must have been defined in a previous DSL step in the sequence.\n",
            "\n",
            "You will be given the input matrices A (2x2) and B (2x2).\n",
            "Provide the complete, multi-line DSL sequence to calculate C = A * B.<|im_end|>\n",
            "<|im_start|>user\n",
            "Input matrices:\n",
            "A = [[-7,-9],[-1,-3]]\n",
            "B = [[1,0],[0,1]]\n",
            "Provide the COMPLETE sequence of DSL steps to calculate C = A * B.<|im_end|>\n",
            "<|im_start|>assistant\n",
            "M0 = A[0,0]*B[0,0]\n",
            "M1 = A[0,1]*B[1,0]\n",
            "S0 = M0+M1\n",
            "C[0,0] = S0\n",
            "M2 = A[0,0]*B[0,1]\n",
            "M3 = A[0,1]*B[1,1]\n",
            "S1 = M2+M3\n",
            "C[0,1] = S1\n",
            "M4 = A[1,0]*B[0,0]\n",
            "M5 = A[1,1]*B[1,0]\n",
            "S2 = M4+M5\n",
            "C[1,0] = S2\n",
            "M6 = A[1,0]*B[0,1]\n",
            "M7 = A[1,1]*B[1,1]\n",
            "S3 = M6+M7\n",
            "C[1,1] = S3<|im_end|>\n",
            "--------------------------------------------------\n",
            "\n",
            "Fine-tuning data saved to dsl_finetune_data_standard_matmul_full_sequence.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================\n",
        "# QWEN DSL Fine-Tuning Full Script (Corrected)\n",
        "# ==============================================\n",
        "# Dependencies (install in your Colab / environment):\n",
        "# !pip install -q datasets accelerate bitsandbytes trl\n",
        "\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM, AutoTokenizer,\n",
        "    BitsAndBytesConfig # TrainingArguments not strictly needed if all args in SFTConfig\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training # PeftModel not used in training script\n",
        "from trl import SFTConfig, SFTTrainer\n",
        "\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 1. (Optional) Mount Google Drive\n",
        "# ------------------------------------------------\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# DRIVE_BASE_PATH = \"/content/drive/MyDrive/Qwen_DSL_FineTune\"\n",
        "# os.makedirs(DRIVE_BASE_PATH, exist_ok=True)\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 2. System Message and Dataset Path\n",
        "# ------------------------------------------------\n",
        "SYSTEM_MESSAGE = (\n",
        "   \"\"\"You are an AI assistant. Your ONLY task is to output the COMPLETE sequence of VALID DSL lines to perform a 2x2 matrix multiplication (C = A * B), with each DSL line on a new line.\n",
        "STRICT RULES:\n",
        "1.  Adhere strictly to the DSL formats defined below for each line in the sequence.\n",
        "2.  NO PYTHON code (e.g., 'np.dot', '@', 'np.array', 'loops').\n",
        "3.  NO EXPLANATIONS, apologies, or conversational text. Output only the DSL lines, each on a new line.\n",
        "4.  The generated sequence must correctly implement the standard 2x2 matrix multiplication algorithm as a sequence of elementary operations.\n",
        "    (e.g., C[0,0] = A[0,0]*B[0,0] + A[0,1]*B[1,0] should be broken down into steps like M0=A[0,0]*B[0,0], M1=A[0,1]*B[1,0], S0=M0+M1, C[0,0]=S0, following this pattern for all C elements).\n",
        "    The fine-tuning examples (assistant responses) will demonstrate the specific M-variable and S-variable naming convention (M for products, S for sums).\n",
        "5.  If you cannot determine a valid full DSL sequence based on the rules and input, output ONLY the exact text: Error: Cannot determine full sequence.\n",
        "\n",
        "DSL FORMAT DEFINITIONS:\n",
        "\n",
        "1.  **Intermediate Variable Assignment (M-variables):**\n",
        "    * Purpose: To store intermediate results from arithmetic operations or direct assignments.\n",
        "    * Naming: `<id>` is an integer (e.g., M0, M1, M2, ...). Use the next available or contextually appropriate ID. (The specific M/S convention for matmul will be learned from examples).\n",
        "    * Formats:\n",
        "        * `M<id> = <operand1> + <operand2>`  (Addition)\n",
        "        * `M<id> = <operand1> - <operand2>`  (Subtraction)\n",
        "        * `M<id> = <operand1> * <operand2>`  (Multiplication of operands)\n",
        "        * `M<id> = <operand>`             (Direct assignment)\n",
        "    * Examples:\n",
        "        * `M0 = A[0,0] + B[0,0]`\n",
        "        * `M1 = A[0,1] - M0`\n",
        "        * `M2 = A[0,0] * B[0,0]`\n",
        "        * `M3 = M1`\n",
        "        * `M4 = B[1,0]`\n",
        "\n",
        "2.  **Output Matrix Cell Assignment (C-matrix):**\n",
        "    * Purpose: To assign a computed value to an element of the 2x2 output matrix C.\n",
        "    * Format: `C[<row>,<col>] = <operand>`\n",
        "    * Constraints: `<row>` and `<col>` must be 0 or 1.\n",
        "    * Example: `C[0,1] = M5` (Note: for standard matmul, the operand will typically be an S-variable as learned from examples like S0, S1, etc.)\n",
        "\n",
        "3.  **Operands (`<operand>`, `<operand1>`, `<operand2>`):**\n",
        "    * Can be an element from input matrix A: `A[<row>,<col>]` (e.g., A[0,0], A[1,0])\n",
        "    * Can be an element from input matrix B: `B[<row>,<col>]` (e.g., B[0,1], B[1,1])\n",
        "    * Can be a previously defined intermediate variable: `M<id>` (e.g., M0, M3). (The examples will show S-variables like S0, S1, etc., being used for sums in the matmul context).\n",
        "    * Constraints: All `<row>` and `<col>` indices for A and B must be 0 or 1. Any `M<id>` (or S-variable, which is a type of intermediate variable) used as an operand must have been defined in a previous DSL step in the sequence.\n",
        "\n",
        "You will be given the input matrices A (2x2) and B (2x2).\n",
        "Provide the complete, multi-line DSL sequence to calculate C = A * B.\"\"\"\n",
        ")\n",
        "\n",
        "# Dataset path (ensure this file exists and is correctly formatted)\n",
        "dataset_path = \"/content/dsl_finetune_data_standard_matmul_full_sequence.jsonl\"\n",
        "if not os.path.exists(dataset_path):\n",
        "  print(f\"Dataset file not found: {dataset_path}. Please ensure it exists.\")\n",
        "  # Consider exiting or handling this error more robustly if the file is essential\n",
        "  # exit()\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 3. Load Dataset\n",
        "# ------------------------------------------------\n",
        "dataset = load_dataset(\"json\", data_files=dataset_path, split=\"train\")\n",
        "print(f\"Loaded {len(dataset)} examples from {dataset_path}\")\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 4. Model & Tokenizer Setup\n",
        "# ------------------------------------------------\n",
        "model_name = \"Qwen/Qwen1.5-0.5B-Chat\"\n",
        "\n",
        "# Quantization config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# Model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 5. LoRA Configuration & PEFT Wrapping\n",
        "# ------------------------------------------------\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 6. Sanity Check: Ensure target_modules exist\n",
        "# ------------------------------------------------\n",
        "print(\"\\nValidating target module names:\")\n",
        "targets = lora_config.target_modules\n",
        "leafs = {name.split('.')[-1] for name, _ in model.named_modules()}\n",
        "for t in targets:\n",
        "    print(f\"{t:10s} → {('FOUND' if t in leafs else 'MISSING')}\")\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 7. Training Configuration and Trainer Initialization\n",
        "# ------------------------------------------------\n",
        "output_dir = \"./qwen_dsl_finetuned_adapter\" # Base directory for all outputs\n",
        "\n",
        "# SFTConfig inherits from TrainingArguments, so all parameters are set here.\n",
        "sft_args = SFTConfig(\n",
        "    # SFT specific parameters\n",
        "    dataset_text_field=\"text\",         # Field in your dataset that contains the ChatML string\n",
        "    max_seq_length=1500,                # Max sequence length (corrected from max_length)\n",
        "    packing=False,                     # Whether to pack multiple short examples if True\n",
        "\n",
        "    # Standard TrainingArguments parameters integrated here\n",
        "    output_dir=output_dir,             # Trainer will save checkpoints, logs etc. here\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=2,     # Effective batch size = 2 * 2 = 4\n",
        "    learning_rate=1e-4,\n",
        "    num_train_epochs=25,                # Increased epochs for small dataset (e.g., 25 examples). Adjust as needed.\n",
        "    logging_steps=10,                  # Log training progress every 10 steps\n",
        "    save_strategy=\"epoch\",             # Save a checkpoint at the end of each epoch\n",
        "    bf16=True,                         # Use bfloat16 precision (ensure GPU supports it)\n",
        "    # fp16=False,                      # If bf16 not supported, set fp16=True\n",
        "    optim=\"paged_adamw_8bit\",          # Optimizer\n",
        "    # report_to=\"none\",                # Already handled by os.environ[\"WANDB_DISABLED\"]\n",
        "    # Add other relevant TrainingArguments as needed:\n",
        "    # weight_decay=0.01,\n",
        "    # lr_scheduler_type=\"cosine\",\n",
        "    # warmup_steps=50,\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    processing_class=tokenizer,                # Pass the tokenizer instance directly\n",
        "    args=sft_args,                      # Pass the comprehensive SFTConfig object\n",
        "    train_dataset=dataset,\n",
        "    peft_config=lora_config,\n",
        "    # dataset_text_field, max_seq_length, packing are sourced from sft_args now\n",
        ")\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 8. Start Fine-Tuning\n",
        "# ------------------------------------------------\n",
        "print(\"Starting fine-tuning with effective arguments...\")\n",
        "trainer.train() # Single, effective train call\n",
        "print(\"Fine-tuning completed.\")\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 9. Save LoRA Adapter & Tokenizer\n",
        "# ------------------------------------------------\n",
        "# The adapter will be saved in a subdirectory of output_dir (e.g. output_dir/checkpoint-xxx or output_dir if save_strategy=\"no\")\n",
        "# To save the final adapter explicitly to a desired path after training:\n",
        "adapter_path = os.path.join(output_dir, \"final_adapter\")\n",
        "os.makedirs(adapter_path, exist_ok=True) # Ensure the directory exists\n",
        "trainer.save_model(adapter_path)\n",
        "tokenizer.save_pretrained(adapter_path)\n",
        "print(f\"Final adapter and tokenizer saved at {adapter_path}\")\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 10. (Optional) Test the Fine-Tuned Model (after training and saving)\n",
        "# ------------------------------------------------\n",
        "def test_model(prompt: str, trained_model, trained_tokenizer): # Pass the trained model and tokenizer\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_MESSAGE},\n",
        "        {\"role\": \"user\",   \"content\": prompt},\n",
        "    ]\n",
        "    text_in = trained_tokenizer.apply_chat_template(\n",
        "        messages, tokenize=False, add_generation_prompt=True\n",
        "    )\n",
        "    inputs = trained_tokenizer([text_in], return_tensors=\"pt\").to(trained_model.device)\n",
        "    with torch.no_grad():\n",
        "        out = trained_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=1500, # Increased from 25 to accommodate full DSL sequence\n",
        "            do_sample=False,\n",
        "            pad_token_id=trained_tokenizer.eos_token_id,\n",
        "            eos_token_id=trained_tokenizer.eos_token_id\n",
        "        )\n",
        "    gen_tokens = out[0][inputs[\"input_ids\"].shape[-1]:]\n",
        "    return trained_tokenizer.decode(gen_tokens, skip_special_tokens=True).strip()\n",
        "\n",
        "# To use the test_model function after training, you would typically load the saved adapter:\n",
        "# from peft import PeftModel\n",
        "#\n",
        "# print(\"\\n--- Testing the fine-tuned model ---\")\n",
        "# # Load the base model again (quantized)\n",
        "# base_model_for_test = AutoModelForCausalLM.from_pretrained(\n",
        "# model_name,\n",
        "# quantization_config=bnb_config,\n",
        "# device_map=\"auto\",\n",
        "# trust_remote_code=True\n",
        "# )\n",
        "# # Load the PeftModel with the adapter\n",
        "# loaded_model = PeftModel.from_pretrained(base_model_for_test, adapter_path)\n",
        "# loaded_model.eval() # Set to evaluation mode\n",
        "#\n",
        "# loaded_tokenizer = AutoTokenizer.from_pretrained(adapter_path) # Load tokenizer from adapter path\n",
        "#\n",
        "# test_prompt_example = (\n",
        "#     \"Input matrices:\\n\"\n",
        "#     \"A = [[1,2],[3,4]]\\n\"\n",
        "#     \"B = [[5,6],[7,8]]\\n\"\n",
        "#     \"Provide the COMPLETE sequence of DSL steps to calculate C = A * B.\"\n",
        "# )\n",
        "# response = test_model(test_prompt_example, loaded_model, loaded_tokenizer)\n",
        "# print(f\"Test Prompt:\\n{test_prompt_example}\")\n",
        "# print(f\"\\nModel Response:\\n{response}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "26c7014c8b5b462fb2d69846742287ea",
            "0d55c0f804ab4f30ba2a72c7fe6c9fbf",
            "75294f86ab4444b48f66ecce68dd721d",
            "f0c6f11b5070433bb16067d65513e844",
            "d53d85a8ddb84a86b821b81037621f39",
            "06d1deb6bb7b49f7a779ece734361af7",
            "abe90b35ca4c496b86f961d39735be73",
            "3a0e698746294b738a70dc1a7182e86c",
            "016888f0dc6140c7992ae6d2f2984965",
            "71d2f8d3238b485ebc084309645fa034",
            "d09d30160cb1403bba84a40acfb49c78",
            "6a884ba704534f66b4ea06fee49a9755",
            "b124b01bd6464fc2ab947f248a1ee265",
            "52182d5e0a47410bb7a6476f04c3d89e",
            "44fdcbfcead74027a00ab40df938ff8b",
            "046ae726716e4e3fb9c7d9a91397f282",
            "5e7adde370da4a3684f78df5aa92e086",
            "7e887ef21a084a9fbbd3171def60d7ed",
            "4e8988fa14614a7891ad7dd2d48ee7ec",
            "4820adf77f7a42e7baffc541f7086ebf",
            "459a70aa61d34616bf293ed24ecb1d4f",
            "585e24f4da0848fb99c6f6a60ff695c3",
            "6791c2b7724f4cc3a1fd714838f28412",
            "5331e6e182e341e88e9005ab2deef986",
            "d5c31b762f9445ae98345d5f1e937c3d",
            "eec2b6e051c2459c8d7256a66ac96d50",
            "6285aec7e81e4566bedc28cadff44c4a",
            "baaaf3b1b297472d8da9d7726b1d7bec",
            "7ce2f713594a4854b85ae5d11cd484fb",
            "cf549ed91a274a1b86f25139542dede8",
            "9e1b933eaa634586935e84f1e8abe781",
            "beb917b64e7049ecaca5931be663ed59",
            "34f2a509663544f8a5fa282df9520ff2",
            "d65040eb96af450cbf65953254e6dbcb",
            "bca15887a83f40929efb489a3e7d8c9c",
            "a764d35670044e23a353bcfb0049c36d",
            "6621de102bee44898e788c4a73bfdf83",
            "95b60879e80e4d97966cbd24564b6c99",
            "2450026d7b1743cd8419434a939b9541",
            "53d1f66bf5614926a4d74c035d85f05a",
            "82959c4c90e6485694b9c22b7dfe6b4c",
            "aa14e53c3e624980a6e7d62ae4cf1682",
            "afde729678cf4694b87c10ba73e748d8",
            "c772025c55ed441f82d90d06d01abb92",
            "0f46c20cb4aa4e7a97bdcd009e72439c",
            "67cf449fdb20440da175060044121477",
            "1d0680d89ea148eb9062c8d2ec1c18e1",
            "1aba49649c1344a687e5c28d2f8c1015",
            "ccf5aad63b5141b597238b58d6df23fd",
            "d56dcbd7b27048bcba83553588f38ffe",
            "efe3b96ac3d84566a3b564666b9c1d21",
            "eebffaea8bfa49f58893c20ba90eb87b",
            "082d7fdbb5b14c6fb25122ec820e60ef",
            "0e2499a5c5ff4351b325e8457cc7a71d",
            "c1797e5dfeaa4ff9aae1cdc775d3f086"
          ]
        },
        "id": "q3o8lqorg5ZZ",
        "outputId": "6acc7347-5a3e-4051-ebae-2869ec2c1ac2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "26c7014c8b5b462fb2d69846742287ea"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 200 examples from /content/dsl_finetune_data_standard_matmul_full_sequence.jsonl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 7,569,408 || all params: 471,557,120 || trainable%: 1.6052\n",
            "\n",
            "Validating target module names:\n",
            "v_proj     → FOUND\n",
            "up_proj    → FOUND\n",
            "q_proj     → FOUND\n",
            "gate_proj  → FOUND\n",
            "o_proj     → FOUND\n",
            "down_proj  → FOUND\n",
            "k_proj     → FOUND\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Converting train dataset to ChatML:   0%|          | 0/200 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6a884ba704534f66b4ea06fee49a9755"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Adding EOS to train dataset:   0%|          | 0/200 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6791c2b7724f4cc3a1fd714838f28412"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizing train dataset:   0%|          | 0/200 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d65040eb96af450cbf65953254e6dbcb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Truncating train dataset:   0%|          | 0/200 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0f46c20cb4aa4e7a97bdcd009e72439c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting fine-tuning with effective arguments...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='84' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  84/1250 11:50 < 2:48:15, 0.12 it/s, Epoch 1.66/25]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.731300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.928600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.282900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.066700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.029800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.024800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.023800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.023000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-5a40065c94d6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;31m# ------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting fine-tuning with effective arguments...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Single, effective train call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Fine-tuning completed.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2238\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2239\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2240\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2241\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2242\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2553\u001b[0m                     )\n\u001b[1;32m   2554\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2555\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2557\u001b[0m                     if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3744\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3745\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3747\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m    659\u001b[0m             \u001b[0;31m# cu_seq_lens_k, and max_length_k, max_length_q and position_ids.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"attention_mask\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m                 \u001b[0mnum_tokens_in_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather_for_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    662\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;34m\"position_ids\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m                 \u001b[0mlocal_num_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"position_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"position_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Adding Validation"
      ],
      "metadata": {
        "id": "SRXya2JHkTfL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================\n",
        "# QWEN DSL Fine-Tuning Full Script (Corrected for SFTConfig & with Validation)\n",
        "# ==============================================\n",
        "# Dependencies (install in your Colab / environment):\n",
        "# !pip install -q datasets accelerate bitsandbytes trl transformers # Ensure transformers is also listed\n",
        "\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "from datasets import load_dataset, DatasetDict\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM, AutoTokenizer,\n",
        "    BitsAndBytesConfig, TrainingArguments # Import TrainingArguments\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from trl import SFTTrainer # SFTConfig might not be needed if passing args directly\n",
        "\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 1. (Optional) Mount Google Drive\n",
        "# ------------------------------------------------\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# DRIVE_BASE_PATH = \"/content/drive/MyDrive/Qwen_DSL_FineTune\"\n",
        "# os.makedirs(DRIVE_BASE_PATH, exist_ok=True)\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 2. System Message and Dataset Path\n",
        "# ------------------------------------------------\n",
        "SYSTEM_MESSAGE = (\n",
        "   \"\"\"You are an AI assistant. Your ONLY task is to output the COMPLETE sequence of VALID DSL lines to perform a 2x2 matrix multiplication (C = A * B), with each DSL line on a new line.\n",
        "STRICT RULES:\n",
        "1.  Adhere strictly to the DSL formats defined below for each line in the sequence.\n",
        "2.  NO PYTHON code (e.g., 'np.dot', '@', 'np.array', 'loops').\n",
        "3.  NO EXPLANATIONS, apologies, or conversational text. Output only the DSL lines, each on a new line.\n",
        "4.  The generated sequence must correctly implement the standard 2x2 matrix multiplication algorithm as a sequence of elementary operations.\n",
        "    (e.g., C[0,0] = A[0,0]*B[0,0] + A[0,1]*B[1,0] should be broken down into steps like M0=A[0,0]*B[0,0], M1=A[0,1]*B[1,0], S0=M0+M1, C[0,0]=S0, following this pattern for all C elements).\n",
        "    The fine-tuning examples (assistant responses) will demonstrate the specific M-variable and S-variable naming convention (M for products, S for sums).\n",
        "5.  If you cannot determine a valid full DSL sequence based on the rules and input, output ONLY the exact text: Error: Cannot determine full sequence.\n",
        "\n",
        "DSL FORMAT DEFINITIONS:\n",
        "\n",
        "1.  **Intermediate Variable Assignment (M-variables):**\n",
        "    * Purpose: To store intermediate results from arithmetic operations or direct assignments.\n",
        "    * Naming: `<id>` is an integer (e.g., M0, M1, M2, ...). Use the next available or contextually appropriate ID. (The specific M/S convention for matmul will be learned from examples).\n",
        "    * Formats:\n",
        "        * `M<id> = <operand1> + <operand2>`  (Addition)\n",
        "        * `M<id> = <operand1> - <operand2>`  (Subtraction)\n",
        "        * `M<id> = <operand1> * <operand2>`  (Multiplication of operands)\n",
        "        * `M<id> = <operand>`             (Direct assignment)\n",
        "    * Examples:\n",
        "        * `M0 = A[0,0] + B[0,0]`\n",
        "        * `M1 = A[0,1] - M0`\n",
        "        * `M2 = A[0,0] * B[0,0]`\n",
        "        * `M3 = M1`\n",
        "        * `M4 = B[1,0]`\n",
        "\n",
        "2.  **Output Matrix Cell Assignment (C-matrix):**\n",
        "    * Purpose: To assign a computed value to an element of the 2x2 output matrix C.\n",
        "    * Format: `C[<row>,<col>] = <operand>`\n",
        "    * Constraints: `<row>` and `<col>` must be 0 or 1.\n",
        "    * Example: `C[0,1] = M5` (Note: for standard matmul, the operand will typically be an S-variable as learned from examples like S0, S1, etc.)\n",
        "\n",
        "3.  **Operands (`<operand>`, `<operand1>`, `<operand2>`):**\n",
        "    * Can be an element from input matrix A: `A[<row>,<col>]` (e.g., A[0,0], A[1,0])\n",
        "    * Can be an element from input matrix B: `B[<row>,<col>]` (e.g., B[0,1], B[1,1])\n",
        "    * Can be a previously defined intermediate variable: `M<id>` (e.g., M0, M3). (The examples will show S-variables like S0, S1, etc., being used for sums in the matmul context).\n",
        "    * Constraints: All `<row>` and `<col>` indices for A and B must be 0 or 1. Any `M<id>` (or S-variable, which is a type of intermediate variable) used as an operand must have been defined in a previous DSL step in the sequence.\n",
        "\n",
        "You will be given the input matrices A (2x2) and B (2x2).\n",
        "Provide the complete, multi-line DSL sequence to calculate C = A * B.\"\"\"\n",
        ")\n",
        "\n",
        "dataset_path = \"/content/dsl_finetune_data_standard_matmul_full_sequence.jsonl\"\n",
        "if not os.path.exists(dataset_path):\n",
        "  print(f\"Dataset file not found: {dataset_path}. Please ensure it exists.\")\n",
        "  # exit()\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 3. Load Dataset and Split for Validation\n",
        "# ------------------------------------------------\n",
        "full_dataset = load_dataset(\"json\", data_files=dataset_path, split=\"train\")\n",
        "print(f\"Loaded {len(full_dataset)} total examples from {dataset_path}\")\n",
        "\n",
        "full_dataset_shuffled = full_dataset.shuffle(seed=42)\n",
        "\n",
        "if len(full_dataset_shuffled) >= 10:\n",
        "    split_dataset = full_dataset_shuffled.train_test_split(test_size=0.2, seed=42)\n",
        "    train_dataset = split_dataset[\"train\"]\n",
        "    eval_dataset = split_dataset[\"test\"]\n",
        "    print(f\"Using {len(train_dataset)} examples for training and {len(eval_dataset)} for evaluation.\")\n",
        "else:\n",
        "    print(\"Dataset too small for a meaningful train/test split. Using full dataset for both training and evaluation.\")\n",
        "    train_dataset = full_dataset_shuffled\n",
        "    eval_dataset = full_dataset_shuffled\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 4. Model & Tokenizer Setup\n",
        "# ------------------------------------------------\n",
        "model_name = \"Qwen/Qwen1.5-0.5B-Chat\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 5. LoRA Configuration & PEFT Wrapping\n",
        "# ------------------------------------------------\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 6. Sanity Check: Ensure target_modules exist\n",
        "# ------------------------------------------------\n",
        "print(\"\\nValidating target module names:\")\n",
        "targets = lora_config.target_modules\n",
        "leafs = {name.split('.')[-1] for name, _ in model.named_modules()}\n",
        "for t in targets:\n",
        "    print(f\"{t:10s} → {('FOUND' if t in leafs else 'MISSING')}\")\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 7. Training Configuration and Trainer Initialization\n",
        "# ------------------------------------------------\n",
        "output_dir = \"./qwen_dsl_finetuned_adapter\"\n",
        "\n",
        "from trl import SFTConfig, SFTTrainer\n",
        "\n",
        "# 1. Build your SFTConfig (this replaces TrainingArguments for SFT-specific options)\n",
        "sft_config = SFTConfig(\n",
        "    # Tell it which column in your dataset holds the raw text\n",
        "    dataset_text_field=\"text\",\n",
        "    # How long to truncate / pad sequences\n",
        "    max_seq_length=2048,\n",
        "    # Whether to pack multiple examples into one batch element\n",
        "    packing=True,\n",
        "\n",
        "    # You can also include Trainer hyperparams here:\n",
        "    output_dir=\"./qwen_dsl_finetuned_adapter\",\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=2,\n",
        "    learning_rate=2e-4,\n",
        "    num_train_epochs=3,\n",
        "    save_strategy=\"epoch\",\n",
        "    bf16=True,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    report_to=[\"none\"],\n",
        ")\n",
        "\n",
        "# 2. Instantiate SFTTrainer using that config\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=sft_config,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,  # if you have an eval split\n",
        "    processing_class=tokenizer,\n",
        "    peft_config=lora_config,\n",
        ")\n",
        "\n",
        "# 3. Train!\n",
        "trainer.train()\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 8. Start Fine-Tuning\n",
        "# ------------------------------------------------\n",
        "print(\"Starting fine-tuning with effective arguments and evaluation...\")\n",
        "trainer.train()\n",
        "print(\"Fine-tuning completed.\")\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 9. Save LoRA Adapter & Tokenizer\n",
        "# ------------------------------------------------\n",
        "adapter_path = os.path.join(output_dir, \"final_adapter_after_full_train\")\n",
        "os.makedirs(adapter_path, exist_ok=True)\n",
        "trainer.save_model(adapter_path)\n",
        "tokenizer.save_pretrained(adapter_path)\n",
        "print(f\"Final adapter and tokenizer (after full training) saved at {adapter_path}\")\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 10. (Optional) Test the Fine-Tuned Model\n",
        "# ------------------------------------------------\n",
        "def test_model(prompt: str, trained_model, trained_tokenizer):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_MESSAGE},\n",
        "        {\"role\": \"user\",   \"content\": prompt},\n",
        "    ]\n",
        "    text_in = trained_tokenizer.apply_chat_template(\n",
        "        messages, tokenize=False, add_generation_prompt=True\n",
        "    )\n",
        "    inputs = trained_tokenizer([text_in], return_tensors=\"pt\").to(trained_model.device)\n",
        "    with torch.no_grad():\n",
        "        out = trained_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=1500,\n",
        "            do_sample=False,\n",
        "            pad_token_id=trained_tokenizer.eos_token_id,\n",
        "            eos_token_id=trained_tokenizer.eos_token_id\n",
        "        )\n",
        "    gen_tokens = out[0][inputs[\"input_ids\"].shape[-1]:]\n",
        "    return trained_tokenizer.decode(gen_tokens, skip_special_tokens=True).strip()\n",
        "\n",
        "# Example of how to test after training\n",
        "# (Ensure bnb_config, model_name, SYSTEM_MESSAGE are available if running this part)\n",
        "# from peft import PeftModel\n",
        "# print(\"\\n--- Potentially testing the fine-tuned model ---\")\n",
        "# adapter_to_test_path = adapter_path\n",
        "#\n",
        "# try:\n",
        "#     base_model_for_test = AutoModelForCausalLM.from_pretrained(\n",
        "#         model_name,\n",
        "#         quantization_config=bnb_config,\n",
        "#         device_map=\"auto\",\n",
        "#         trust_remote_code=True\n",
        "#     )\n",
        "#     print(f\"Loading adapter from: {adapter_to_test_path}\")\n",
        "#     loaded_model = PeftModel.from_pretrained(base_model_for_test, adapter_to_test_path)\n",
        "#     loaded_model.eval()\n",
        "#     loaded_tokenizer = AutoTokenizer.from_pretrained(adapter_to_test_path)\n",
        "#\n",
        "#     test_prompt_example = (\n",
        "#         \"Input matrices:\\n\"\n",
        "#         \"A = [[1,2],[3,4]]\\n\"\n",
        "#         \"B = [[5,6],[7,8]]\\n\"\n",
        "#         \"Provide the COMPLETE sequence of DSL steps to calculate C = A * B.\"\n",
        "#     )\n",
        "#     response = test_model(test_prompt_example, loaded_model, loaded_tokenizer)\n",
        "#     print(f\"Test Prompt:\\n{test_prompt_example}\")\n",
        "#     print(f\"\\nModel Response:\\n{response}\")\n",
        "# except Exception as e:\n",
        "#     print(f\"Could not run optional test: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 639
        },
        "id": "zn02Eq1CkSra",
        "outputId": "48fb421c-d37f-4866-86db-94687dd13f25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 200 total examples from /content/dsl_finetune_data_standard_matmul_full_sequence.jsonl\n",
            "Using 160 examples for training and 40 for evaluation.\n",
            "trainable params: 7,569,408 || all params: 471,557,120 || trainable%: 1.6052\n",
            "\n",
            "Validating target module names:\n",
            "v_proj     → FOUND\n",
            "up_proj    → FOUND\n",
            "q_proj     → FOUND\n",
            "gate_proj  → FOUND\n",
            "o_proj     → FOUND\n",
            "down_proj  → FOUND\n",
            "k_proj     → FOUND\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='69' max='69' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [69/69 18:33, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting fine-tuning with effective arguments and evaluation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='69' max='69' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [69/69 18:32, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tuning completed.\n",
            "Final adapter and tokenizer (after full training) saved at ./qwen_dsl_finetuned_adapter/final_adapter_after_full_train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " import os\n",
        "import json\n",
        "import torch\n",
        "import random\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM, AutoTokenizer,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from peft import PeftModel\n",
        "\n",
        "# Ensure WANDB is disabled if you had it enabled previously for training\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "# ------------------------------------------------\n",
        "# Configuration (Adjust these paths if necessary)\n",
        "# ------------------------------------------------\n",
        "BASE_MODEL_NAME = \"Qwen/Qwen1.5-0.5B-Chat\"\n",
        "# This is the directory where trainer.save_model() saved the adapter\n",
        "ADAPTER_PATH = \"./qwen_dsl_finetuned_adapter/final_adapter\" # Output from step 9\n",
        "\n",
        "# ------------------------------------------------\n",
        "# System Message (MUST be IDENTICAL to the one used for fine-tuning)\n",
        "# ------------------------------------------------\n",
        "SYSTEM_MESSAGE = (\n",
        "   \"\"\"You are an AI assistant. Your ONLY task is to output the COMPLETE sequence of VALID DSL lines to perform a 2x2 matrix multiplication (C = A * B), with each DSL line on a new line.\n",
        "STRICT RULES:\n",
        "1.  Adhere strictly to the DSL formats defined below for each line in the sequence.\n",
        "2.  NO PYTHON code (e.g., 'np.dot', '@', 'np.array', 'loops').\n",
        "3.  NO EXPLANATIONS, apologies, or conversational text. Output only the DSL lines, each on a new line.\n",
        "4.  The generated sequence must correctly implement the standard 2x2 matrix multiplication algorithm as a sequence of elementary operations.\n",
        "    (e.g., C[0,0] = A[0,0]*B[0,0] + A[0,1]*B[1,0] should be broken down into steps like M0=A[0,0]*B[0,0], M1=A[0,1]*B[1,0], S0=M0+M1, C[0,0]=S0, following this pattern for all C elements).\n",
        "    The fine-tuning examples (assistant responses) will demonstrate the specific M-variable and S-variable naming convention (M for products, S for sums).\n",
        "5.  If you cannot determine a valid full DSL sequence based on the rules and input, output ONLY the exact text: Error: Cannot determine full sequence.\n",
        "\n",
        "DSL FORMAT DEFINITIONS:\n",
        "\n",
        "1.  **Intermediate Variable Assignment (M-variables):**\n",
        "    * Purpose: To store intermediate results from arithmetic operations or direct assignments.\n",
        "    * Naming: `<id>` is an integer (e.g., M0, M1, M2, ...). Use the next available or contextually appropriate ID. (The specific M/S convention for matmul will be learned from examples).\n",
        "    * Formats:\n",
        "        * `M<id> = <operand1> + <operand2>`  (Addition)\n",
        "        * `M<id> = <operand1> - <operand2>`  (Subtraction)\n",
        "        * `M<id> = <operand1> * <operand2>`  (Multiplication of operands)\n",
        "        * `M<id> = <operand>`             (Direct assignment)\n",
        "    * Examples:\n",
        "        * `M0 = A[0,0] + B[0,0]`\n",
        "        * `M1 = A[0,1] - M0`\n",
        "        * `M2 = A[0,0] * B[0,0]`\n",
        "        * `M3 = M1`\n",
        "        * `M4 = B[1,0]`\n",
        "\n",
        "2.  **Output Matrix Cell Assignment (C-matrix):**\n",
        "    * Purpose: To assign a computed value to an element of the 2x2 output matrix C.\n",
        "    * Format: `C[<row>,<col>] = <operand>`\n",
        "    * Constraints: `<row>` and `<col>` must be 0 or 1.\n",
        "    * Example: `C[0,1] = M5` (Note: for standard matmul, the operand will typically be an S-variable as learned from examples like S0, S1, etc.)\n",
        "\n",
        "3.  **Operands (`<operand>`, `<operand1>`, `<operand2>`):**\n",
        "    * Can be an element from input matrix A: `A[<row>,<col>]` (e.g., A[0,0], A[1,0])\n",
        "    * Can be an element from input matrix B: `B[<row>,<col>]` (e.g., B[0,1], B[1,1])\n",
        "    * Can be a previously defined intermediate variable: `M<id>` (e.g., M0, M3). (The examples will show S-variables like S0, S1, etc., being used for sums in the matmul context).\n",
        "    * Constraints: All `<row>` and `<col>` indices for A and B must be 0 or 1. Any `M<id>` (or S-variable, which is a type of intermediate variable) used as an operand must have been defined in a previous DSL step in the sequence.\n",
        "\n",
        "You will be given the input matrices A (2x2) and B (2x2).\n",
        "Provide the complete, multi-line DSL sequence to calculate C = A * B.\"\"\"\n",
        ")\n",
        "\n",
        "# ------------------------------------------------\n",
        "# Helper functions from your data generation script\n",
        "# ------------------------------------------------\n",
        "def get_standard_ops_definition():\n",
        "    \"\"\"\n",
        "    Defines the 16 DSL steps for a standard 2x2 matrix multiplication.\n",
        "    \"\"\"\n",
        "    ops = [\n",
        "        {'dsl': \"M0 = A[0,0]*B[0,0]\"}, {'dsl': \"M1 = A[0,1]*B[1,0]\"},\n",
        "        {'dsl': \"S0 = M0+M1\"},         {'dsl': \"C[0,0] = S0\"},\n",
        "        {'dsl': \"M2 = A[0,0]*B[0,1]\"}, {'dsl': \"M3 = A[0,1]*B[1,1]\"},\n",
        "        {'dsl': \"S1 = M2+M3\"},         {'dsl': \"C[0,1] = S1\"},\n",
        "        {'dsl': \"M4 = A[1,0]*B[0,0]\"}, {'dsl': \"M5 = A[1,1]*B[1,0]\"},\n",
        "        {'dsl': \"S2 = M4+M5\"},         {'dsl': \"C[1,0] = S2\"},\n",
        "        {'dsl': \"M6 = A[1,0]*B[0,1]\"}, {'dsl': \"M7 = A[1,1]*B[1,1]\"},\n",
        "        {'dsl': \"S3 = M6+M7\"},         {'dsl': \"C[1,1] = S3\"},\n",
        "    ]\n",
        "    return ops\n",
        "\n",
        "def generate_random_matrix():\n",
        "    \"\"\"Generates a 2x2 matrix with random integer elements between -9 and 9.\"\"\"\n",
        "    return [[random.randint(-9, 9) for _ in range(2)] for _ in range(2)]\n",
        "\n",
        "EXPECTED_DSL_SEQUENCE = \"\\n\".join([op['dsl'] for op in get_standard_ops_definition()])\n",
        "\n",
        "# ------------------------------------------------\n",
        "# Load Fine-Tuned Model for Inference\n",
        "# ------------------------------------------------\n",
        "def load_model_for_inference(base_model_name, adapter_path):\n",
        "    print(f\"Loading base model: {base_model_name}\")\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True, # Or 8bit if you trained with that\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "    )\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model_name,\n",
        "        quantization_config=bnb_config, # Use same quantization as training for base\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "    print(f\"Loading adapter from: {adapter_path}\")\n",
        "    # Load the PEFT model by specifying the adapter path\n",
        "    # The `is_trainable=False` is good practice for inference.\n",
        "    model = PeftModel.from_pretrained(model, adapter_path, is_trainable=False)\n",
        "\n",
        "    print(\"Merging adapter into the base model...\")\n",
        "    # Merge LoRA layers for faster inference.\n",
        "    # If you run out of memory, you can skip this and use the PeftModel directly,\n",
        "    # but inference will be slower.\n",
        "    model = model.merge_and_unload()\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(adapter_path, trust_remote_code=True)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token # Or tokenizer.unk_token\n",
        "    tokenizer.padding_side = \"right\" # Important for generation\n",
        "\n",
        "    print(\"Model and tokenizer loaded successfully.\")\n",
        "    return model, tokenizer\n",
        "\n",
        "# ------------------------------------------------\n",
        "# Generate Model Response\n",
        "# ------------------------------------------------\n",
        "def get_model_response(model, tokenizer, user_prompt_content):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_MESSAGE},\n",
        "        {\"role\": \"user\", \"content\": user_prompt_content},\n",
        "    ]\n",
        "\n",
        "    # Apply chat template\n",
        "    text_input = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True # Crucial for Qwen and other instruction-tuned models\n",
        "    )\n",
        "\n",
        "    inputs = tokenizer([text_input], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # Generation parameters\n",
        "    # max_new_tokens needs to be sufficient for the 16 lines of DSL.\n",
        "    # 16 lines * ~20 chars/line = 320. Add some buffer.\n",
        "    # Using do_sample=False for deterministic output, good for testing exact match.\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=2000, # Increased for full sequence\n",
        "            do_sample=False,    # For deterministic output\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id # Model should stop after generating its response\n",
        "        )\n",
        "\n",
        "    # Decode only the generated part\n",
        "    generated_tokens = outputs[0][inputs[\"input_ids\"].shape[-1]:]\n",
        "    response_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "    return response_text.strip()\n",
        "\n",
        "# ------------------------------------------------\n",
        "# Main Testing Logic\n",
        "# ------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    if not os.path.exists(ADAPTER_PATH):\n",
        "        print(f\"ERROR: Adapter path not found: {ADAPTER_PATH}\")\n",
        "        print(\"Please ensure you have trained the model and the adapter is saved correctly.\")\n",
        "        exit(1)\n",
        "\n",
        "    model, tokenizer = load_model_for_inference(BASE_MODEL_NAME, ADAPTER_PATH)\n",
        "    model.eval() # Set model to evaluation mode\n",
        "\n",
        "    num_test_cases = 10\n",
        "    correct_predictions = 0\n",
        "\n",
        "    print(f\"\\n--- Starting {num_test_cases} Test Cases ---\")\n",
        "    print(f\"Expected DSL Sequence (all test cases):\\n{EXPECTED_DSL_SEQUENCE}\\n\")\n",
        "\n",
        "    for i in range(num_test_cases):\n",
        "        print(f\"--- Test Case {i+1}/{num_test_cases} ---\")\n",
        "\n",
        "        # Generate random matrices for the prompt\n",
        "        matrix_a = generate_random_matrix()\n",
        "        matrix_b = generate_random_matrix()\n",
        "\n",
        "        matrix_a_str = str(matrix_a).replace(\" \", \"\")\n",
        "        matrix_b_str = str(matrix_b).replace(\" \", \"\")\n",
        "\n",
        "        user_prompt = (\n",
        "            f\"Input matrices:\\n\"\n",
        "            f\"A = {matrix_a_str}\\n\"\n",
        "            f\"B = {matrix_b_str}\\n\"\n",
        "            f\"Provide the COMPLETE sequence of DSL steps to calculate C = A * B.\"\n",
        "        )\n",
        "        print(f\"User Prompt (content part):\\n{user_prompt}\")\n",
        "\n",
        "        generated_dsl = get_model_response(model, tokenizer, user_prompt)\n",
        "\n",
        "        print(f\"\\nModel's Generated DSL:\\n{generated_dsl}\")\n",
        "\n",
        "        # Normalize whitespace for comparison (e.g., handle potential trailing newlines)\n",
        "        normalized_generated = \"\\n\".join([line.strip() for line in generated_dsl.strip().split('\\n')])\n",
        "        normalized_expected = \"\\n\".join([line.strip() for line in EXPECTED_DSL_SEQUENCE.strip().split('\\n')])\n",
        "\n",
        "        if normalized_generated == normalized_expected:\n",
        "            print(\"\\nResult: CORRECT\")\n",
        "            correct_predictions += 1\n",
        "        else:\n",
        "            print(\"\\nResult: INCORRECT\")\n",
        "            # For detailed diff, you could use difflib\n",
        "            # import difflib\n",
        "            # diff = difflib.unified_diff(\n",
        "            #     normalized_expected.splitlines(keepends=True),\n",
        "            #     normalized_generated.splitlines(keepends=True),\n",
        "            #     fromfile='expected',\n",
        "            #     tofile='generated',\n",
        "            # )\n",
        "            # print(\"Diff:\")\n",
        "            # print(''.join(diff))\n",
        "            if len(normalized_generated.split('\\n')) != len(normalized_expected.split('\\n')):\n",
        "                print(f\"\"\"Length Mismatch: Expected {len(normalized_expected.split('     '))} lines, Got {len(normalized_generated.split('     '))} lines.\"\"\")\n",
        "\n",
        "\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "    accuracy = (correct_predictions / num_test_cases) * 100\n",
        "    print(f\"\\n--- Test Summary ---\")\n",
        "    print(f\"Total Test Cases: {num_test_cases}\")\n",
        "    print(f\"Correct Predictions: {correct_predictions}\")\n",
        "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "    # Clean up model from GPU memory if necessary\n",
        "    del model\n",
        "    del tokenizer\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MIOPpY4rwafm",
        "outputId": "d4989ce2-d319-4ed6-81f0-283760f05149"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading base model: Qwen/Qwen1.5-0.5B-Chat\n",
            "Loading adapter from: ./qwen_dsl_finetuned_adapter/final_adapter\n",
            "Merging adapter into the base model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py:336: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
            "  warnings.warn(\n",
            "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model and tokenizer loaded successfully.\n",
            "\n",
            "--- Starting 10 Test Cases ---\n",
            "Expected DSL Sequence (all test cases):\n",
            "M0 = A[0,0]*B[0,0]\n",
            "M1 = A[0,1]*B[1,0]\n",
            "S0 = M0+M1\n",
            "C[0,0] = S0\n",
            "M2 = A[0,0]*B[0,1]\n",
            "M3 = A[0,1]*B[1,1]\n",
            "S1 = M2+M3\n",
            "C[0,1] = S1\n",
            "M4 = A[1,0]*B[0,0]\n",
            "M5 = A[1,1]*B[1,0]\n",
            "S2 = M4+M5\n",
            "C[1,0] = S2\n",
            "M6 = A[1,0]*B[0,1]\n",
            "M7 = A[1,1]*B[1,1]\n",
            "S3 = M6+M7\n",
            "C[1,1] = S3\n",
            "\n",
            "--- Test Case 1/10 ---\n",
            "User Prompt (content part):\n",
            "Input matrices:\n",
            "A = [[-6,-9],[-1,-2]]\n",
            "B = [[-2,-5],[-6,8]]\n",
            "Provide the COMPLETE sequence of DSL steps to calculate C = A * B.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model's Generated DSL:\n",
            "Here's the complete sequence of DSL steps to calculate C = A * B:\n",
            "```\n",
            "M0 = A[0,0] + B[0,0]\n",
            "M1 = A[0,1] - M0\n",
            "M2 = A[0,0] * B[0,0]\n",
            "M3 = M1\n",
            "S0 = M0 + M1\n",
            "C = M0 * M1\n",
            "```\n",
            "\n",
            "Explanation:\n",
            "\n",
            "  * `M0 = A[0,0] + B[0,0]`: This line calculates the sum of the first two elements of the input matrices A and B.\n",
            "  * `M1 = A[0,1] - M0`: This line calculates the difference between the second and third elements of the input matrices A and B.\n",
            "  * `M2 = A[0,0] * B[0,0]:` This line calculates the product of the first and third elements of the input matrices A and B.\n",
            "  * `M3 = M1`: This line calculates the final result of the calculation, which is the computed value of A * B.\n",
            "\n",
            "Note that the values returned by these steps are not necessarily the same as the actual values calculated by the matmul operation. However, they represent the computed value of A * B.\n",
            "\n",
            "Result: INCORRECT\n",
            "Length Mismatch: Expected 1 lines, Got 1 lines.\n",
            "------------------------------\n",
            "--- Test Case 2/10 ---\n",
            "User Prompt (content part):\n",
            "Input matrices:\n",
            "A = [[-7,9],[4,-8]]\n",
            "B = [[-9,-7],[-3,-2]]\n",
            "Provide the COMPLETE sequence of DSL steps to calculate C = A * B.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model's Generated DSL:\n",
            "Here's the complete sequence of DSL steps to calculate C = A * B:\n",
            "```\n",
            "M0 = A[0,0] + B[0,0]\n",
            "M1 = A[0,1] - M0\n",
            "M2 = A[0,0] * B[0,0]\n",
            "M3 = M1\n",
            "S0 = M0 + M1\n",
            "C = M0 * M1\n",
            "```\n",
            "\n",
            "Explanation:\n",
            "\n",
            "  * `M0 = A[0,0] + B[0,0]`: This line calculates the sum of the first two columns of A and the second column of B.\n",
            "  * `M1 = A[0,1] - M0`: This line calculates the difference between the first two columns of A and the second column of B.\n",
            "  * `M2 = A[0,0] * B[0,0]:` This line calculates the product of the first two columns of A and the second column of B.\n",
            "  * `M3 = M1`: This line assigns the result of the previous step to the current element of the output matrix C.\n",
            "  * `S0 = M0 + M1`: This line adds the values of M0 and M1 together to get the final value of C.\n",
            "  * `C = M0 * M1`: This line combines the values of M0 and M1 to get the final value of C.\n",
            "\n",
            "Note that the values of M0 and M1 are determined by the input matrices A and B, respectively. These values can be adjusted as needed to match your specific use case.\n",
            "\n",
            "Result: INCORRECT\n",
            "Length Mismatch: Expected 1 lines, Got 1 lines.\n",
            "------------------------------\n",
            "--- Test Case 3/10 ---\n",
            "User Prompt (content part):\n",
            "Input matrices:\n",
            "A = [[7,-9],[8,-3]]\n",
            "B = [[8,4],[-2,5]]\n",
            "Provide the COMPLETE sequence of DSL steps to calculate C = A * B.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model's Generated DSL:\n",
            "Here's the complete sequence of DSL steps to calculate C = A * B:\n",
            "```\n",
            "M0 = A[0,0] + B[0,0]\n",
            "M1 = A[0,1] - M0\n",
            "M2 = A[0,0] * B[0,0]\n",
            "M3 = M1\n",
            "S0 = M0 + M1\n",
            "C = M0 + M1\n",
            "```\n",
            "\n",
            "Explanation:\n",
            "\n",
            "  * `M0 = A[0,0] + B[0,0]`: This line calculates the sum of the first two elements of the input matrices.\n",
            "  * `M1 = A[0,1] - M0`: This line calculates the difference between the second and third elements of the input matrices.\n",
            "  * `M2 = A[0,0] * B[0,0]:` This line calculates the product of the first and third elements of the input matrices.\n",
            "  * `M3 = M1`: This line calculates the final result of the calculation.\n",
            "  * `S0 = M0 + M1`: This line adds the values of `M0` and `M1` together to get the final result.\n",
            "  * `C = M0 + M1`: This line combines the values of `M0` and `M1` to get the final result.\n",
            "\n",
            "Note that the values of `M0` and `M1` are not explicitly defined in the input matrices. However, they can be calculated using the same logic as before.\n",
            "\n",
            "Result: INCORRECT\n",
            "Length Mismatch: Expected 1 lines, Got 1 lines.\n",
            "------------------------------\n",
            "--- Test Case 4/10 ---\n",
            "User Prompt (content part):\n",
            "Input matrices:\n",
            "A = [[9,-1],[-9,-4]]\n",
            "B = [[4,1],[-1,-5]]\n",
            "Provide the COMPLETE sequence of DSL steps to calculate C = A * B.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model's Generated DSL:\n",
            "Here's the complete sequence of DSL steps to calculate C = A * B:\n",
            "```\n",
            "M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 = M0 =\n",
            "\n",
            "Result: INCORRECT\n",
            "Length Mismatch: Expected 1 lines, Got 1 lines.\n",
            "------------------------------\n",
            "--- Test Case 5/10 ---\n",
            "User Prompt (content part):\n",
            "Input matrices:\n",
            "A = [[-3,1],[-6,-7]]\n",
            "B = [[3,-6],[2,2]]\n",
            "Provide the COMPLETE sequence of DSL steps to calculate C = A * B.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model's Generated DSL:\n",
            "Here's the complete sequence of DSL steps to calculate C = A * B:\n",
            "```\n",
            "M0 = A[0,0] + B[0,0]\n",
            "M1 = A[0,1] - M0\n",
            "M2 = A[0,0] * B[0,0]\n",
            "M3 = M1\n",
            "S0 = M0 + M1\n",
            "C = M0 * M1\n",
            "```\n",
            "\n",
            "Explanation:\n",
            "\n",
            "  * `M0 = A[0,0] + B[0,0]`: This line calculates the sum of the first two columns of A and the second column of B.\n",
            "  * `M1 = A[0,1] - M0`: This line calculates the difference between the first two columns of A and the second column of B.\n",
            "  * `M2 = A[0,0] * B[0,0]:` This line calculates the product of the first two columns of A and the second column of B.\n",
            "  * `M3 = M1`: This line assigns the result of the previous step to the current element of the output matrix C.\n",
            "  * `S0 = M0 + M1`: This line adds the values of M0 and M1 together to get the final value of C.\n",
            "  * `C = M0 * M1`: This line combines the values of M0 and M1 to get the final value of C.\n",
            "\n",
            "Note that the values of M0 and M1 are determined by the input matrices A and B, respectively. These values can be adjusted as needed to match your specific use case.\n",
            "\n",
            "Result: INCORRECT\n",
            "Length Mismatch: Expected 1 lines, Got 1 lines.\n",
            "------------------------------\n",
            "--- Test Case 6/10 ---\n",
            "User Prompt (content part):\n",
            "Input matrices:\n",
            "A = [[-1,-8],[5,8]]\n",
            "B = [[-6,3],[-7,8]]\n",
            "Provide the COMPLETE sequence of DSL steps to calculate C = A * B.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model's Generated DSL:\n",
            "Here's the complete sequence of DSL steps to calculate C = A * B:\n",
            "```\n",
            "M0 = A[0,0] + B[0,0]\n",
            "M1 = A[0,1] - M0\n",
            "M2 = A[0,0] * B[0,0]\n",
            "M3 = M1\n",
            "S0 = M0 + M1\n",
            "C = M0 * M1\n",
            "```\n",
            "\n",
            "Explanation:\n",
            "\n",
            "  * `M0 = A[0,0] + B[0,0]`: This line calculates the sum of the first two columns of A and the second column of B.\n",
            "  * `M1 = A[0,1] - M0`: This line calculates the difference between the first two columns of A and the second column of B.\n",
            "  * `M2 = A[0,0] * B[0,0]:` This line calculates the product of the first two columns of A and the second column of B.\n",
            "  * `M3 = M1`: This line assigns the result of the previous step to the current element of the output matrix C.\n",
            "  * `S0 = M0 + M1`: This line adds the values of M0 and M1 together to get the final value of C.\n",
            "  * `C = M0 * M1`: This line combines the values of M0 and M1 to get the final value of C.\n",
            "\n",
            "Note that the values of M0 and M1 are determined by the input matrices A and B, respectively. These values can be adjusted as needed to match your specific use case.\n",
            "\n",
            "Result: INCORRECT\n",
            "Length Mismatch: Expected 1 lines, Got 1 lines.\n",
            "------------------------------\n",
            "--- Test Case 7/10 ---\n",
            "User Prompt (content part):\n",
            "Input matrices:\n",
            "A = [[0,2],[9,-3]]\n",
            "B = [[-7,-8],[-2,0]]\n",
            "Provide the COMPLETE sequence of DSL steps to calculate C = A * B.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-7f1e8bfbe20b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    202\u001b[0m        \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"User Prompt (content part):\\n{user_prompt}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m        \u001b[0mgenerated_dsl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_prompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m        \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nModel's Generated DSL:\\n{generated_dsl}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-46-7f1e8bfbe20b>\u001b[0m in \u001b[0;36mget_model_response\u001b[0;34m(model, tokenizer, user_prompt_content)\u001b[0m\n\u001b[1;32m    152\u001b[0m    \u001b[0;31m# Using do_sample=False for deterministic output, good for testing exact match.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m    \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m        outputs = model.generate(\n\u001b[0m\u001b[1;32m    155\u001b[0m            \u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m            \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Increased for full sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2596\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2597\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2598\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2599\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3558\u001b[0m                 \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3559\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3560\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3562\u001b[0m             \u001b[0;31m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    967\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 969\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    970\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_requested_to_return_tuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_configured_to_return_tuple\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_top_level_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         outputs: BaseModelOutputWithPast = self.model(\n\u001b[0m\u001b[1;32m    704\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    967\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 969\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    970\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_requested_to_return_tuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_configured_to_return_tuple\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_top_level_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    434\u001b[0m                 \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m             layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    437\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_checkpointing\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         hidden_states, self_attn_weights = self.self_attn(\n\u001b[0m\u001b[1;32m    258\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0mquery_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mkey_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0mvalue_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0mcos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mposition_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul_4bit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquant_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, gc\n",
        "\n",
        "# 3. Force a full GC pass\n",
        "gc.collect()\n",
        "\n",
        "# 4. Return any freed blocks to CUDA\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# 5. (Optional) Check current usage\n",
        "print(torch.cuda.memory_summary())"
      ],
      "metadata": {
        "id": "L4MO_t1uZ14E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}